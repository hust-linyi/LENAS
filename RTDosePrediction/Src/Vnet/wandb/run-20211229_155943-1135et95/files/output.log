
Param size = 182.449 MB
Start training !
Local time: 15:59:51
Epoch: 0, iter: -1
    Begin lr is 0.000300000000, 0.000300000000
                Iter            0            0.34614
                Iter           10            0.34157
                Iter           20            0.33371
                Iter           30            0.32773
                Iter           40            0.31971
                Iter           50            0.31390
                Iter           60            0.30745
                Iter           70            0.30181
                Iter           80            0.29560
                Iter           90            0.28984
                Iter          100            0.28376
                Iter          110            0.27855
                Iter          120            0.27414
                Iter          130            0.27060
                Iter          140            0.26614
                Iter          150            0.26231
                Iter          160            0.25905
                Iter          170            0.25749
                Iter          180            0.25379
                Iter          190            0.25041
                Iter          200            0.24770
                Iter          210            0.24592
                Iter          220            0.24392
                Iter          230            0.24292
                Iter          240            0.23873
                Iter          250            0.23509
                Iter          260            0.23265
                Iter          270            0.22973
                Iter          280            0.22729
                Iter          290            0.22604
                Iter          300            0.22349
                Iter          310            0.22279
                Iter          320            0.22071
                Iter          330            0.21858
                Iter          340            0.21637
                Iter          350            0.21521
                Iter          360            0.21468
                Iter          370            0.21464
                Iter          380            0.21309
                Iter          390            0.21095
                Iter          400            0.20984
                Iter          410            0.20791
                Iter          420            0.20726
                Iter          430            0.20592
                Iter          440            0.20513
                Iter          450            0.20561
                Iter          460            0.20405
                Iter          470            0.20321
                Iter          480            0.20255
                Iter          490            0.20211
========> pt_241:  11.202661693096161
========> pt_242:  17.768301367759705
========> pt_243:  7.5151534378528595
========> pt_244:  15.444499850273132
========> pt_245:  12.573406100273132
========> pt_246:  11.651457846164703
========> pt_247:  15.97748413681984
========> pt_248:  11.971950829029083
========> pt_249:  9.082440435886383
========> pt_250:  16.443484872579575
========> pt_251:  10.628093928098679
========> pt_252:  10.622859746217728
========> pt_253:  10.215458273887634
========> pt_254:  8.441132679581642
========> pt_255:  11.655753254890442
========> pt_256:  16.869744211435318
========> pt_257:  10.172723233699799
========> pt_258:  13.264618515968323
========> pt_259:  12.171360850334167
========> pt_260:  17.658381462097168
========> pt_261:  9.247394353151321
========> pt_262:  13.359038233757019
========> pt_263:  15.369217544794083
========> pt_264:  13.118238747119904
========> pt_265:  16.77651047706604
========> pt_266:  14.224286675453186
========> pt_267:  16.364697813987732
========> pt_268:  10.42986884713173
========> pt_269:  15.857832282781601
========> pt_270:  11.006950438022614
========> pt_271:  10.506957769393921
========> pt_272:  10.588578879833221
========> pt_273:  17.54432886838913
========> pt_274:  10.055365115404129
========> pt_275:  9.574254304170609
========> pt_276:  15.37409394979477
========> pt_277:  16.574434340000153
========> pt_278:  9.401154965162277
========> pt_279:  8.477593585848808
========> pt_280:  8.336030766367912
========> pt_281:  15.440519452095032
========> pt_282:  17.851147055625916
========> pt_283:  15.945847481489182
========> pt_284:  11.322627514600754
========> pt_285:  12.933466285467148
========> pt_286:  16.432444900274277
========> pt_287:  9.25351306796074
========> pt_288:  15.903581827878952
========> pt_289:  8.272481560707092
========> pt_290:  8.721509277820587
========> pt_291:  15.981328934431076
========> pt_292:  11.104960441589355
========> pt_293:  15.851241052150726
========> pt_294:  15.314716547727585
========> pt_295:  9.285685867071152
========> pt_296:  15.716626197099686
========> pt_297:  8.657289370894432
========> pt_298:  11.329943686723709
========> pt_299:  15.78637182712555
========> pt_300:  7.6105473935604095
========> pt_301:  7.247104495763779
========> pt_302:  8.698160946369171
========> pt_303:  15.485665053129196
========> pt_304:  16.566301435232162
========> pt_305:  12.101218849420547
========> pt_306:  16.050952523946762
========> pt_307:  16.048616021871567
========> pt_308:  10.742766112089157
========> pt_309:  15.437285900115967
========> pt_310:  9.175455123186111
========> pt_311:  16.059852093458176
========> pt_312:  16.990973204374313
========> pt_313:  15.287664234638214
========> pt_314:  16.59887582063675
========> pt_315:  16.064918339252472
========> pt_316:  9.48204591870308
========> pt_317:  7.870809733867645
========> pt_318:  10.403253585100174
========> pt_319:  16.350460797548294
========> pt_320:  15.716520845890045
========> pt_321:  9.868064224720001
========> pt_322:  16.483938694000244
========> pt_323:  13.507053554058075
========> pt_324:  16.06608137488365
========> pt_325:  11.482722759246826
========> pt_326:  18.450303375720978
========> pt_327:  16.83472380042076
========> pt_328:  15.091098695993423
========> pt_329:  12.777572572231293
========> pt_330:  14.96756136417389
========> pt_331:  12.903289943933487
========> pt_332:  16.401675045490265
========> pt_333:  14.968886077404022
========> pt_334:  15.507806539535522
========> pt_335:  11.19975671172142
========> pt_336:  9.3158058822155
========> pt_337:  17.337888479232788
========> pt_338:  17.011368572711945
========> pt_339:  16.33522868156433
========> pt_340:  8.356453776359558
===============================================> mean Dose score: 13.154798255860806
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.218604574054,     best is           0.218604574054
            Average val evaluation index is   -13.154798255861,     best is           -13.154798255861
    Train use time   1538.33306
    Train loader use time     80.40874
    Val use time     43.99854
    Total use time   1586.86866
    End lr is 0.000299971096, 0.000299971096
    time: 16:26:17
Epoch: 1, iter: 499
    Begin lr is 0.000299971096, 0.000299971096
========> pt_241:  9.033926725387573
========> pt_242:  14.971785843372345
========> pt_243:  6.08211413025856
========> pt_244:  12.829401195049286
========> pt_245:  10.5446557700634
========> pt_246:  9.60300475358963
========> pt_247:  13.554004728794098
========> pt_248:  10.24003118276596
========> pt_249:  7.195681110024452
========> pt_250:  13.670328110456467
========> pt_251:  8.576296716928482
========> pt_252:  8.624885007739067
========> pt_253:  8.523313403129578
========> pt_254:  6.635939180850983
========> pt_255:  9.59683284163475
========> pt_256:  14.296777695417404
========> pt_257:  8.702547624707222
========> pt_258:  10.905572324991226
========> pt_259:  10.209441781044006
========> pt_260:  15.712161809206009
========> pt_261:  7.055723592638969
========> pt_262:  10.962090641260147
========> pt_263:  13.132803291082382
========> pt_264:  10.70131927728653
========> pt_265:  13.888024389743805
========> pt_266:  11.796873807907104
========> pt_267:  13.757915645837784
========> pt_268:  8.256458267569542
========> pt_269:  13.15654382109642
========> pt_270:  8.969041854143143
========> pt_271:  9.080076813697815
========> pt_272:  8.628018945455551
========> pt_273:  14.595616310834885
========> pt_274:  8.286749348044395
========> pt_275:  7.859344705939293
========> pt_276:  12.741368263959885
========> pt_277:  13.750476390123367
========> pt_278:  7.898640185594559
========> pt_279:  6.924950927495956
========> pt_280:  6.822822317481041
========> pt_281:  12.770015448331833
========> pt_282:  14.777006059885025
========> pt_283:  13.591941595077515
========> pt_284:  9.17195349931717
========> pt_285:  10.86296558380127
========> pt_286:  13.835465610027313
========> pt_287:  7.396645322442055
========> pt_288:  13.25657844543457
========> pt_289:  6.573661491274834
========> pt_290:  6.778217032551765
========> pt_291:  13.686301857233047
========> pt_292:  9.00856003165245
========> pt_293:  13.213960230350494
========> pt_294:  12.78898075222969
========> pt_295:  7.428830116987228
========> pt_296:  12.992767542600632
========> pt_297:  7.000939920544624
========> pt_298:  9.284553080797195
========> pt_299:  13.204643428325653
========> pt_300:  6.252698078751564
========> pt_301:  6.047665849328041
========> pt_302:  7.458943873643875
========> pt_303:  12.696217447519302
========> pt_304:  13.827573657035828
========> pt_305:  9.815069437026978
========> pt_306:  13.472291827201843
========> pt_307:  13.472273051738739
========> pt_308:  8.80543664097786
========> pt_309:  12.833862453699112
========> pt_310:  7.274956852197647
========> pt_311:  13.427051305770874
========> pt_312:  14.115956425666809
========> pt_313:  12.72770181298256
========> pt_314:  13.979171961545944
========> pt_315:  13.533564507961273
========> pt_316:  7.40068256855011
========> pt_317:  6.515046060085297
========> pt_318:  8.849420249462128
========> pt_319:  13.534284234046936
========> pt_320:  12.943699955940247
========> pt_321:  8.183298632502556
========> pt_322:  13.818488419055939
========> pt_323:  11.503579169511795
========> pt_324:  13.39398980140686
========> pt_325:  9.484632760286331
========> pt_326:  15.388378947973251
========> pt_327:  14.197233319282532
========> pt_328:  12.657523304224014
========> pt_329:  12.010000348091125
========> pt_330:  13.579172194004059
========> pt_331:  10.525466203689575
========> pt_332:  13.598345071077347
========> pt_333:  12.351987063884735
========> pt_334:  12.845137119293213
========> pt_335:  9.208530187606812
========> pt_336:  7.69735261797905
========> pt_337:  14.363657981157303
========> pt_338:  14.118432700634003
========> pt_339:  13.64200845360756
========> pt_340:  6.65510892868042
===============================================> mean Dose score: 10.936034392565489
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.177767321959,     best is           0.177767321959
            Average val evaluation index is   -10.936034392565,     best is           -10.936034392565
    Train use time   1533.81461
    Train loader use time     72.21616
    Val use time     43.86804
    Total use time   1582.32942
    End lr is 0.000299884394, 0.000299884394
    time: 16:52:40
Epoch: 2, iter: 999
    Begin lr is 0.000299884394, 0.000299884394
========> pt_241:  7.733682617545128
========> pt_242:  12.910867929458618
========> pt_243:  5.687148794531822
========> pt_244:  11.255277842283249
========> pt_245:  9.101054221391678
========> pt_246:  8.26848030090332
========> pt_247:  11.826165616512299
========> pt_248:  9.010287374258041
========> pt_249:  6.224251165986061
========> pt_250:  11.774379760026932
========> pt_251:  7.3381926119327545
========> pt_252:  7.526969462633133
========> pt_253:  7.333312034606934
========> pt_254:  5.874558687210083
========> pt_255:  8.389365077018738
========> pt_256:  12.204998135566711
========> pt_257:  7.63200044631958
========> pt_258:  9.338321834802628
========> pt_259:  8.917408287525177
========> pt_260:  13.77055361866951
========> pt_261:  6.036377102136612
========> pt_262:  9.382432699203491
========> pt_263:  11.459263861179352
========> pt_264:  9.331006705760956
========> pt_265:  11.878793239593506
========> pt_266:  10.178532153367996
========> pt_267:  11.948124766349792
========> pt_268:  7.100760713219643
========> pt_269:  11.281117051839828
========> pt_270:  7.895539626479149
========> pt_271:  8.189308866858482
========> pt_272:  7.130575627088547
========> pt_273:  12.720461785793304
========> pt_274:  7.580482140183449
========> pt_275:  6.841328144073486
========> pt_276:  11.04816883802414
========> pt_277:  11.854402869939804
========> pt_278:  7.1086860448122025
========> pt_279:  6.083445623517036
========> pt_280:  5.895486027002335
========> pt_281:  11.13408014178276
========> pt_282:  12.713302075862885
========> pt_283:  12.091336697340012
========> pt_284:  7.854496985673904
========> pt_285:  9.452530890703201
========> pt_286:  11.92426010966301
========> pt_287:  6.489568799734116
========> pt_288:  11.349813342094421
========> pt_289:  5.858936980366707
========> pt_290:  5.858074352145195
========> pt_291:  12.028254270553589
========> pt_292:  7.498283162713051
========> pt_293:  11.465186476707458
========> pt_294:  11.113469898700714
========> pt_295:  6.4177849888801575
========> pt_296:  11.22475415468216
========> pt_297:  6.117984130978584
========> pt_298:  7.912873551249504
========> pt_299:  11.325111091136932
========> pt_300:  5.644691735506058
========> pt_301:  5.205724015831947
========> pt_302:  6.794458851218224
========> pt_303:  10.693833082914352
========> pt_304:  11.912185400724411
========> pt_305:  8.323573246598244
========> pt_306:  11.616946458816528
========> pt_307:  11.528700739145279
========> pt_308:  7.622142285108566
========> pt_309:  10.916084498167038
========> pt_310:  6.136242747306824
========> pt_311:  11.748171299695969
========> pt_312:  12.136043161153793
========> pt_313:  11.095424592494965
========> pt_314:  12.051787227392197
========> pt_315:  11.684088557958603
========> pt_316:  6.464539542794228
========> pt_317:  5.792347192764282
========> pt_318:  7.709939479827881
========> pt_319:  11.59257486462593
========> pt_320:  11.185737699270248
========> pt_321:  6.66229210793972
========> pt_322:  11.887004375457764
========> pt_323:  10.32438725233078
========> pt_324:  11.768400818109512
========> pt_325:  8.397936597466469
========> pt_326:  13.522138595581055
========> pt_327:  11.934083849191666
========> pt_328:  11.05517104268074
========> pt_329:  11.139762848615646
========> pt_330:  12.993889898061752
========> pt_331:  9.027320891618729
========> pt_332:  11.675961911678314
========> pt_333:  10.682126581668854
========> pt_334:  10.941042304039001
========> pt_335:  8.026392608880997
========> pt_336:  6.758512705564499
========> pt_337:  12.463673949241638
========> pt_338:  12.23599225282669
========> pt_339:  11.758945286273956
========> pt_340:  5.6692662090063095
===============================================> mean Dose score: 9.496712105721235
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.154007827416,     best is           0.154007827416
            Average val evaluation index is   -9.496712105721,     best is           -9.496712105721
    Train use time   1534.10888
    Train loader use time     72.44918
    Val use time     43.97710
    Total use time   1582.74765
    End lr is 0.000299739928, 0.000299739928
    time: 17:19:03
Epoch: 3, iter: 1499
    Begin lr is 0.000299739928, 0.000299739928
========> pt_241:  6.8786730617284775
========> pt_242:  10.874705463647842
========> pt_243:  6.134157627820969
========> pt_244:  9.869660139083862
========> pt_245:  8.698931783437729
========> pt_246:  7.93894849717617
========> pt_247:  10.41596457362175
========> pt_248:  7.600160390138626
========> pt_249:  6.186823323369026
========> pt_250:  10.308200716972351
========> pt_251:  7.046902775764465
========> pt_252:  7.068822085857391
========> pt_253:  7.1579403430223465
========> pt_254:  6.321169063448906
========> pt_255:  7.259724214673042
========> pt_256:  10.411857962608337
========> pt_257:  6.871436685323715
========> pt_258:  8.534418046474457
========> pt_259:  7.91642002761364
========> pt_260:  11.469716578722
========> pt_261:  6.385139673948288
========> pt_262:  8.351843878626823
========> pt_263:  10.145154595375061
========> pt_264:  8.675966784358025
========> pt_265:  10.407316386699677
========> pt_266:  8.954845517873764
========> pt_267:  10.848298817873001
========> pt_268:  7.1774595230817795
========> pt_269:  10.086257010698318
========> pt_270:  8.103630691766739
========> pt_271:  7.3998527973890305
========> pt_272:  6.3696154952049255
========> pt_273:  11.110674440860748
========> pt_274:  6.973135024309158
========> pt_275:  5.989152640104294
========> pt_276:  9.685737788677216
========> pt_277:  10.627014338970184
========> pt_278:  7.463158443570137
========> pt_279:  5.514652356505394
========> pt_280:  5.502273067831993
========> pt_281:  9.777813702821732
========> pt_282:  11.172811836004257
========> pt_283:  10.712618976831436
========> pt_284:  7.232961878180504
========> pt_285:  8.30915629863739
========> pt_286:  10.799984335899353
========> pt_287:  6.583882123231888
========> pt_288:  10.231584310531616
========> pt_289:  6.293843984603882
========> pt_290:  6.413370668888092
========> pt_291:  10.519893020391464
========> pt_292:  7.348160818219185
========> pt_293:  10.429704040288925
========> pt_294:  9.82720673084259
========> pt_295:  6.121066436171532
========> pt_296:  9.896008372306824
========> pt_297:  6.336981654167175
========> pt_298:  7.339040637016296
========> pt_299:  10.035013556480408
========> pt_300:  5.9845662117004395
========> pt_301:  5.238298401236534
========> pt_302:  7.156935855746269
========> pt_303:  9.243287742137909
========> pt_304:  10.675989091396332
========> pt_305:  7.2738903015851974
========> pt_306:  10.464860051870346
========> pt_307:  10.452492237091064
========> pt_308:  7.582540139555931
========> pt_309:  9.876047968864441
========> pt_310:  5.9374841302633286
========> pt_311:  10.07572814822197
========> pt_312:  10.72987675666809
========> pt_313:  9.57275539636612
========> pt_314:  10.936877280473709
========> pt_315:  10.377248525619507
========> pt_316:  6.706512495875359
========> pt_317:  5.69704607129097
========> pt_318:  8.12518909573555
========> pt_319:  10.341142266988754
========> pt_320:  9.791286140680313
========> pt_321:  6.298646852374077
========> pt_322:  10.82920417189598
========> pt_323:  9.859184473752975
========> pt_324:  10.516945272684097
========> pt_325:  7.640685141086578
========> pt_326:  11.332562863826752
========> pt_327:  10.270941853523254
========> pt_328:  9.619604349136353
========> pt_329:  8.195984587073326
========> pt_330:  11.01782038807869
========> pt_331:  7.95505054295063
========> pt_332:  10.394127666950226
========> pt_333:  9.449098110198975
========> pt_334:  9.46942463517189
========> pt_335:  7.5389643758535385
========> pt_336:  6.745006889104843
========> pt_337:  10.902685075998306
========> pt_338:  10.911269634962082
========> pt_339:  10.542596727609634
========> pt_340:  5.4403286427259445
===============================================> mean Dose score: 8.632871025800705
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.135971775874,     best is           0.135971775874
            Average val evaluation index is   -8.632871025801,     best is           -8.632871025801
    Train use time   1533.76384
    Train loader use time     72.57641
    Val use time     43.77533
    Total use time   1582.16620
    End lr is 0.000299537754, 0.000299537754
    time: 17:45:25
Epoch: 4, iter: 1999
    Begin lr is 0.000299537754, 0.000299537754
========> pt_241:  6.6063714772462845
========> pt_242:  9.500686824321747
========> pt_243:  6.6208963841199875
========> pt_244:  9.313315004110336
========> pt_245:  7.984122782945633
========> pt_246:  6.976459845900536
========> pt_247:  9.68817338347435
========> pt_248:  7.501209527254105
========> pt_249:  5.797403529286385
========> pt_250:  9.279260486364365
========> pt_251:  6.716156303882599
========> pt_252:  6.5269580483436584
========> pt_253:  6.604821979999542
========> pt_254:  5.940523147583008
========> pt_255:  6.7962696403265
========> pt_256:  9.590259343385696
========> pt_257:  6.731759756803513
========> pt_258:  7.793556526303291
========> pt_259:  7.370080128312111
========> pt_260:  11.095208674669266
========> pt_261:  6.336200907826424
========> pt_262:  7.5932327657938
========> pt_263:  9.506384134292603
========> pt_264:  8.166764229536057
========> pt_265:  9.283907413482666
========> pt_266:  8.06367389857769
========> pt_267:  9.854258000850677
========> pt_268:  6.290569230914116
========> pt_269:  8.955737352371216
========> pt_270:  7.485330179333687
========> pt_271:  7.584357187151909
========> pt_272:  5.6850433349609375
========> pt_273:  10.261165052652359
========> pt_274:  6.767225563526154
========> pt_275:  5.254734754562378
========> pt_276:  9.012228548526764
========> pt_277:  9.58421990275383
========> pt_278:  6.967751160264015
========> pt_279:  5.81742599606514
========> pt_280:  5.8497098833322525
========> pt_281:  9.070293754339218
========> pt_282:  9.939026087522507
========> pt_283:  10.479938834905624
========> pt_284:  6.401372104883194
========> pt_285:  7.994714751839638
========> pt_286:  9.553892314434052
========> pt_287:  6.272109299898148
========> pt_288:  9.226229190826416
========> pt_289:  6.188471391797066
========> pt_290:  5.941125005483627
========> pt_291:  9.680060297250748
========> pt_292:  6.7786650359630585
========> pt_293:  9.267963916063309
========> pt_294:  8.928821682929993
========> pt_295:  5.897690057754517
========> pt_296:  9.008175134658813
========> pt_297:  6.694296970963478
========> pt_298:  6.640437990427017
========> pt_299:  8.95856723189354
========> pt_300:  6.331729218363762
========> pt_301:  5.14423593878746
========> pt_302:  7.494410201907158
========> pt_303:  8.258155882358551
========> pt_304:  9.574957340955734
========> pt_305:  6.8212879449129105
========> pt_306:  9.417753517627716
========> pt_307:  9.397568851709366
========> pt_308:  7.1432167291641235
========> pt_309:  8.619409874081612
========> pt_310:  5.462219268083572
========> pt_311:  9.307762682437897
========> pt_312:  9.625687599182129
========> pt_313:  9.133866429328918
========> pt_314:  9.949334859848022
========> pt_315:  9.395478516817093
========> pt_316:  6.014177203178406
========> pt_317:  5.435003191232681
========> pt_318:  7.9299018532037735
========> pt_319:  9.373295307159424
========> pt_320:  9.20839250087738
========> pt_321:  5.720913335680962
========> pt_322:  9.468798786401749
========> pt_323:  9.58031877875328
========> pt_324:  9.651938825845718
========> pt_325:  7.202478870749474
========> pt_326:  10.415796637535095
========> pt_327:  9.094880223274231
========> pt_328:  9.40448135137558
========> pt_329:  7.366414740681648
========> pt_330:  10.83497241139412
========> pt_331:  7.179018929600716
========> pt_332:  9.162863045930862
========> pt_333:  8.791832774877548
========> pt_334:  8.578214943408966
========> pt_335:  6.835691332817078
========> pt_336:  6.064147055149078
========> pt_337:  10.335879921913147
========> pt_338:  9.738976657390594
========> pt_339:  9.44401204586029
========> pt_340:  4.992101490497589
===============================================> mean Dose score: 8.005768404155969
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.122049257532,     best is           0.122049257532
            Average val evaluation index is   -8.005768404156,     best is           -8.005768404156
    Train use time   1534.69208
    Train loader use time     72.14145
    Val use time     43.99397
    Total use time   1583.30035
    End lr is 0.000299277950, 0.000299277950
    time: 18:11:48
Epoch: 5, iter: 2499
    Begin lr is 0.000299277950, 0.000299277950
========> pt_241:  5.818280801177025
========> pt_242:  8.500627428293228
========> pt_243:  5.6818608939647675
========> pt_244:  8.107697144150734
========> pt_245:  6.8416233360767365
========> pt_246:  6.199829503893852
========> pt_247:  8.387732654809952
========> pt_248:  6.538601443171501
========> pt_249:  4.73745197057724
========> pt_250:  8.155405595898628
========> pt_251:  5.847081318497658
========> pt_252:  5.632640495896339
========> pt_253:  5.599107518792152
========> pt_254:  4.761692136526108
========> pt_255:  5.739477574825287
========> pt_256:  8.28079178929329
========> pt_257:  5.972521752119064
========> pt_258:  6.633974015712738
========> pt_259:  6.400385871529579
========> pt_260:  10.392113476991653
========> pt_261:  5.19540898501873
========> pt_262:  6.537535414099693
========> pt_263:  8.278987780213356
========> pt_264:  7.007295414805412
========> pt_265:  8.067722618579865
========> pt_266:  7.026094347238541
========> pt_267:  8.803969025611877
========> pt_268:  5.277592316269875
========> pt_269:  7.864829748868942
========> pt_270:  6.660759821534157
========> pt_271:  6.524786874651909
========> pt_272:  5.072323828935623
========> pt_273:  8.710023388266563
========> pt_274:  5.940595120191574
========> pt_275:  4.810500517487526
========> pt_276:  7.664909139275551
========> pt_277:  8.309119790792465
========> pt_278:  5.734895840287209
========> pt_279:  5.007906258106232
========> pt_280:  4.871690794825554
========> pt_281:  7.905607968568802
========> pt_282:  8.646408468484879
========> pt_283:  9.225839078426361
========> pt_284:  5.7076239585876465
========> pt_285:  6.993358284235001
========> pt_286:  8.555670827627182
========> pt_287:  5.389062762260437
========> pt_288:  8.014888986945152
========> pt_289:  5.428277924656868
========> pt_290:  4.651591256260872
========> pt_291:  8.476351797580719
========> pt_292:  5.846299007534981
========> pt_293:  7.958626225590706
========> pt_294:  7.8598761558532715
========> pt_295:  4.781496599316597
========> pt_296:  7.711725756525993
========> pt_297:  5.5403100699186325
========> pt_298:  5.915475636720657
========> pt_299:  7.83355139195919
========> pt_300:  5.441374331712723
========> pt_301:  4.442185387015343
========> pt_302:  6.212233826518059
========> pt_303:  7.265876308083534
========> pt_304:  8.389599248766899
========> pt_305:  5.659136846661568
========> pt_306:  8.279039412736893
========> pt_307:  8.26834000647068
========> pt_308:  6.316695809364319
========> pt_309:  7.439529523253441
========> pt_310:  4.898138642311096
========> pt_311:  8.090336620807648
========> pt_312:  8.352744057774544
========> pt_313:  7.855355441570282
========> pt_314:  8.934290558099747
========> pt_315:  8.41614879667759
========> pt_316:  4.973898679018021
========> pt_317:  4.924289733171463
========> pt_318:  6.440524160861969
========> pt_319:  7.972846552729607
========> pt_320:  7.768643572926521
========> pt_321:  4.608050957322121
========> pt_322:  8.377270549535751
========> pt_323:  8.365479037165642
========> pt_324:  8.495076671242714
========> pt_325:  6.226093769073486
========> pt_326:  8.951494097709656
========> pt_327:  7.940876632928848
========> pt_328:  8.139448538422585
========> pt_329:  6.753364577889442
========> pt_330:  9.865269809961319
========> pt_331:  6.423628330230713
========> pt_332:  8.075648993253708
========> pt_333:  7.556691020727158
========> pt_334:  7.56054624915123
========> pt_335:  6.212103441357613
========> pt_336:  5.140633657574654
========> pt_337:  8.69247354567051
========> pt_338:  8.357963636517525
========> pt_339:  8.260407894849777
========> pt_340:  4.200506582856178
===============================================> mean Dose score: 6.945821414142847
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.110805587143,     best is           0.110805587143
            Average val evaluation index is   -6.945821414143,     best is           -6.945821414143
    Train use time   1535.22913
    Train loader use time     72.65375
    Val use time     43.80463
    Total use time   1583.74945
    End lr is 0.000298960615, 0.000298960615
    time: 18:38:12
Epoch: 6, iter: 2999
    Begin lr is 0.000298960615, 0.000298960615
========> pt_241:  5.779922008514404
========> pt_242:  7.63360470533371
========> pt_243:  6.407011523842812
========> pt_244:  7.8440578281879425
========> pt_245:  6.716674715280533
========> pt_246:  6.157092377543449
========> pt_247:  7.924797534942627
========> pt_248:  6.731443181633949
========> pt_249:  5.683588758111
========> pt_250:  7.642711848020554
========> pt_251:  5.864571705460548
========> pt_252:  5.849372446537018
========> pt_253:  6.023903414607048
========> pt_254:  5.55465504527092
========> pt_255:  5.612946599721909
========> pt_256:  7.78255358338356
========> pt_257:  5.729642882943153
========> pt_258:  6.708381175994873
========> pt_259:  6.009244993329048
========> pt_260:  8.603823110461235
========> pt_261:  6.099161729216576
========> pt_262:  6.447915434837341
========> pt_263:  7.879311889410019
========> pt_264:  7.353446111083031
========> pt_265:  7.626844495534897
========> pt_266:  6.642513200640678
========> pt_267:  8.487426191568375
========> pt_268:  6.095627769827843
========> pt_269:  7.495046481490135
========> pt_270:  7.034635618329048
========> pt_271:  6.238214895129204
========> pt_272:  4.548956751823425
========> pt_273:  8.326014056801796
========> pt_274:  6.115411892533302
========> pt_275:  4.83417272567749
========> pt_276:  7.167103812098503
========> pt_277:  7.880510911345482
========> pt_278:  6.7269302904605865
========> pt_279:  5.98599836230278
========> pt_280:  5.699870735406876
========> pt_281:  7.595324665307999
========> pt_282:  8.068825677037239
========> pt_283:  8.502637445926666
========> pt_284:  5.34964993596077
========> pt_285:  6.872246116399765
========> pt_286:  8.028601855039597
========> pt_287:  6.446496844291687
========> pt_288:  7.605301737785339
========> pt_289:  6.374640539288521
========> pt_290:  5.159147307276726
========> pt_291:  7.7610113471746445
========> pt_292:  6.087577268481255
========> pt_293:  7.942952364683151
========> pt_294:  7.373725175857544
========> pt_295:  5.3234826773405075
========> pt_296:  7.357932925224304
========> pt_297:  6.603230759501457
========> pt_298:  5.93973770737648
========> pt_299:  7.39462748169899
========> pt_300:  5.857779160141945
========> pt_301:  5.03527045249939
========> pt_302:  7.0541829615831375
========> pt_303:  6.878010705113411
========> pt_304:  8.053409978747368
========> pt_305:  5.5924177169799805
========> pt_306:  7.908926010131836
========> pt_307:  7.851279601454735
========> pt_308:  6.623696014285088
========> pt_309:  6.883128583431244
========> pt_310:  5.316895619034767
========> pt_311:  7.650852054357529
========> pt_312:  7.977212369441986
========> pt_313:  7.587542235851288
========> pt_314:  8.595879524946213
========> pt_315:  7.946807071566582
========> pt_316:  6.2783583998680115
========> pt_317:  4.992728903889656
========> pt_318:  6.915473490953445
========> pt_319:  7.555240094661713
========> pt_320:  7.391757443547249
========> pt_321:  4.769793227314949
========> pt_322:  7.931443527340889
========> pt_323:  9.09777581691742
========> pt_324:  8.154124170541763
========> pt_325:  6.077190786600113
========> pt_326:  8.42872366309166
========> pt_327:  7.33347162604332
========> pt_328:  8.203917220234871
========> pt_329:  6.268667653203011
========> pt_330:  9.077417999505997
========> pt_331:  5.992521792650223
========> pt_332:  7.5606124848127365
========> pt_333:  7.377307638525963
========> pt_334:  7.1296608448028564
========> pt_335:  6.217378303408623
========> pt_336:  5.830967798829079
========> pt_337:  8.523508459329605
========> pt_338:  7.873809635639191
========> pt_339:  7.720235213637352
========> pt_340:  4.653127193450928
===============================================> mean Dose score: 6.889007660746574
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.102225556239,     best is           0.102225556239
            Average val evaluation index is   -6.889007660747,     best is           -6.889007660747
    Train use time   1534.37623
    Train loader use time     71.97884
    Val use time     43.86214
    Total use time   1582.87294
    End lr is 0.000298585873, 0.000298585873
    time: 19:04:35
Epoch: 7, iter: 3499
    Begin lr is 0.000298585873, 0.000298585873
========> pt_241:  5.867775529623032
========> pt_242:  7.240507006645203
========> pt_243:  6.8964435160160065
========> pt_244:  7.547677755355835
========> pt_245:  6.9057561457157135
========> pt_246:  6.579124629497528
========> pt_247:  7.687373459339142
========> pt_248:  6.704706400632858
========> pt_249:  5.982075855135918
========> pt_250:  7.240176871418953
========> pt_251:  6.207221820950508
========> pt_252:  5.527893230319023
========> pt_253:  6.206240281462669
========> pt_254:  5.7805123925209045
========> pt_255:  5.823108181357384
========> pt_256:  7.316310852766037
========> pt_257:  6.060914546251297
========> pt_258:  6.446253284811974
========> pt_259:  6.430796906352043
========> pt_260:  8.32123152911663
========> pt_261:  6.5183354169130325
========> pt_262:  6.1302559822797775
========> pt_263:  7.834882885217667
========> pt_264:  7.2340623289346695
========> pt_265:  7.136813774704933
========> pt_266:  6.417899206280708
========> pt_267:  7.802264168858528
========> pt_268:  6.296811029314995
========> pt_269:  7.08476610481739
========> pt_270:  7.939238995313644
========> pt_271:  6.641172841191292
========> pt_272:  5.016963332891464
========> pt_273:  7.8077382594347
========> pt_274:  6.205560714006424
========> pt_275:  4.626473858952522
========> pt_276:  7.139754742383957
========> pt_277:  7.440559044480324
========> pt_278:  7.134776636958122
========> pt_279:  6.0520874708890915
========> pt_280:  5.733063668012619
========> pt_281:  7.201313748955727
========> pt_282:  7.711714282631874
========> pt_283:  8.482666090130806
========> pt_284:  5.505645349621773
========> pt_285:  7.077500522136688
========> pt_286:  7.5596486777067184
========> pt_287:  6.828566566109657
========> pt_288:  7.254590690135956
========> pt_289:  6.448560580611229
========> pt_290:  5.7199860364198685
========> pt_291:  7.86530539393425
========> pt_292:  6.150126159191132
========> pt_293:  7.458347752690315
========> pt_294:  7.252728268504143
========> pt_295:  5.616661012172699
========> pt_296:  7.010786086320877
========> pt_297:  6.475735455751419
========> pt_298:  6.126631796360016
========> pt_299:  7.044945955276489
========> pt_300:  6.613881140947342
========> pt_301:  5.174533277750015
========> pt_302:  7.357599139213562
========> pt_303:  6.461018621921539
========> pt_304:  7.563543021678925
========> pt_305:  5.4669929295778275
========> pt_306:  7.373772636055946
========> pt_307:  7.487045526504517
========> pt_308:  6.5602365136146545
========> pt_309:  6.578455492854118
========> pt_310:  5.630476623773575
========> pt_311:  7.436727285385132
========> pt_312:  7.378493100404739
========> pt_313:  7.202698439359665
========> pt_314:  7.961423248052597
========> pt_315:  7.210450619459152
========> pt_316:  6.568270325660706
========> pt_317:  5.745084658265114
========> pt_318:  8.010967522859573
========> pt_319:  7.13749386370182
========> pt_320:  7.083119601011276
========> pt_321:  5.221308171749115
========> pt_322:  7.41669125854969
========> pt_323:  8.67521420121193
========> pt_324:  7.659117430448532
========> pt_325:  6.087615862488747
========> pt_326:  7.780757918953896
========> pt_327:  6.95964902639389
========> pt_328:  7.613066956400871
========> pt_329:  6.406153067946434
========> pt_330:  9.889383763074875
========> pt_331:  6.14916704595089
========> pt_332:  6.949359551072121
========> pt_333:  7.009556815028191
========> pt_334:  6.7320335656404495
========> pt_335:  6.5872617065906525
========> pt_336:  6.081725582480431
========> pt_337:  8.11173751950264
========> pt_338:  7.444923296570778
========> pt_339:  7.39590160548687
========> pt_340:  4.987840503454208
===============================================> mean Dose score: 6.829177895188332
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.095545697734,     best is           0.095545697734
            Average val evaluation index is   -6.829177895188,     best is           -6.829177895188
    Train use time   1532.56436
    Train loader use time     71.36642
    Val use time     43.81953
    Total use time   1581.07453
    End lr is 0.000298153867, 0.000298153867
    time: 19:30:56
Epoch: 8, iter: 3999
    Begin lr is 0.000298153867, 0.000298153867
========> pt_241:  4.94734026491642
========> pt_242:  6.449223458766937
========> pt_243:  5.568526983261108
========> pt_244:  6.626331880688667
========> pt_245:  5.938709229230881
========> pt_246:  5.516941398382187
========> pt_247:  6.75610788166523
========> pt_248:  5.363457724452019
========> pt_249:  5.083446726202965
========> pt_250:  6.301286369562149
========> pt_251:  5.107201859354973
========> pt_252:  4.896654859185219
========> pt_253:  4.873975142836571
========> pt_254:  4.88115519285202
========> pt_255:  4.758932664990425
========> pt_256:  6.447005346417427
========> pt_257:  5.255495682358742
========> pt_258:  5.3861332684755325
========> pt_259:  5.302359238266945
========> pt_260:  7.905559465289116
========> pt_261:  5.062879770994186
========> pt_262:  5.3646014630794525
========> pt_263:  6.7759008705616
========> pt_264:  6.307715401053429
========> pt_265:  6.405505836009979
========> pt_266:  5.838397666811943
========> pt_267:  6.991294547915459
========> pt_268:  5.103591233491898
========> pt_269:  6.317101567983627
========> pt_270:  6.53403639793396
========> pt_271:  5.917897149920464
========> pt_272:  4.375965371727943
========> pt_273:  6.929038763046265
========> pt_274:  5.429375246167183
========> pt_275:  4.3648383021354675
========> pt_276:  6.048690676689148
========> pt_277:  6.494967266917229
========> pt_278:  5.818458124995232
========> pt_279:  4.476217478513718
========> pt_280:  4.392534717917442
========> pt_281:  6.403985023498535
========> pt_282:  6.836715638637543
========> pt_283:  7.925199642777443
========> pt_284:  4.745980203151703
========> pt_285:  5.714016482234001
========> pt_286:  6.897323355078697
========> pt_287:  5.648851543664932
========> pt_288:  6.3520364463329315
========> pt_289:  5.150917395949364
========> pt_290:  4.550875499844551
========> pt_291:  6.712389215826988
========> pt_292:  4.8998112231493
========> pt_293:  6.628587543964386
========> pt_294:  6.354148164391518
========> pt_295:  4.24876943230629
========> pt_296:  6.096847653388977
========> pt_297:  5.367311909794807
========> pt_298:  5.052859932184219
========> pt_299:  6.195501759648323
========> pt_300:  5.314255580306053
========> pt_301:  4.312463887035847
========> pt_302:  6.316712498664856
========> pt_303:  5.844822004437447
========> pt_304:  6.658759713172913
========> pt_305:  4.760580733418465
========> pt_306:  6.523808464407921
========> pt_307:  6.753664463758469
========> pt_308:  5.646422728896141
========> pt_309:  5.88347963988781
========> pt_310:  4.3663812801241875
========> pt_311:  6.2580506503582
========> pt_312:  6.572167798876762
========> pt_313:  6.340998560190201
========> pt_314:  7.2306227684021
========> pt_315:  6.592811420559883
========> pt_316:  5.387028232216835
========> pt_317:  4.731890261173248
========> pt_318:  6.052017062902451
========> pt_319:  6.282512992620468
========> pt_320:  6.0690005123615265
========> pt_321:  4.244084171950817
========> pt_322:  6.726344600319862
========> pt_323:  8.086525723338127
========> pt_324:  6.719766929745674
========> pt_325:  5.426880717277527
========> pt_326:  6.977810636162758
========> pt_327:  6.124343276023865
========> pt_328:  6.6038331389427185
========> pt_329:  6.519388407468796
========> pt_330:  9.104044735431671
========> pt_331:  5.336102396249771
========> pt_332:  6.415978893637657
========> pt_333:  6.028121113777161
========> pt_334:  5.762684568762779
========> pt_335:  5.761998742818832
========> pt_336:  4.976934567093849
========> pt_337:  7.036177814006805
========> pt_338:  6.55946359038353
========> pt_339:  6.472747027873993
========> pt_340:  3.901943601667881
===============================================> mean Dose score: 5.887806064635515
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.089873456582,     best is           0.089873456582
            Average val evaluation index is   -5.887806064636,     best is           -5.887806064636
    Train use time   1532.51319
    Train loader use time     72.52448
    Val use time     43.73059
    Total use time   1580.84938
    End lr is 0.000297664764, 0.000297664764
    time: 19:57:17
Epoch: 9, iter: 4499
    Begin lr is 0.000297664764, 0.000297664764
========> pt_241:  4.775058701634407
========> pt_242:  6.118523925542831
========> pt_243:  5.229765996336937
========> pt_244:  6.167952939867973
========> pt_245:  5.643819719552994
========> pt_246:  5.326729789376259
========> pt_247:  6.280363202095032
========> pt_248:  5.114067420363426
========> pt_249:  4.624000191688538
========> pt_250:  6.003720834851265
========> pt_251:  5.244488567113876
========> pt_252:  4.7157251089811325
========> pt_253:  5.417131558060646
========> pt_254:  4.2610230296850204
========> pt_255:  4.471402615308762
========> pt_256:  6.044689416885376
========> pt_257:  5.177895650267601
========> pt_258:  5.04698321223259
========> pt_259:  5.002940148115158
========> pt_260:  8.13213862478733
========> pt_261:  4.99856024980545
========> pt_262:  5.189442038536072
========> pt_263:  6.195875182747841
========> pt_264:  5.724845752120018
========> pt_265:  5.82601472735405
========> pt_266:  5.484872907400131
========> pt_267:  6.570689752697945
========> pt_268:  4.842521548271179
========> pt_269:  5.834081396460533
========> pt_270:  6.511009857058525
========> pt_271:  5.641902536153793
========> pt_272:  4.290084056556225
========> pt_273:  6.560564562678337
========> pt_274:  5.0420379638671875
========> pt_275:  3.781074211001396
========> pt_276:  5.567237213253975
========> pt_277:  5.9556759893894196
========> pt_278:  5.373614728450775
========> pt_279:  4.8490069061517715
========> pt_280:  4.049935452640057
========> pt_281:  5.91951496899128
========> pt_282:  6.355608478188515
========> pt_283:  7.259050905704498
========> pt_284:  4.354852885007858
========> pt_285:  5.312175676226616
========> pt_286:  6.35223776102066
========> pt_287:  5.325268432497978
========> pt_288:  5.803861245512962
========> pt_289:  4.58701878786087
========> pt_290:  4.517596513032913
========> pt_291:  6.420047953724861
========> pt_292:  4.946406185626984
========> pt_293:  6.126535311341286
========> pt_294:  5.853012800216675
========> pt_295:  4.4662075489759445
========> pt_296:  5.578632354736328
========> pt_297:  5.126244351267815
========> pt_298:  4.864620268344879
========> pt_299:  5.77326662838459
========> pt_300:  5.315624102950096
========> pt_301:  4.22042578458786
========> pt_302:  5.824280604720116
========> pt_303:  5.297546982765198
========> pt_304:  6.184821128845215
========> pt_305:  4.6214232593774796
========> pt_306:  6.046303063631058
========> pt_307:  6.198457330465317
========> pt_308:  5.6023238599300385
========> pt_309:  5.500453412532806
========> pt_310:  4.5217907428741455
========> pt_311:  5.879428833723068
========> pt_312:  5.964179188013077
========> pt_313:  5.938719660043716
========> pt_314:  6.873896270990372
========> pt_315:  6.243746876716614
========> pt_316:  4.958583116531372
========> pt_317:  4.877598807215691
========> pt_318:  6.742690205574036
========> pt_319:  5.756400525569916
========> pt_320:  5.643516704440117
========> pt_321:  4.0168726444244385
========> pt_322:  6.243418827652931
========> pt_323:  8.178628757596016
========> pt_324:  6.426103040575981
========> pt_325:  5.22636815905571
========> pt_326:  6.387470439076424
========> pt_327:  5.715112239122391
========> pt_328:  6.181829571723938
========> pt_329:  5.564465746283531
========> pt_330:  8.447138741612434
========> pt_331:  5.237848311662674
========> pt_332:  5.842641964554787
========> pt_333:  5.568078458309174
========> pt_334:  5.516109019517899
========> pt_335:  5.588580742478371
========> pt_336:  5.0747062265872955
========> pt_337:  6.783642619848251
========> pt_338:  6.160368174314499
========> pt_339:  6.093279272317886
========> pt_340:  3.9696959033608437
===============================================> mean Dose score: 5.584641960635781
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.085382416293,     best is           0.085382416293
            Average val evaluation index is   -5.584641960636,     best is           -5.584641960636
    Train use time   1532.31994
    Train loader use time     70.33191
    Val use time     45.42053
    Total use time   1582.64627
    End lr is 0.000297118753, 0.000297118753
    time: 20:23:39
Epoch: 10, iter: 4999
    Begin lr is 0.000297118753, 0.000297118753
========> pt_241:  4.639352783560753
========> pt_242:  5.738690048456192
========> pt_243:  5.249674767255783
========> pt_244:  5.898721665143967
========> pt_245:  5.206746235489845
========> pt_246:  5.197053924202919
========> pt_247:  5.943417698144913
========> pt_248:  4.901672080159187
========> pt_249:  4.704180806875229
========> pt_250:  5.6590429693460464
========> pt_251:  4.821120128035545
========> pt_252:  4.5801761746406555
========> pt_253:  4.89782989025116
========> pt_254:  4.427000731229782
========> pt_255:  4.251030571758747
========> pt_256:  5.784258097410202
========> pt_257:  5.052560046315193
========> pt_258:  5.118763372302055
========> pt_259:  4.804262891411781
========> pt_260:  7.816378623247147
========> pt_261:  4.977049827575684
========> pt_262:  4.978440776467323
========> pt_263:  6.229310631752014
========> pt_264:  5.8114684373140335
========> pt_265:  5.58357760310173
========> pt_266:  5.353449359536171
========> pt_267:  6.5166544914245605
========> pt_268:  4.8880791664123535
========> pt_269:  5.633771196007729
========> pt_270:  6.116424202919006
========> pt_271:  5.515514984726906
========> pt_272:  4.134087339043617
========> pt_273:  6.166984960436821
========> pt_274:  4.972696006298065
========> pt_275:  4.242713823914528
========> pt_276:  5.341359004378319
========> pt_277:  5.63167043030262
========> pt_278:  5.416203215718269
========> pt_279:  4.659831076860428
========> pt_280:  4.326396062970161
========> pt_281:  5.729663744568825
========> pt_282:  5.965245738625526
========> pt_283:  6.954064890742302
========> pt_284:  4.10870473831892
========> pt_285:  5.169324651360512
========> pt_286:  6.054558530449867
========> pt_287:  5.283232256770134
========> pt_288:  5.61294712126255
========> pt_289:  5.173969492316246
========> pt_290:  4.447101429104805
========> pt_291:  6.013070493936539
========> pt_292:  4.722471237182617
========> pt_293:  5.997153073549271
========> pt_294:  5.69660484790802
========> pt_295:  4.1706757619977
========> pt_296:  5.270424261689186
========> pt_297:  5.4669444262981415
========> pt_298:  4.701153263449669
========> pt_299:  5.634758993983269
========> pt_300:  5.259785354137421
========> pt_301:  4.260338246822357
========> pt_302:  5.9605492651462555
========> pt_303:  5.133908912539482
========> pt_304:  5.9252868592739105
========> pt_305:  4.443603456020355
========> pt_306:  5.82703173160553
========> pt_307:  6.055350750684738
========> pt_308:  5.240005925297737
========> pt_309:  5.101559832692146
========> pt_310:  4.380407854914665
========> pt_311:  5.745857581496239
========> pt_312:  5.766425058245659
========> pt_313:  5.688891783356667
========> pt_314:  6.5757716447114944
========> pt_315:  5.960157066583633
========> pt_316:  5.157550349831581
========> pt_317:  4.559437111020088
========> pt_318:  6.03204570710659
========> pt_319:  5.4810333251953125
========> pt_320:  5.282069742679596
========> pt_321:  4.040253311395645
========> pt_322:  5.704334080219269
========> pt_323:  8.085574433207512
========> pt_324:  6.053139939904213
========> pt_325:  4.993436112999916
========> pt_326:  6.023123189806938
========> pt_327:  5.4757798463106155
========> pt_328:  6.059148088097572
========> pt_329:  5.422080457210541
========> pt_330:  8.003997132182121
========> pt_331:  4.956321194767952
========> pt_332:  5.570851489901543
========> pt_333:  5.392244681715965
========> pt_334:  5.1278626918792725
========> pt_335:  5.258004814386368
========> pt_336:  4.66143324971199
========> pt_337:  6.2973664700984955
========> pt_338:  5.6571174412965775
========> pt_339:  5.90106338262558
========> pt_340:  3.913765884935856
===============================================> mean Dose score: 5.39793650507927
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.081454227746,     best is           0.081454227746
            Average val evaluation index is   -5.397936505079,     best is           -5.397936505079
    Train use time   1532.31306
    Train loader use time     70.45084
    Val use time     45.44904
    Total use time   1582.57542
    End lr is 0.000296516044, 0.000296516044
    time: 20:50:02
Epoch: 11, iter: 5499
    Begin lr is 0.000296516044, 0.000296516044
========> pt_241:  4.465397074818611
========> pt_242:  5.336308926343918
========> pt_243:  4.768982753157616
========> pt_244:  5.48754945397377
========> pt_245:  4.947760626673698
========> pt_246:  4.749482870101929
========> pt_247:  5.620483905076981
========> pt_248:  4.496026113629341
========> pt_249:  4.478272870182991
========> pt_250:  5.32694049179554
========> pt_251:  4.595025479793549
========> pt_252:  4.455022066831589
========> pt_253:  4.6547262370586395
========> pt_254:  4.02063112705946
========> pt_255:  4.198492653667927
========> pt_256:  5.353556275367737
========> pt_257:  4.725900366902351
========> pt_258:  4.785868152976036
========> pt_259:  4.614864364266396
========> pt_260:  7.29503408074379
========> pt_261:  4.513967111706734
========> pt_262:  4.6572791785001755
========> pt_263:  5.814104825258255
========> pt_264:  5.210791304707527
========> pt_265:  5.198989361524582
========> pt_266:  5.123270004987717
========> pt_267:  5.914132669568062
========> pt_268:  4.2163363844156265
========> pt_269:  5.360173061490059
========> pt_270:  6.019653901457787
========> pt_271:  5.262088477611542
========> pt_272:  4.115202352404594
========> pt_273:  5.701337829232216
========> pt_274:  4.956047385931015
========> pt_275:  3.636680468916893
========> pt_276:  4.981481358408928
========> pt_277:  5.122036561369896
========> pt_278:  5.069927871227264
========> pt_279:  4.3509491533041
========> pt_280:  3.93958892673254
========> pt_281:  5.33922016620636
========> pt_282:  5.707777813076973
========> pt_283:  6.723516285419464
========> pt_284:  4.370093084871769
========> pt_285:  4.797493293881416
========> pt_286:  5.682514905929565
========> pt_287:  5.11484868824482
========> pt_288:  5.102462619543076
========> pt_289:  4.437784627079964
========> pt_290:  3.832164853811264
========> pt_291:  5.668071880936623
========> pt_292:  4.275752380490303
========> pt_293:  5.399935841560364
========> pt_294:  5.280057117342949
========> pt_295:  4.006200097501278
========> pt_296:  4.92387980222702
========> pt_297:  4.553223475813866
========> pt_298:  4.434891641139984
========> pt_299:  5.006216466426849
========> pt_300:  4.836667254567146
========> pt_301:  4.097577407956123
========> pt_302:  5.545462369918823
========> pt_303:  4.890701994299889
========> pt_304:  5.424240157008171
========> pt_305:  4.314661920070648
========> pt_306:  5.378437936306
========> pt_307:  5.577089115977287
========> pt_308:  4.876852482557297
========> pt_309:  4.951095879077911
========> pt_310:  4.230024479329586
========> pt_311:  5.320905223488808
========> pt_312:  5.314896032214165
========> pt_313:  5.306644216179848
========> pt_314:  6.2590499222278595
========> pt_315:  5.724170878529549
========> pt_316:  4.380816742777824
========> pt_317:  4.5891425013542175
========> pt_318:  5.732025280594826
========> pt_319:  5.133647620677948
========> pt_320:  4.9189116060733795
========> pt_321:  4.004968479275703
========> pt_322:  5.301569104194641
========> pt_323:  7.8644417226314545
========> pt_324:  5.8047692477703094
========> pt_325:  4.9179597944021225
========> pt_326:  5.766356736421585
========> pt_327:  5.05311131477356
========> pt_328:  5.530612021684647
========> pt_329:  4.518359005451202
========> pt_330:  8.325773626565933
========> pt_331:  4.8512109369039536
========> pt_332:  5.443098023533821
========> pt_333:  4.9019500613212585
========> pt_334:  4.872307777404785
========> pt_335:  5.353173986077309
========> pt_336:  4.361327551305294
========> pt_337:  5.882316082715988
========> pt_338:  5.2796075493097305
========> pt_339:  5.23749940097332
========> pt_340:  3.662613295018673
===============================================> mean Dose score: 5.059325158596039
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.078502557255,     best is           0.078502557255
            Average val evaluation index is   -5.059325158596,     best is           -5.059325158596
    Train use time   1534.24986
    Train loader use time     74.00354
    Val use time     45.59750
    Total use time   1584.77174
    End lr is 0.000295856870, 0.000295856870
    time: 21:16:27
Epoch: 12, iter: 5999
    Begin lr is 0.000295856870, 0.000295856870
========> pt_241:  4.731451123952866
========> pt_242:  5.178020820021629
========> pt_243:  5.7799214869737625
========> pt_244:  5.722712129354477
========> pt_245:  5.605121925473213
========> pt_246:  5.531189367175102
========> pt_247:  5.608345046639442
========> pt_248:  4.78732168674469
========> pt_249:  5.2944266051054
========> pt_250:  5.485488325357437
========> pt_251:  5.153803080320358
========> pt_252:  4.728614464402199
========> pt_253:  5.53022138774395
========> pt_254:  4.956400468945503
========> pt_255:  4.339055679738522
========> pt_256:  5.297020748257637
========> pt_257:  4.847479835152626
========> pt_258:  5.000046640634537
========> pt_259:  4.934331476688385
========> pt_260:  7.633481100201607
========> pt_261:  6.309368684887886
========> pt_262:  5.041567534208298
========> pt_263:  5.840783715248108
========> pt_264:  6.295967176556587
========> pt_265:  5.174750238656998
========> pt_266:  5.269544422626495
========> pt_267:  5.861101374030113
========> pt_268:  5.75398214161396
========> pt_269:  5.241170525550842
========> pt_270:  7.121351137757301
========> pt_271:  5.543404370546341
========> pt_272:  4.446399956941605
========> pt_273:  5.97699761390686
========> pt_274:  5.2191223949193954
========> pt_275:  4.519743695855141
========> pt_276:  5.097232609987259
========> pt_277:  5.2646127343177795
========> pt_278:  6.441510915756226
========> pt_279:  5.083650127053261
========> pt_280:  4.369636736810207
========> pt_281:  5.349818915128708
========> pt_282:  5.666097849607468
========> pt_283:  7.007655277848244
========> pt_284:  4.2713479697704315
========> pt_285:  5.070764943957329
========> pt_286:  5.7572730630636215
========> pt_287:  6.191159412264824
========> pt_288:  5.078051388263702
========> pt_289:  5.510791391134262
========> pt_290:  5.152745917439461
========> pt_291:  5.730657279491425
========> pt_292:  4.980703219771385
========> pt_293:  5.879773572087288
========> pt_294:  5.329019874334335
========> pt_295:  4.837385937571526
========> pt_296:  5.031351074576378
========> pt_297:  5.608007088303566
========> pt_298:  5.252372696995735
========> pt_299:  5.07753349840641
========> pt_300:  6.197329759597778
========> pt_301:  4.7958290576934814
========> pt_302:  6.501297727227211
========> pt_303:  4.810287207365036
========> pt_304:  5.602306127548218
========> pt_305:  4.689662680029869
========> pt_306:  5.368581861257553
========> pt_307:  5.651567727327347
========> pt_308:  5.6656524538993835
========> pt_309:  4.930933639407158
========> pt_310:  5.3219738602638245
========> pt_311:  5.296392813324928
========> pt_312:  5.336684957146645
========> pt_313:  5.590092167258263
========> pt_314:  6.1853645741939545
========> pt_315:  5.5787429213523865
========> pt_316:  6.5089101344347
========> pt_317:  5.199209973216057
========> pt_318:  7.960262298583984
========> pt_319:  5.177007466554642
========> pt_320:  5.076533183455467
========> pt_321:  4.716973155736923
========> pt_322:  5.461731106042862
========> pt_323:  8.573912233114243
========> pt_324:  5.840745121240616
========> pt_325:  5.140900686383247
========> pt_326:  5.6444064527750015
========> pt_327:  5.1108114421367645
========> pt_328:  5.580942258238792
========> pt_329:  5.24320974946022
========> pt_330:  8.279372677206993
========> pt_331:  5.2985285222530365
========> pt_332:  5.101621896028519
========> pt_333:  4.963822513818741
========> pt_334:  4.882829859852791
========> pt_335:  5.816965997219086
========> pt_336:  5.3784071654081345
========> pt_337:  6.43124021589756
========> pt_338:  5.609450191259384
========> pt_339:  5.6066276133060455
========> pt_340:  4.325868263840675
===============================================> mean Dose score: 5.492518515884877
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.075773137845,     best is           0.075773137845
            Average val evaluation index is   -5.492518515885,     best is           -5.059325158596
    Train use time   1534.79671
    Train loader use time     74.35515
    Val use time     45.31416
    Total use time   1583.92842
    End lr is 0.000295141484, 0.000295141484
    time: 21:42:50
Epoch: 13, iter: 6499
    Begin lr is 0.000295141484, 0.000295141484
========> pt_241:  4.072188287973404
========> pt_242:  4.830147996544838
========> pt_243:  4.610977321863174
========> pt_244:  4.88624282181263
========> pt_245:  4.649294391274452
========> pt_246:  4.695206135511398
========> pt_247:  5.057272166013718
========> pt_248:  3.9868316426873207
========> pt_249:  4.388210624456406
========> pt_250:  4.710185304284096
========> pt_251:  4.082841798663139
========> pt_252:  4.235579930245876
========> pt_253:  4.32965125888586
========> pt_254:  3.8150353729724884
========> pt_255:  3.7677744030952454
========> pt_256:  4.7577915340662
========> pt_257:  4.655663967132568
========> pt_258:  4.161187894642353
========> pt_259:  4.342188313603401
========> pt_260:  7.572893723845482
========> pt_261:  4.548853486776352
========> pt_262:  4.486149698495865
========> pt_263:  5.2358633279800415
========> pt_264:  4.9566800147295
========> pt_265:  4.780616760253906
========> pt_266:  4.799097031354904
========> pt_267:  5.406121835112572
========> pt_268:  4.493960291147232
========> pt_269:  4.865141287446022
========> pt_270:  5.721585601568222
========> pt_271:  5.493753179907799
========> pt_272:  4.046266153454781
========> pt_273:  5.251989886164665
========> pt_274:  4.775608405470848
========> pt_275:  4.006717726588249
========> pt_276:  4.434947967529297
========> pt_277:  4.65790294110775
========> pt_278:  4.828057661652565
========> pt_279:  3.5645446181297302
========> pt_280:  3.539402447640896
========> pt_281:  5.060830116271973
========> pt_282:  5.1841869950294495
========> pt_283:  7.151458635926247
========> pt_284:  3.9100652933120728
========> pt_285:  4.175367020070553
========> pt_286:  5.239298716187477
========> pt_287:  4.7616081684827805
========> pt_288:  4.531498700380325
========> pt_289:  4.266398288309574
========> pt_290:  3.8670525327324867
========> pt_291:  5.181154757738113
========> pt_292:  3.8020724803209305
========> pt_293:  4.982193261384964
========> pt_294:  4.828826412558556
========> pt_295:  3.805611915886402
========> pt_296:  4.4028497487306595
========> pt_297:  4.803425297141075
========> pt_298:  4.376557841897011
========> pt_299:  4.570183455944061
========> pt_300:  5.073952078819275
========> pt_301:  3.9314038679003716
========> pt_302:  5.214934423565865
========> pt_303:  4.505766928195953
========> pt_304:  4.903823956847191
========> pt_305:  4.007514901459217
========> pt_306:  4.74274143576622
========> pt_307:  5.070074945688248
========> pt_308:  4.578758627176285
========> pt_309:  4.505729898810387
========> pt_310:  4.127388410270214
========> pt_311:  4.589107036590576
========> pt_312:  4.731200784444809
========> pt_313:  4.757904186844826
========> pt_314:  5.660007297992706
========> pt_315:  5.193377584218979
========> pt_316:  4.574991017580032
========> pt_317:  4.933568462729454
========> pt_318:  5.238886177539825
========> pt_319:  4.693379178643227
========> pt_320:  4.437609910964966
========> pt_321:  3.6568784341216087
========> pt_322:  4.791492968797684
========> pt_323:  7.592753991484642
========> pt_324:  5.26494026184082
========> pt_325:  4.723308831453323
========> pt_326:  5.156719535589218
========> pt_327:  4.534039124846458
========> pt_328:  4.9025654792785645
========> pt_329:  4.9565339833498
========> pt_330:  7.941993772983551
========> pt_331:  4.498358443379402
========> pt_332:  4.969261661171913
========> pt_333:  4.350678995251656
========> pt_334:  4.240831062197685
========> pt_335:  5.248844996094704
========> pt_336:  4.451142325997353
========> pt_337:  5.245577543973923
========> pt_338:  4.848982393741608
========> pt_339:  4.868224635720253
========> pt_340:  3.4257621318101883
===============================================> mean Dose score: 4.735420765355229
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.072664819799,     best is           0.072664819799
            Average val evaluation index is   -4.735420765355,     best is           -4.735420765355
    Train use time   1536.79054
    Train loader use time     75.00247
    Val use time     45.11223
    Total use time   1587.00175
    End lr is 0.000294370163, 0.000294370163
    time: 22:09:17
Epoch: 14, iter: 6999
    Begin lr is 0.000294370163, 0.000294370163
========> pt_241:  4.134092815220356
========> pt_242:  4.6923162788152695
========> pt_243:  5.442646369338036
========> pt_244:  5.016003176569939
========> pt_245:  4.9855780601501465
========> pt_246:  4.838143736124039
========> pt_247:  5.170255601406097
========> pt_248:  4.556514397263527
========> pt_249:  4.683281108736992
========> pt_250:  4.82839822769165
========> pt_251:  4.567694142460823
========> pt_252:  4.189050421118736
========> pt_253:  4.940032437443733
========> pt_254:  4.399458169937134
========> pt_255:  4.175756089389324
========> pt_256:  4.875422939658165
========> pt_257:  4.643614813685417
========> pt_258:  4.382666125893593
========> pt_259:  4.495754390954971
========> pt_260:  7.512884736061096
========> pt_261:  5.290629267692566
========> pt_262:  4.4627466052770615
========> pt_263:  5.52959032356739
========> pt_264:  5.617354661226273
========> pt_265:  4.6751513332128525
========> pt_266:  4.7196173667907715
========> pt_267:  5.55102251470089
========> pt_268:  4.789132475852966
========> pt_269:  4.641791507601738
========> pt_270:  6.015554070472717
========> pt_271:  5.497730448842049
========> pt_272:  4.017130807042122
========> pt_273:  5.1941536366939545
========> pt_274:  4.818656370043755
========> pt_275:  4.095810689032078
========> pt_276:  4.5358603447675705
========> pt_277:  4.769071415066719
========> pt_278:  5.572052597999573
========> pt_279:  4.225650317966938
========> pt_280:  4.195986390113831
========> pt_281:  4.802829176187515
========> pt_282:  5.0331806391477585
========> pt_283:  6.852245554327965
========> pt_284:  3.7103115767240524
========> pt_285:  4.655981063842773
========> pt_286:  5.174256861209869
========> pt_287:  5.272931307554245
========> pt_288:  4.608837962150574
========> pt_289:  4.761810004711151
========> pt_290:  4.587027132511139
========> pt_291:  5.088683515787125
========> pt_292:  4.356324151158333
========> pt_293:  5.153285190463066
========> pt_294:  4.8475199937820435
========> pt_295:  4.0452840924263
========> pt_296:  4.50885497033596
========> pt_297:  5.426192805171013
========> pt_298:  4.544572159647942
========> pt_299:  4.703986272215843
========> pt_300:  5.528659895062447
========> pt_301:  4.2571959644556046
========> pt_302:  5.889695882797241
========> pt_303:  4.334806427359581
========> pt_304:  5.0625501573085785
========> pt_305:  4.46585550904274
========> pt_306:  4.859119579195976
========> pt_307:  5.111665204167366
========> pt_308:  5.0143832713365555
========> pt_309:  4.253687039017677
========> pt_310:  4.579180553555489
========> pt_311:  4.697031006217003
========> pt_312:  4.813018515706062
========> pt_313:  4.753452315926552
========> pt_314:  5.777251198887825
========> pt_315:  5.2092887461185455
========> pt_316:  5.316096618771553
========> pt_317:  4.66081939637661
========> pt_318:  6.411836817860603
========> pt_319:  4.58255909383297
========> pt_320:  4.486800581216812
========> pt_321:  4.103008210659027
========> pt_322:  4.9102915823459625
========> pt_323:  7.788738012313843
========> pt_324:  5.286838188767433
========> pt_325:  4.778587445616722
========> pt_326:  5.201943889260292
========> pt_327:  4.539108499884605
========> pt_328:  5.281577929854393
========> pt_329:  5.049334838986397
========> pt_330:  8.467349484562874
========> pt_331:  4.50902447104454
========> pt_332:  4.572405219078064
========> pt_333:  4.599899277091026
========> pt_334:  4.330167323350906
========> pt_335:  5.4040320217609406
========> pt_336:  4.644058644771576
========> pt_337:  5.590585544705391
========> pt_338:  4.852920547127724
========> pt_339:  4.902153983712196
========> pt_340:  3.830002024769783
===============================================> mean Dose score: 4.93585352525115
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.071173146337,     best is           0.071173146337
            Average val evaluation index is   -4.935853525251,     best is           -4.735420765355
    Train use time   1536.04040
    Train loader use time     72.57803
    Val use time     45.80331
    Total use time   1585.13888
    End lr is 0.000293543203, 0.000293543203
    time: 22:35:43
Epoch: 15, iter: 7499
    Begin lr is 0.000293543203, 0.000293543203
========> pt_241:  4.052827917039394
========> pt_242:  4.550551101565361
========> pt_243:  5.3446801751852036
========> pt_244:  5.224645510315895
========> pt_245:  5.093255341053009
========> pt_246:  4.6950663626194
========> pt_247:  5.212359577417374
========> pt_248:  4.221165589988232
========> pt_249:  4.7581107169389725
========> pt_250:  4.499557465314865
========> pt_251:  4.133557975292206
========> pt_252:  4.163296222686768
========> pt_253:  4.635680094361305
========> pt_254:  4.46394145488739
========> pt_255:  4.119882918894291
========> pt_256:  4.739241898059845
========> pt_257:  4.493585303425789
========> pt_258:  4.30934950709343
========> pt_259:  4.381134882569313
========> pt_260:  6.185877248644829
========> pt_261:  4.761781319975853
========> pt_262:  4.521793872117996
========> pt_263:  5.212165042757988
========> pt_264:  5.433674827218056
========> pt_265:  4.7512102127075195
========> pt_266:  4.531404301524162
========> pt_267:  5.2892424911260605
========> pt_268:  4.813029989600182
========> pt_269:  4.53644186258316
========> pt_270:  6.411439925432205
========> pt_271:  5.355180874466896
========> pt_272:  3.9360638335347176
========> pt_273:  5.182322487235069
========> pt_274:  4.776504412293434
========> pt_275:  4.140364080667496
========> pt_276:  4.419256374239922
========> pt_277:  4.586114436388016
========> pt_278:  5.738132521510124
========> pt_279:  4.185725599527359
========> pt_280:  4.0855832770466805
========> pt_281:  4.82226125895977
========> pt_282:  4.915360435843468
========> pt_283:  6.9807203114032745
========> pt_284:  3.745066523551941
========> pt_285:  4.447852969169617
========> pt_286:  5.366729870438576
========> pt_287:  5.357329621911049
========> pt_288:  4.5439087599515915
========> pt_289:  4.946507886052132
========> pt_290:  4.656205847859383
========> pt_291:  4.8789819329977036
========> pt_292:  4.232613928616047
========> pt_293:  5.088110342621803
========> pt_294:  4.605575725436211
========> pt_295:  4.048110842704773
========> pt_296:  4.389140531420708
========> pt_297:  5.129031985998154
========> pt_298:  4.475100338459015
========> pt_299:  4.602513238787651
========> pt_300:  5.474037900567055
========> pt_301:  4.132118262350559
========> pt_302:  6.048102378845215
========> pt_303:  4.4804393500089645
========> pt_304:  5.002354979515076
========> pt_305:  4.2292870208621025
========> pt_306:  4.723418354988098
========> pt_307:  5.057811439037323
========> pt_308:  4.853212088346481
========> pt_309:  4.162331633269787
========> pt_310:  4.303392991423607
========> pt_311:  4.578529670834541
========> pt_312:  4.6742235124111176
========> pt_313:  5.158540233969688
========> pt_314:  5.5507659167051315
========> pt_315:  4.919926002621651
========> pt_316:  5.493835061788559
========> pt_317:  4.753882065415382
========> pt_318:  5.917665064334869
========> pt_319:  4.618598595261574
========> pt_320:  4.3850985914468765
========> pt_321:  4.204419702291489
========> pt_322:  4.899860769510269
========> pt_323:  7.823559194803238
========> pt_324:  5.1080092042684555
========> pt_325:  4.638282060623169
========> pt_326:  5.20452968776226
========> pt_327:  4.531138315796852
========> pt_328:  5.090240836143494
========> pt_329:  7.191663160920143
========> pt_330:  8.931489884853363
========> pt_331:  4.423949718475342
========> pt_332:  4.520233944058418
========> pt_333:  4.392386078834534
========> pt_334:  4.052314460277557
========> pt_335:  5.6617435067892075
========> pt_336:  4.687492549419403
========> pt_337:  5.503209754824638
========> pt_338:  4.767000898718834
========> pt_339:  4.729040041565895
========> pt_340:  3.7636584043502808
===============================================> mean Dose score: 4.878181146457791
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.069554453857,     best is           0.069554453857
            Average val evaluation index is   -4.878181146458,     best is           -4.735420765355
    Train use time   1538.93520
    Train loader use time     76.02987
    Val use time     46.07317
    Total use time   1588.55187
    End lr is 0.000292660925, 0.000292660925
    time: 23:02:11
Epoch: 16, iter: 7999
    Begin lr is 0.000292660925, 0.000292660925
========> pt_241:  4.3247391283512115
========> pt_242:  4.35248039662838
========> pt_243:  6.139615550637245
========> pt_244:  5.016069412231445
========> pt_245:  5.042370706796646
========> pt_246:  5.132279619574547
========> pt_247:  4.993608742952347
========> pt_248:  4.494133442640305
========> pt_249:  5.3175026923418045
========> pt_250:  4.702596887946129
========> pt_251:  4.659292325377464
========> pt_252:  4.474822357296944
========> pt_253:  5.5479078739881516
========> pt_254:  5.276031345129013
========> pt_255:  4.096039906144142
========> pt_256:  4.64941643178463
========> pt_257:  5.191861987113953
========> pt_258:  4.729480221867561
========> pt_259:  4.346622973680496
========> pt_260:  6.730301529169083
========> pt_261:  5.809091776609421
========> pt_262:  4.531915411353111
========> pt_263:  5.276264473795891
========> pt_264:  5.793917551636696
========> pt_265:  4.791806414723396
========> pt_266:  4.8705799132585526
========> pt_267:  5.553840398788452
========> pt_268:  5.411976128816605
========> pt_269:  4.775552079081535
========> pt_270:  6.585691347718239
========> pt_271:  5.89461974799633
========> pt_272:  3.766384497284889
========> pt_273:  5.183970034122467
========> pt_274:  5.083033666014671
========> pt_275:  4.3349141255021095
========> pt_276:  4.494089633226395
========> pt_277:  4.730412736535072
========> pt_278:  6.494083255529404
========> pt_279:  4.669802933931351
========> pt_280:  4.660132527351379
========> pt_281:  4.895280078053474
========> pt_282:  4.973811060190201
========> pt_283:  6.737672984600067
========> pt_284:  3.832820951938629
========> pt_285:  4.743062183260918
========> pt_286:  5.38480281829834
========> pt_287:  5.758863762021065
========> pt_288:  4.66605618596077
========> pt_289:  5.636232867836952
========> pt_290:  5.312491208314896
========> pt_291:  4.9863301217556
========> pt_292:  4.468319788575172
========> pt_293:  5.566608756780624
========> pt_294:  4.761349484324455
========> pt_295:  4.846154600381851
========> pt_296:  4.543895721435547
========> pt_297:  6.0607121884822845
========> pt_298:  4.599020481109619
========> pt_299:  4.74393367767334
========> pt_300:  5.9435950219631195
========> pt_301:  4.766409993171692
========> pt_302:  6.712363138794899
========> pt_303:  4.616037830710411
========> pt_304:  5.088670998811722
========> pt_305:  4.5709428191185
========> pt_306:  4.928132966160774
========> pt_307:  5.195513293147087
========> pt_308:  5.322357714176178
========> pt_309:  4.300701059401035
========> pt_310:  4.614900350570679
========> pt_311:  4.608767554163933
========> pt_312:  4.842066243290901
========> pt_313:  4.685836136341095
========> pt_314:  5.777821764349937
========> pt_315:  5.0193483382463455
========> pt_316:  5.9935033321380615
========> pt_317:  5.252836346626282
========> pt_318:  6.575866565108299
========> pt_319:  4.585720673203468
========> pt_320:  4.402696415781975
========> pt_321:  4.069994688034058
========> pt_322:  5.08316196501255
========> pt_323:  8.011590763926506
========> pt_324:  5.257445201277733
========> pt_325:  4.972564056515694
========> pt_326:  4.861108213663101
========> pt_327:  4.595692530274391
========> pt_328:  5.21942175924778
========> pt_329:  6.019064038991928
========> pt_330:  7.744457125663757
========> pt_331:  4.817280545830727
========> pt_332:  4.564126804471016
========> pt_333:  4.625106900930405
========> pt_334:  4.180370680987835
========> pt_335:  5.938476622104645
========> pt_336:  5.392986312508583
========> pt_337:  5.449346080422401
========> pt_338:  4.808664172887802
========> pt_339:  5.068314746022224
========> pt_340:  4.432236477732658
===============================================> mean Dose score: 5.113621693477034
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.068225708455,     best is           0.068225708455
            Average val evaluation index is   -5.113621693477,     best is           -4.735420765355
    Train use time   1535.10485
    Train loader use time     74.49715
    Val use time     46.08061
    Total use time   1584.62364
    End lr is 0.000291723667, 0.000291723667
    time: 23:28:36
Epoch: 17, iter: 8499
    Begin lr is 0.000291723667, 0.000291723667
========> pt_241:  4.008681587874889
========> pt_242:  4.09483514726162
========> pt_243:  5.238816291093826
========> pt_244:  4.663025513291359
========> pt_245:  4.636082202196121
========> pt_246:  4.674212038516998
========> pt_247:  4.525879621505737
========> pt_248:  3.8157253712415695
========> pt_249:  4.622953459620476
========> pt_250:  4.290313012897968
========> pt_251:  4.413384348154068
========> pt_252:  4.0200235322117805
========> pt_253:  4.933987781405449
========> pt_254:  4.373876601457596
========> pt_255:  3.680758997797966
========> pt_256:  4.108865894377232
========> pt_257:  4.371139034628868
========> pt_258:  4.028449021279812
========> pt_259:  4.048572927713394
========> pt_260:  6.693473979830742
========> pt_261:  5.027532875537872
========> pt_262:  4.238051511347294
========> pt_263:  4.748440831899643
========> pt_264:  5.208643078804016
========> pt_265:  4.2810749635100365
========> pt_266:  4.45499025285244
========> pt_267:  4.8279716074466705
========> pt_268:  4.714518785476685
========> pt_269:  4.315123222768307
========> pt_270:  6.2456051260232925
========> pt_271:  5.185764655470848
========> pt_272:  3.8127778843045235
========> pt_273:  4.646916687488556
========> pt_274:  4.490263611078262
========> pt_275:  3.8236992061138153
========> pt_276:  4.029381796717644
========> pt_277:  4.16140902787447
========> pt_278:  5.488176345825195
========> pt_279:  4.0545667335391045
========> pt_280:  3.8962063938379288
========> pt_281:  4.455656781792641
========> pt_282:  4.6046411246061325
========> pt_283:  6.437576413154602
========> pt_284:  3.539428785443306
========> pt_285:  4.085346236824989
========> pt_286:  4.709897935390472
========> pt_287:  5.176693499088287
========> pt_288:  4.077510870993137
========> pt_289:  4.684191718697548
========> pt_290:  4.616334065794945
========> pt_291:  4.59961399435997
========> pt_292:  4.001692943274975
========> pt_293:  4.760705903172493
========> pt_294:  4.312154613435268
========> pt_295:  4.000985212624073
========> pt_296:  3.8427018001675606
========> pt_297:  5.032701343297958
========> pt_298:  4.316209852695465
========> pt_299:  4.051217660307884
========> pt_300:  5.637539327144623
========> pt_301:  4.225633107125759
========> pt_302:  5.932760015130043
========> pt_303:  4.145360179245472
========> pt_304:  4.462593272328377
========> pt_305:  4.222952648997307
========> pt_306:  4.261756576597691
========> pt_307:  4.5367855578660965
========> pt_308:  4.848466068506241
========> pt_309:  3.8086704909801483
========> pt_310:  4.72330205142498
========> pt_311:  4.217684827744961
========> pt_312:  4.241295754909515
========> pt_313:  4.345002807676792
========> pt_314:  5.167619213461876
========> pt_315:  4.5239222794771194
========> pt_316:  5.1478011906147
========> pt_317:  4.763473197817802
========> pt_318:  6.410755664110184
========> pt_319:  4.094403050839901
========> pt_320:  3.9993658289313316
========> pt_321:  4.621052443981171
========> pt_322:  4.446927756071091
========> pt_323:  7.896645292639732
========> pt_324:  4.6747299283742905
========> pt_325:  4.500080049037933
========> pt_326:  4.576583281159401
========> pt_327:  4.062465466558933
========> pt_328:  4.610729068517685
========> pt_329:  4.886360168457031
========> pt_330:  7.59472019970417
========> pt_331:  4.531180039048195
========> pt_332:  4.106588587164879
========> pt_333:  3.972613140940666
========> pt_334:  3.744412250816822
========> pt_335:  5.40825754404068
========> pt_336:  4.815945401787758
========> pt_337:  5.027193874120712
========> pt_338:  4.307931438088417
========> pt_339:  4.397451803088188
========> pt_340:  3.9107412099838257
===============================================> mean Dose score: 4.6000918377190825
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.066310426630,     best is           0.066310426630
            Average val evaluation index is   -4.600091837719,     best is           -4.600091837719
    Train use time   1536.93090
    Train loader use time     76.50040
    Val use time     46.29666
    Total use time   1588.22462
    End lr is 0.000290731791, 0.000290731791
    time: 23:55:04
Epoch: 18, iter: 8999
    Begin lr is 0.000290731791, 0.000290731791
========> pt_241:  3.7837424129247665
========> pt_242:  4.000473581254482
========> pt_243:  4.94463711977005
========> pt_244:  4.409785196185112
========> pt_245:  4.760871231555939
========> pt_246:  4.544102773070335
========> pt_247:  4.322023466229439
========> pt_248:  3.9044642075896263
========> pt_249:  4.326613284647465
========> pt_250:  4.072212539613247
========> pt_251:  4.374983310699463
========> pt_252:  4.062418006360531
========> pt_253:  4.6953292191028595
========> pt_254:  4.107253812253475
========> pt_255:  3.642640896141529
========> pt_256:  4.03848372399807
========> pt_257:  4.189683571457863
========> pt_258:  3.786083869636059
========> pt_259:  3.8202596455812454
========> pt_260:  6.223843842744827
========> pt_261:  4.718427211046219
========> pt_262:  4.152787700295448
========> pt_263:  4.573863968253136
========> pt_264:  5.018046051263809
========> pt_265:  4.137286990880966
========> pt_266:  4.483449682593346
========> pt_267:  4.920425117015839
========> pt_268:  4.554344788193703
========> pt_269:  4.160302318632603
========> pt_270:  6.3044870644807816
========> pt_271:  4.667247906327248
========> pt_272:  3.5267099738121033
========> pt_273:  4.491492360830307
========> pt_274:  4.4885581731796265
========> pt_275:  4.215063564479351
========> pt_276:  3.8819482550024986
========> pt_277:  4.092384427785873
========> pt_278:  5.037820264697075
========> pt_279:  4.313515834510326
========> pt_280:  3.7604741379618645
========> pt_281:  4.250423759222031
========> pt_282:  4.486973732709885
========> pt_283:  5.964619368314743
========> pt_284:  3.4938278794288635
========> pt_285:  4.030233733355999
========> pt_286:  4.714408218860626
========> pt_287:  5.169848278164864
========> pt_288:  4.008693844079971
========> pt_289:  4.369610659778118
========> pt_290:  4.558500424027443
========> pt_291:  4.282308667898178
========> pt_292:  4.004759080708027
========> pt_293:  4.512260630726814
========> pt_294:  4.103504195809364
========> pt_295:  3.8684995472431183
========> pt_296:  3.7770917266607285
========> pt_297:  4.9247095733881
========> pt_298:  4.154875427484512
========> pt_299:  4.100004397332668
========> pt_300:  5.581221282482147
========> pt_301:  4.018871188163757
========> pt_302:  5.39275735616684
========> pt_303:  3.923702798783779
========> pt_304:  4.35112439095974
========> pt_305:  4.1299473494291306
========> pt_306:  4.173670969903469
========> pt_307:  4.482403993606567
========> pt_308:  4.630128815770149
========> pt_309:  3.8189730048179626
========> pt_310:  4.532002508640289
========> pt_311:  4.0000931173563
========> pt_312:  4.192864969372749
========> pt_313:  4.146920889616013
========> pt_314:  5.2336446940898895
========> pt_315:  4.708450138568878
========> pt_316:  4.870283156633377
========> pt_317:  4.902425184845924
========> pt_318:  5.731795281171799
========> pt_319:  3.986876495182514
========> pt_320:  3.9094464853405952
========> pt_321:  4.190009273588657
========> pt_322:  4.433829262852669
========> pt_323:  8.176339194178581
========> pt_324:  4.764791131019592
========> pt_325:  4.545635059475899
========> pt_326:  4.336537681519985
========> pt_327:  3.9490071684122086
========> pt_328:  4.685104414820671
========> pt_329:  5.418086498975754
========> pt_330:  7.189073711633682
========> pt_331:  4.111623279750347
========> pt_332:  4.09140445291996
========> pt_333:  3.9725396037101746
========> pt_334:  3.6743636056780815
========> pt_335:  5.211689919233322
========> pt_336:  4.522227793931961
========> pt_337:  4.8818545788526535
========> pt_338:  4.204059317708015
========> pt_339:  4.260219074785709
========> pt_340:  3.7016096711158752
===============================================> mean Dose score: 4.473213034123182
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.064492708892,     best is           0.064492708892
            Average val evaluation index is   -4.473213034123,     best is           -4.473213034123
    Train use time   1537.20249
    Train loader use time     77.31250
    Val use time     46.03261
    Total use time   1588.50035
    End lr is 0.000289685679, 0.000289685679
    time: 00:21:33
Epoch: 19, iter: 9499
    Begin lr is 0.000289685679, 0.000289685679
========> pt_241:  3.8678330183029175
========> pt_242:  4.158208332955837
========> pt_243:  5.059160143136978
========> pt_244:  4.53502431511879
========> pt_245:  4.675242081284523
========> pt_246:  4.44738045334816
========> pt_247:  4.711068272590637
========> pt_248:  3.753827102482319
========> pt_249:  4.315624944865704
========> pt_250:  4.436642453074455
========> pt_251:  4.053639955818653
========> pt_252:  4.028954654932022
========> pt_253:  4.88001249730587
========> pt_254:  4.1233110055327415
========> pt_255:  3.5620125383138657
========> pt_256:  4.594489336013794
========> pt_257:  4.364968687295914
========> pt_258:  3.8100048527121544
========> pt_259:  3.630911707878113
========> pt_260:  7.021150141954422
========> pt_261:  4.704871848225594
========> pt_262:  4.019348658621311
========> pt_263:  4.947133734822273
========> pt_264:  4.994178786873817
========> pt_265:  4.169008657336235
========> pt_266:  4.535861909389496
========> pt_267:  4.984482824802399
========> pt_268:  4.27865918725729
========> pt_269:  4.232723452150822
========> pt_270:  5.780234932899475
========> pt_271:  5.128001421689987
========> pt_272:  3.6454832926392555
========> pt_273:  4.449845775961876
========> pt_274:  4.230497516691685
========> pt_275:  3.7889818102121353
========> pt_276:  4.036369398236275
========> pt_277:  3.9696312323212624
========> pt_278:  5.135574713349342
========> pt_279:  3.940747268497944
========> pt_280:  4.122599624097347
========> pt_281:  4.079135209321976
========> pt_282:  4.462771117687225
========> pt_283:  6.350869759917259
========> pt_284:  3.308812640607357
========> pt_285:  3.983481526374817
========> pt_286:  4.690999910235405
========> pt_287:  5.023164972662926
========> pt_288:  4.186503738164902
========> pt_289:  4.388773888349533
========> pt_290:  4.284133017063141
========> pt_291:  4.5128703117370605
========> pt_292:  3.876042328774929
========> pt_293:  4.492709636688232
========> pt_294:  4.442507177591324
========> pt_295:  3.6554598435759544
========> pt_296:  3.8844406977295876
========> pt_297:  4.980698004364967
========> pt_298:  3.946082890033722
========> pt_299:  4.271778240799904
========> pt_300:  5.340982973575592
========> pt_301:  4.013652391731739
========> pt_302:  5.494550094008446
========> pt_303:  3.817259483039379
========> pt_304:  4.511694759130478
========> pt_305:  4.1011616960167885
========> pt_306:  4.281200133264065
========> pt_307:  4.602812081575394
========> pt_308:  4.364390559494495
========> pt_309:  3.761572241783142
========> pt_310:  4.449650198221207
========> pt_311:  4.1058433055877686
========> pt_312:  4.209285154938698
========> pt_313:  4.072269909083843
========> pt_314:  5.263496115803719
========> pt_315:  4.65679831802845
========> pt_316:  4.596521258354187
========> pt_317:  4.731815159320831
========> pt_318:  5.734874978661537
========> pt_319:  3.9583933353424072
========> pt_320:  3.734908737242222
========> pt_321:  4.389465972781181
========> pt_322:  4.494810923933983
========> pt_323:  8.2463289052248
========> pt_324:  4.696432799100876
========> pt_325:  4.440569654107094
========> pt_326:  4.572825580835342
========> pt_327:  4.643981978297234
========> pt_328:  4.764979928731918
========> pt_329:  5.499612167477608
========> pt_330:  7.070488929748535
========> pt_331:  4.541258290410042
========> pt_332:  4.141242876648903
========> pt_333:  4.0255070105195045
========> pt_334:  3.7881340458989143
========> pt_335:  4.937567636370659
========> pt_336:  4.515778422355652
========> pt_337:  4.81024444103241
========> pt_338:  4.193070977926254
========> pt_339:  4.545962065458298
========> pt_340:  3.5779064893722534
===============================================> mean Dose score: 4.496442394331098
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.063466419116,     best is           0.063466419116
            Average val evaluation index is   -4.496442394331,     best is           -4.473213034123
    Train use time   1539.34902
    Train loader use time     80.37955
    Val use time     45.43103
    Total use time   1588.08604
    End lr is 0.000288585736, 0.000288585736
    time: 00:48:01
Epoch: 20, iter: 9999
    Begin lr is 0.000288585736, 0.000288585736
========> pt_241:  3.6544078961014748
========> pt_242:  3.7289485707879066
========> pt_243:  5.145241990685463
========> pt_244:  4.272746220231056
========> pt_245:  4.798668324947357
========> pt_246:  4.523823708295822
========> pt_247:  4.215930104255676
========> pt_248:  3.717253804206848
========> pt_249:  4.6472640335559845
========> pt_250:  4.0421707555651665
========> pt_251:  4.198692925274372
========> pt_252:  4.110584631562233
========> pt_253:  4.906943291425705
========> pt_254:  4.4884757697582245
========> pt_255:  3.547288663685322
========> pt_256:  3.7876636162400246
========> pt_257:  4.110418260097504
========> pt_258:  3.8131025433540344
========> pt_259:  3.689902648329735
========> pt_260:  6.724649593234062
========> pt_261:  5.182122737169266
========> pt_262:  4.145652242004871
========> pt_263:  4.413042217493057
========> pt_264:  5.214563608169556
========> pt_265:  4.001898430287838
========> pt_266:  4.340193159878254
========> pt_267:  4.815683588385582
========> pt_268:  4.892385005950928
========> pt_269:  4.049967788159847
========> pt_270:  6.572733670473099
========> pt_271:  4.78875957429409
========> pt_272:  3.4940367564558983
========> pt_273:  4.49001744389534
========> pt_274:  4.391068667173386
========> pt_275:  3.9940111711621284
========> pt_276:  3.7328963726758957
========> pt_277:  4.044588096439838
========> pt_278:  5.632728114724159
========> pt_279:  4.043530933558941
========> pt_280:  3.782385364174843
========> pt_281:  4.090415351092815
========> pt_282:  4.384041428565979
========> pt_283:  6.340092122554779
========> pt_284:  3.275218643248081
========> pt_285:  3.8858191296458244
========> pt_286:  4.760264158248901
========> pt_287:  5.288247913122177
========> pt_288:  3.8037826120853424
========> pt_289:  4.451494365930557
========> pt_290:  4.94852676987648
========> pt_291:  4.129365831613541
========> pt_292:  4.042603634297848
========> pt_293:  4.727247506380081
========> pt_294:  3.852858543395996
========> pt_295:  4.428615421056747
========> pt_296:  3.7048249691724777
========> pt_297:  5.057720169425011
========> pt_298:  4.252408482134342
========> pt_299:  4.043132476508617
========> pt_300:  5.6228965520858765
========> pt_301:  4.256649911403656
========> pt_302:  5.762322619557381
========> pt_303:  3.8396544381976128
========> pt_304:  4.352845996618271
========> pt_305:  4.416768103837967
========> pt_306:  4.096729643642902
========> pt_307:  4.3054423853755
========> pt_308:  4.620767161250114
========> pt_309:  3.6886942386627197
========> pt_310:  4.647095054388046
========> pt_311:  3.8892842456698418
========> pt_312:  4.104982241988182
========> pt_313:  3.9448650926351547
========> pt_314:  5.089865326881409
========> pt_315:  4.57615040242672
========> pt_316:  5.477685555815697
========> pt_317:  4.961428120732307
========> pt_318:  6.223483979701996
========> pt_319:  3.912675343453884
========> pt_320:  3.701963797211647
========> pt_321:  3.8409533351659775
========> pt_322:  4.504782259464264
========> pt_323:  8.297663629055023
========> pt_324:  4.563680365681648
========> pt_325:  4.5212530344724655
========> pt_326:  4.153874069452286
========> pt_327:  3.8478752225637436
========> pt_328:  4.473490864038467
========> pt_329:  6.044898554682732
========> pt_330:  7.32914075255394
========> pt_331:  4.146035313606262
========> pt_332:  3.9113162085413933
========> pt_333:  3.8325755670666695
========> pt_334:  3.524760715663433
========> pt_335:  5.420308783650398
========> pt_336:  4.970368891954422
========> pt_337:  4.947429969906807
========> pt_338:  4.19920090585947
========> pt_339:  4.202374741435051
========> pt_340:  3.9115548133850098
===============================================> mean Dose score: 4.497489100322127
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.062454197913,     best is           0.062454197913
            Average val evaluation index is   -4.497489100322,     best is           -4.473213034123
    Train use time   1538.99125
    Train loader use time     80.53099
    Val use time     45.30725
    Total use time   1587.63395
    End lr is 0.000287432384, 0.000287432384
    time: 01:14:28
Epoch: 21, iter: 10499
    Begin lr is 0.000287432384, 0.000287432384
========> pt_241:  3.648614101111889
========> pt_242:  3.679168038070202
========> pt_243:  4.868243932723999
========> pt_244:  4.101322069764137
========> pt_245:  4.382134675979614
========> pt_246:  4.417787194252014
========> pt_247:  4.083267897367477
========> pt_248:  3.7297775596380234
========> pt_249:  3.9224975183606148
========> pt_250:  3.8647308945655823
========> pt_251:  4.242673926055431
========> pt_252:  3.7632907181978226
========> pt_253:  4.618889093399048
========> pt_254:  3.971236012876034
========> pt_255:  3.557301200926304
========> pt_256:  3.8602159172296524
========> pt_257:  4.415819942951202
========> pt_258:  3.4921886771917343
========> pt_259:  3.7587762624025345
========> pt_260:  7.085548415780067
========> pt_261:  4.656169340014458
========> pt_262:  4.037628918886185
========> pt_263:  4.337482191622257
========> pt_264:  4.670834541320801
========> pt_265:  3.728654682636261
========> pt_266:  4.22459602355957
========> pt_267:  4.627468436956406
========> pt_268:  4.121524728834629
========> pt_269:  3.847644180059433
========> pt_270:  5.660643577575684
========> pt_271:  5.016085058450699
========> pt_272:  3.890346884727478
========> pt_273:  4.14806853979826
========> pt_274:  4.050513841211796
========> pt_275:  4.0193601325154305
========> pt_276:  3.649330958724022
========> pt_277:  3.8242702931165695
========> pt_278:  4.767727926373482
========> pt_279:  3.8082146644592285
========> pt_280:  3.8518498837947845
========> pt_281:  3.963135704398155
========> pt_282:  4.071937687695026
========> pt_283:  6.330571398139
========> pt_284:  3.26114721596241
========> pt_285:  3.846946097910404
========> pt_286:  4.425625428557396
========> pt_287:  4.810303896665573
========> pt_288:  3.756702095270157
========> pt_289:  3.9943311363458633
========> pt_290:  4.2353517562150955
========> pt_291:  4.154669940471649
========> pt_292:  3.5004639625549316
========> pt_293:  3.9939798787236214
========> pt_294:  3.8549217581748962
========> pt_295:  3.7212741002440453
========> pt_296:  3.409532569348812
========> pt_297:  4.769916832447052
========> pt_298:  4.178944788873196
========> pt_299:  3.9223121106624603
========> pt_300:  5.954523384571075
========> pt_301:  4.08655621111393
========> pt_302:  5.169687122106552
========> pt_303:  3.653416708111763
========> pt_304:  4.074648134410381
========> pt_305:  3.997621014714241
========> pt_306:  3.8525623083114624
========> pt_307:  4.13628563284874
========> pt_308:  4.168378114700317
========> pt_309:  3.4925349801778793
========> pt_310:  4.410998821258545
========> pt_311:  3.695714958012104
========> pt_312:  3.789656162261963
========> pt_313:  3.842833749949932
========> pt_314:  4.825180843472481
========> pt_315:  4.469662234187126
========> pt_316:  4.420744329690933
========> pt_317:  5.358250662684441
========> pt_318:  5.9005361050367355
========> pt_319:  3.5550744831562042
========> pt_320:  3.6439911648631096
========> pt_321:  3.6997996643185616
========> pt_322:  3.9634820073843002
========> pt_323:  7.759742438793182
========> pt_324:  4.31307852268219
========> pt_325:  4.341029450297356
========> pt_326:  4.279311373829842
========> pt_327:  4.018344171345234
========> pt_328:  4.293190352618694
========> pt_329:  5.160830840468407
========> pt_330:  7.633548900485039
========> pt_331:  4.259493872523308
========> pt_332:  3.7078167870640755
========> pt_333:  3.6714669689536095
========> pt_334:  3.302652984857559
========> pt_335:  5.011548697948456
========> pt_336:  4.57274004817009
========> pt_337:  4.4340211898088455
========> pt_338:  3.8575080782175064
========> pt_339:  4.004173129796982
========> pt_340:  3.771478906273842
===============================================> mean Dose score: 4.271580827236176
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.061324671485,     best is           0.061324671485
            Average val evaluation index is   -4.271580827236,     best is           -4.271580827236
    Train use time   1536.67435
    Train loader use time     78.38248
    Val use time     44.42368
    Total use time   1586.06469
    End lr is 0.000286226069, 0.000286226069
    time: 01:40:54
Epoch: 22, iter: 10999
    Begin lr is 0.000286226069, 0.000286226069
========> pt_241:  3.8594484701752663
========> pt_242:  3.633415624499321
========> pt_243:  5.192230716347694
========> pt_244:  4.179725274443626
========> pt_245:  4.475110769271851
========> pt_246:  4.4734397530555725
========> pt_247:  3.996601924300194
========> pt_248:  3.5543401539325714
========> pt_249:  4.25846591591835
========> pt_250:  3.868054933845997
========> pt_251:  4.439431130886078
========> pt_252:  3.886576145887375
========> pt_253:  4.618977755308151
========> pt_254:  4.273303747177124
========> pt_255:  3.5468920320272446
========> pt_256:  3.694349303841591
========> pt_257:  4.176362119615078
========> pt_258:  3.657190576195717
========> pt_259:  3.6292000114917755
========> pt_260:  5.83100900053978
========> pt_261:  5.027078613638878
========> pt_262:  3.9615293592214584
========> pt_263:  4.357013627886772
========> pt_264:  4.803938493132591
========> pt_265:  3.8004038110375404
========> pt_266:  4.372348226606846
========> pt_267:  4.580204859375954
========> pt_268:  4.494060426950455
========> pt_269:  3.858805149793625
========> pt_270:  6.198146492242813
========> pt_271:  4.701614826917648
========> pt_272:  3.497544378042221
========> pt_273:  4.088415242731571
========> pt_274:  4.322285279631615
========> pt_275:  3.9571862295269966
========> pt_276:  3.676306866109371
========> pt_277:  3.6901170015335083
========> pt_278:  5.222356468439102
========> pt_279:  4.159002900123596
========> pt_280:  3.8194893300533295
========> pt_281:  3.9396144822239876
========> pt_282:  4.10684309899807
========> pt_283:  5.6909748166799545
========> pt_284:  3.3034853637218475
========> pt_285:  3.879220597445965
========> pt_286:  4.331284984946251
========> pt_287:  5.022428557276726
========> pt_288:  3.7027862668037415
========> pt_289:  4.573124945163727
========> pt_290:  4.5714012533426285
========> pt_291:  3.980068825185299
========> pt_292:  3.9355801045894623
========> pt_293:  4.165525548160076
========> pt_294:  3.9279862120747566
========> pt_295:  3.802943453192711
========> pt_296:  3.4544171392917633
========> pt_297:  4.997400864958763
========> pt_298:  4.093594923615456
========> pt_299:  3.8122592121362686
========> pt_300:  5.569635778665543
========> pt_301:  4.127184227108955
========> pt_302:  5.840336233377457
========> pt_303:  3.7529973313212395
========> pt_304:  4.061252623796463
========> pt_305:  3.9999327436089516
========> pt_306:  3.853852078318596
========> pt_307:  4.168525189161301
========> pt_308:  4.690986350178719
========> pt_309:  3.340842016041279
========> pt_310:  4.3830713629722595
========> pt_311:  3.714417666196823
========> pt_312:  3.791554309427738
========> pt_313:  4.006727635860443
========> pt_314:  4.745849296450615
========> pt_315:  4.184311181306839
========> pt_316:  4.877270236611366
========> pt_317:  4.728085622191429
========> pt_318:  6.091936305165291
========> pt_319:  3.591200038790703
========> pt_320:  3.6363719776272774
========> pt_321:  4.163379147648811
========> pt_322:  3.9662232249975204
========> pt_323:  8.244319409132004
========> pt_324:  4.287760332226753
========> pt_325:  4.383876100182533
========> pt_326:  3.989837020635605
========> pt_327:  3.700711838901043
========> pt_328:  4.407184794545174
========> pt_329:  5.007719025015831
========> pt_330:  6.836468949913979
========> pt_331:  4.070941545069218
========> pt_332:  3.619685024023056
========> pt_333:  3.686193712055683
========> pt_334:  3.3104223757982254
========> pt_335:  5.160617008805275
========> pt_336:  4.482676759362221
========> pt_337:  4.470057561993599
========> pt_338:  3.78363523632288
========> pt_339:  4.100239872932434
========> pt_340:  3.571649305522442
===============================================> mean Dose score: 4.295228520408273
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.060106846690,     best is           0.060106846690
            Average val evaluation index is   -4.295228520408,     best is           -4.271580827236
    Train use time   1535.55581
    Train loader use time     77.78166
    Val use time     44.95166
    Total use time   1583.85219
    End lr is 0.000284967255, 0.000284967255
    time: 02:07:18
Epoch: 23, iter: 11499
    Begin lr is 0.000284967255, 0.000284967255
========> pt_241:  3.47680427134037
========> pt_242:  3.5781213641166687
========> pt_243:  4.960980638861656
========> pt_244:  3.924124203622341
========> pt_245:  4.155936762690544
========> pt_246:  4.1853829473257065
========> pt_247:  3.813772462308407
========> pt_248:  3.3967913314700127
========> pt_249:  4.3165819719433784
========> pt_250:  3.7537819892168045
========> pt_251:  4.0205007418990135
========> pt_252:  3.796708956360817
========> pt_253:  4.464080184698105
========> pt_254:  4.241594076156616
========> pt_255:  3.4392637759447098
========> pt_256:  3.771726116538048
========> pt_257:  4.137879200279713
========> pt_258:  3.573470525443554
========> pt_259:  3.6179081350564957
========> pt_260:  5.860938653349876
========> pt_261:  4.43828321993351
========> pt_262:  4.103355817496777
========> pt_263:  4.522938132286072
========> pt_264:  4.409613087773323
========> pt_265:  3.9324041828513145
========> pt_266:  4.228492192924023
========> pt_267:  4.448861628770828
========> pt_268:  4.1849372908473015
========> pt_269:  3.7734174728393555
========> pt_270:  5.652508586645126
========> pt_271:  4.479655474424362
========> pt_272:  3.7934057787060738
========> pt_273:  4.01008740067482
========> pt_274:  3.9003289118409157
========> pt_275:  3.8457809761166573
========> pt_276:  3.5480083897709846
========> pt_277:  3.65975733846426
========> pt_278:  5.095633566379547
========> pt_279:  3.5217368230223656
========> pt_280:  3.459221050143242
========> pt_281:  3.90056986361742
========> pt_282:  3.9269905909895897
========> pt_283:  5.6119634956121445
========> pt_284:  3.0101140588521957
========> pt_285:  3.6797597259283066
========> pt_286:  4.316049218177795
========> pt_287:  4.759596064686775
========> pt_288:  3.6236969754099846
========> pt_289:  4.438165873289108
========> pt_290:  4.576616659760475
========> pt_291:  4.009785428643227
========> pt_292:  3.7161455303430557
========> pt_293:  4.095685258507729
========> pt_294:  3.764835000038147
========> pt_295:  3.517218977212906
========> pt_296:  3.525760769844055
========> pt_297:  4.897921159863472
========> pt_298:  3.9712584391236305
========> pt_299:  3.7681780755519867
========> pt_300:  5.333233401179314
========> pt_301:  3.856346346437931
========> pt_302:  5.612822994589806
========> pt_303:  3.6926214396953583
========> pt_304:  3.885238915681839
========> pt_305:  3.847852274775505
========> pt_306:  3.767654709517956
========> pt_307:  4.1606637462973595
========> pt_308:  4.215434640645981
========> pt_309:  3.3324532955884933
========> pt_310:  3.980059176683426
========> pt_311:  3.545950911939144
========> pt_312:  3.692832663655281
========> pt_313:  3.5014089941978455
========> pt_314:  4.709243923425674
========> pt_315:  4.125172384083271
========> pt_316:  4.486979991197586
========> pt_317:  4.5349085330963135
========> pt_318:  5.524296686053276
========> pt_319:  3.569526895880699
========> pt_320:  3.3480624854564667
========> pt_321:  4.330888092517853
========> pt_322:  3.9778225496411324
========> pt_323:  8.194155544042587
========> pt_324:  4.114313647150993
========> pt_325:  4.0643951669335365
========> pt_326:  3.819839023053646
========> pt_327:  3.750295750796795
========> pt_328:  4.141305722296238
========> pt_329:  5.226967930793762
========> pt_330:  7.0140717923641205
========> pt_331:  4.079790785908699
========> pt_332:  3.5751191154122353
========> pt_333:  3.4919876232743263
========> pt_334:  3.232574872672558
========> pt_335:  5.142049640417099
========> pt_336:  4.366125725209713
========> pt_337:  4.1687919571995735
========> pt_338:  3.7558558955788612
========> pt_339:  3.901638500392437
========> pt_340:  3.7103937193751335
===============================================> mean Dose score: 4.153862342610955
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.059277838260,     best is           0.059277838260
            Average val evaluation index is   -4.153862342611,     best is           -4.153862342611
    Train use time   1536.54391
    Train loader use time     78.29574
    Val use time     44.60655
    Total use time   1586.19906
    End lr is 0.000283656428, 0.000283656428
    time: 02:33:44
Epoch: 24, iter: 11999
    Begin lr is 0.000283656428, 0.000283656428
========> pt_241:  3.6553800478577614
========> pt_242:  3.3926070109009743
========> pt_243:  5.053811743855476
========> pt_244:  4.1505734995007515
========> pt_245:  4.610959067940712
========> pt_246:  4.151582419872284
========> pt_247:  3.9160987362265587
========> pt_248:  3.1331556662917137
========> pt_249:  4.801885709166527
========> pt_250:  3.596769832074642
========> pt_251:  4.355024993419647
========> pt_252:  4.218415766954422
========> pt_253:  4.618640318512917
========> pt_254:  4.602238908410072
========> pt_255:  3.382161855697632
========> pt_256:  3.4668509289622307
========> pt_257:  4.105951525270939
========> pt_258:  3.5263362899422646
========> pt_259:  3.3649059012532234
========> pt_260:  5.977260991930962
========> pt_261:  5.292572006583214
========> pt_262:  4.043154902756214
========> pt_263:  3.9844727143645287
========> pt_264:  4.929767474532127
========> pt_265:  3.787486292421818
========> pt_266:  4.305380322039127
========> pt_267:  4.427657350897789
========> pt_268:  4.620714485645294
========> pt_269:  3.8322097063064575
========> pt_270:  6.390581429004669
========> pt_271:  4.410120025277138
========> pt_272:  3.4471330419182777
========> pt_273:  4.184701554477215
========> pt_274:  4.489777535200119
========> pt_275:  3.567960187792778
========> pt_276:  3.5588081926107407
========> pt_277:  3.667495436966419
========> pt_278:  5.230110213160515
========> pt_279:  3.9756591990590096
========> pt_280:  3.673458732664585
========> pt_281:  4.023808091878891
========> pt_282:  4.0577588230371475
========> pt_283:  5.989713296294212
========> pt_284:  3.2616471126675606
========> pt_285:  3.449080213904381
========> pt_286:  4.415478333830833
========> pt_287:  5.409087836742401
========> pt_288:  3.564209006726742
========> pt_289:  4.7678593546152115
========> pt_290:  5.044317618012428
========> pt_291:  3.7227148562669754
========> pt_292:  3.893877975642681
========> pt_293:  4.48320247232914
========> pt_294:  3.6097029969096184
========> pt_295:  3.6528537049889565
========> pt_296:  3.4861302003264427
========> pt_297:  5.397010520100594
========> pt_298:  3.8789621740579605
========> pt_299:  3.5604875534772873
========> pt_300:  5.014399960637093
========> pt_301:  4.057057872414589
========> pt_302:  5.785015374422073
========> pt_303:  3.919210769236088
========> pt_304:  4.06451903283596
========> pt_305:  3.972947709262371
========> pt_306:  3.8155044987797737
========> pt_307:  4.006841070950031
========> pt_308:  4.813815951347351
========> pt_309:  3.2879452779889107
========> pt_310:  4.143780432641506
========> pt_311:  3.4632640331983566
========> pt_312:  3.74959297478199
========> pt_313:  3.861551582813263
========> pt_314:  4.769288897514343
========> pt_315:  4.169965423643589
========> pt_316:  5.164715796709061
========> pt_317:  4.274948686361313
========> pt_318:  5.825238674879074
========> pt_319:  3.699604608118534
========> pt_320:  3.4826478734612465
========> pt_321:  3.9422477409243584
========> pt_322:  4.3083807453513145
========> pt_323:  8.526226729154587
========> pt_324:  4.313650391995907
========> pt_325:  4.495652690529823
========> pt_326:  3.8071144744753838
========> pt_327:  3.668248802423477
========> pt_328:  3.9792583510279655
========> pt_329:  6.121742352843285
========> pt_330:  6.410442218184471
========> pt_331:  4.100765585899353
========> pt_332:  3.652414306998253
========> pt_333:  3.5195719078183174
========> pt_334:  3.057771399617195
========> pt_335:  5.508763641119003
========> pt_336:  4.405078291893005
========> pt_337:  4.545198529958725
========> pt_338:  3.8606250658631325
========> pt_339:  3.896777220070362
========> pt_340:  3.6236268281936646
===============================================> mean Dose score: 4.266531479358673
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.058992876165,     best is           0.058992876165
            Average val evaluation index is   -4.266531479359,     best is           -4.153862342611
    Train use time   1534.40826
    Train loader use time     77.40680
    Val use time     44.74802
    Total use time   1582.70551
    End lr is 0.000282294094, 0.000282294094
    time: 03:00:07
Epoch: 25, iter: 12499
    Begin lr is 0.000282294094, 0.000282294094
========> pt_241:  3.4877104684710503
========> pt_242:  3.4379906952381134
========> pt_243:  4.389350190758705
========> pt_244:  3.7726033478975296
========> pt_245:  4.212873093783855
========> pt_246:  3.9281491935253143
========> pt_247:  3.5979552939534187
========> pt_248:  3.1188886612653732
========> pt_249:  4.138073734939098
========> pt_250:  3.4200095385313034
========> pt_251:  3.9916428551077843
========> pt_252:  4.033034406602383
========> pt_253:  3.992641866207123
========> pt_254:  3.8529492914676666
========> pt_255:  3.4772319346666336
========> pt_256:  3.3214472234249115
========> pt_257:  3.8104937970638275
========> pt_258:  3.2789427042007446
========> pt_259:  3.5031013935804367
========> pt_260:  5.283387154340744
========> pt_261:  4.3750933557748795
========> pt_262:  3.914013095200062
========> pt_263:  3.851420395076275
========> pt_264:  4.391953200101852
========> pt_265:  3.5624871402978897
========> pt_266:  4.137336798012257
========> pt_267:  4.368194155395031
========> pt_268:  3.918963558971882
========> pt_269:  3.678043596446514
========> pt_270:  5.749173015356064
========> pt_271:  4.144579172134399
========> pt_272:  3.717941977083683
========> pt_273:  3.811809904873371
========> pt_274:  4.281257763504982
========> pt_275:  4.032685495913029
========> pt_276:  3.3618348091840744
========> pt_277:  3.4300119057297707
========> pt_278:  4.497336745262146
========> pt_279:  3.6075135692954063
========> pt_280:  3.123391903936863
========> pt_281:  3.787628673017025
========> pt_282:  3.8928930461406708
========> pt_283:  5.436234548687935
========> pt_284:  3.4238240867853165
========> pt_285:  3.4922296181321144
========> pt_286:  4.36555489897728
========> pt_287:  4.659588560461998
========> pt_288:  3.4227700531482697
========> pt_289:  3.8576235994696617
========> pt_290:  4.016315378248692
========> pt_291:  3.392396569252014
========> pt_292:  3.5303109511733055
========> pt_293:  3.816852942109108
========> pt_294:  3.4560440853238106
========> pt_295:  3.2323521748185158
========> pt_296:  3.2351992651820183
========> pt_297:  4.483611360192299
========> pt_298:  3.6370614543557167
========> pt_299:  3.528759367763996
========> pt_300:  4.842658191919327
========> pt_301:  3.7183675542473793
========> pt_302:  4.951778054237366
========> pt_303:  3.8687824830412865
========> pt_304:  3.7767579406499863
========> pt_305:  3.9171332120895386
========> pt_306:  3.604421094059944
========> pt_307:  3.961130380630493
========> pt_308:  3.963618390262127
========> pt_309:  3.3734044060111046
========> pt_310:  3.8084428384900093
========> pt_311:  3.2218892872333527
========> pt_312:  3.548896834254265
========> pt_313:  3.687455579638481
========> pt_314:  4.691106304526329
========> pt_315:  4.19395811855793
========> pt_316:  4.248243197798729
========> pt_317:  4.134909808635712
========> pt_318:  4.964092671871185
========> pt_319:  3.4962160140275955
========> pt_320:  3.3090533316135406
========> pt_321:  3.470952846109867
========> pt_322:  3.901556357741356
========> pt_323:  7.894694209098816
========> pt_324:  4.105794541537762
========> pt_325:  4.185824170708656
========> pt_326:  3.8254359364509583
========> pt_327:  3.643432855606079
========> pt_328:  3.872733414173126
========> pt_329:  6.330535411834717
========> pt_330:  7.140558958053589
========> pt_331:  3.944920115172863
========> pt_332:  3.7441496551036835
========> pt_333:  3.3329179883003235
========> pt_334:  3.0312979966402054
========> pt_335:  5.10121613740921
========> pt_336:  3.8391321152448654
========> pt_337:  4.000028446316719
========> pt_338:  3.4862245991826057
========> pt_339:  3.421662822365761
========> pt_340:  3.2070644944906235
===============================================> mean Dose score: 3.9743321780115366
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.057803902566,     best is           0.057803902566
            Average val evaluation index is   -3.974332178012,     best is           -3.974332178012
    Train use time   1532.60781
    Train loader use time     75.93493
    Val use time     44.59296
    Total use time   1582.06939
    End lr is 0.000280880776, 0.000280880776
    time: 03:26:29
Epoch: 26, iter: 12999
    Begin lr is 0.000280880776, 0.000280880776
========> pt_241:  3.3975522592663765
========> pt_242:  3.2373858243227005
========> pt_243:  4.827607572078705
========> pt_244:  3.840675875544548
========> pt_245:  4.661898985505104
========> pt_246:  4.246285073459148
========> pt_247:  3.7531887367367744
========> pt_248:  3.2736055180430412
========> pt_249:  4.5441267639398575
========> pt_250:  3.593216836452484
========> pt_251:  4.160530753433704
========> pt_252:  4.363132081925869
========> pt_253:  4.540059790015221
========> pt_254:  4.260679334402084
========> pt_255:  3.3902238309383392
========> pt_256:  3.5708972439169884
========> pt_257:  3.906012661755085
========> pt_258:  3.4531664848327637
========> pt_259:  3.3437879383563995
========> pt_260:  6.987950429320335
========> pt_261:  4.928235188126564
========> pt_262:  4.139750227332115
========> pt_263:  3.972688242793083
========> pt_264:  5.009067729115486
========> pt_265:  3.5389750450849533
========> pt_266:  4.546280726790428
========> pt_267:  4.679174497723579
========> pt_268:  4.428124129772186
========> pt_269:  3.6872028931975365
========> pt_270:  5.686337798833847
========> pt_271:  4.24184363335371
========> pt_272:  3.175823427736759
========> pt_273:  3.9975500851869583
========> pt_274:  4.493984803557396
========> pt_275:  3.6738279834389687
========> pt_276:  3.4928739815950394
========> pt_277:  3.4375695511698723
========> pt_278:  5.374471619725227
========> pt_279:  3.6952082812786102
========> pt_280:  3.744632862508297
========> pt_281:  3.755314275622368
========> pt_282:  3.8992438465356827
========> pt_283:  6.26543827354908
========> pt_284:  3.2534657046198845
========> pt_285:  3.5630973428487778
========> pt_286:  4.373007453978062
========> pt_287:  5.30759759247303
========> pt_288:  3.4505509585142136
========> pt_289:  4.542241916060448
========> pt_290:  4.544001072645187
========> pt_291:  3.437766171991825
========> pt_292:  3.786178007721901
========> pt_293:  4.445522725582123
========> pt_294:  3.587828539311886
========> pt_295:  3.558514825999737
========> pt_296:  3.3550193160772324
========> pt_297:  5.1922281086444855
========> pt_298:  3.572857975959778
========> pt_299:  3.7628844380378723
========> pt_300:  4.9229446798563
========> pt_301:  4.122461676597595
========> pt_302:  5.705294758081436
========> pt_303:  3.6125631257891655
========> pt_304:  3.977811597287655
========> pt_305:  3.9130891859531403
========> pt_306:  3.812529109418392
========> pt_307:  4.154077209532261
========> pt_308:  4.169446490705013
========> pt_309:  3.2775700092315674
========> pt_310:  4.242224358022213
========> pt_311:  3.374454788863659
========> pt_312:  3.77678245306015
========> pt_313:  3.5707736387848854
========> pt_314:  4.895257130265236
========> pt_315:  4.394796639680862
========> pt_316:  4.836089387536049
========> pt_317:  4.227186255156994
========> pt_318:  5.587688907980919
========> pt_319:  3.5283859446644783
========> pt_320:  3.1792043149471283
========> pt_321:  4.522029608488083
========> pt_322:  4.098022021353245
========> pt_323:  8.632230386137962
========> pt_324:  4.171013720333576
========> pt_325:  4.671694561839104
========> pt_326:  3.9740661531686783
========> pt_327:  3.564000390470028
========> pt_328:  4.216031543910503
========> pt_329:  6.215473115444183
========> pt_330:  6.1890096217393875
========> pt_331:  4.351646713912487
========> pt_332:  3.5379353538155556
========> pt_333:  3.510136194527149
========> pt_334:  3.041159026324749
========> pt_335:  4.798848256468773
========> pt_336:  4.411284625530243
========> pt_337:  4.437651112675667
========> pt_338:  3.6539465934038162
========> pt_339:  3.7926946580410004
========> pt_340:  3.453104943037033
===============================================> mean Dose score: 4.194709755107761
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.057155788183,     best is           0.057155788183
            Average val evaluation index is   -4.194709755108,     best is           -3.974332178012
    Train use time   1533.11744
    Train loader use time     76.26815
    Val use time     44.88122
    Total use time   1581.29225
    End lr is 0.000279417021, 0.000279417021
    time: 03:52:50
Epoch: 27, iter: 13499
    Begin lr is 0.000279417021, 0.000279417021
========> pt_241:  3.258446156978607
========> pt_242:  3.293163552880287
========> pt_243:  4.815520867705345
========> pt_244:  3.671928271651268
========> pt_245:  4.173427931964397
========> pt_246:  4.092813394963741
========> pt_247:  3.570144400000572
========> pt_248:  3.146352730691433
========> pt_249:  4.2070892080664635
========> pt_250:  3.362712040543556
========> pt_251:  3.8663607090711594
========> pt_252:  3.711741119623184
========> pt_253:  4.355416931211948
========> pt_254:  4.130167178809643
========> pt_255:  3.3048726618289948
========> pt_256:  3.270665854215622
========> pt_257:  3.984748087823391
========> pt_258:  3.2933033257722855
========> pt_259:  3.5625439882278442
========> pt_260:  5.721139684319496
========> pt_261:  4.426264315843582
========> pt_262:  3.899155966937542
========> pt_263:  3.916279710829258
========> pt_264:  4.554064720869064
========> pt_265:  3.524113744497299
========> pt_266:  3.8868867233395576
========> pt_267:  4.173563793301582
========> pt_268:  4.31728683412075
========> pt_269:  3.6167286708950996
========> pt_270:  5.761590376496315
========> pt_271:  4.476459473371506
========> pt_272:  3.618147000670433
========> pt_273:  3.782867006957531
========> pt_274:  3.9188728109002113
========> pt_275:  3.5681114345788956
========> pt_276:  3.2383595407009125
========> pt_277:  3.3639606088399887
========> pt_278:  5.051397010684013
========> pt_279:  3.479755148291588
========> pt_280:  3.2932621240615845
========> pt_281:  3.675006404519081
========> pt_282:  3.757745437324047
========> pt_283:  5.63683994114399
========> pt_284:  2.9717208445072174
========> pt_285:  3.4851400554180145
========> pt_286:  4.110904596745968
========> pt_287:  4.70293328166008
========> pt_288:  3.3051394298672676
========> pt_289:  4.150117412209511
========> pt_290:  4.340993203222752
========> pt_291:  3.5764211416244507
========> pt_292:  3.406682088971138
========> pt_293:  4.008334241807461
========> pt_294:  3.4448836371302605
========> pt_295:  3.4464800730347633
========> pt_296:  3.091418854892254
========> pt_297:  4.844494014978409
========> pt_298:  3.7079260498285294
========> pt_299:  3.4186550974845886
========> pt_300:  5.073443055152893
========> pt_301:  3.929864801466465
========> pt_302:  5.616375729441643
========> pt_303:  3.6052560806274414
========> pt_304:  3.7677955254912376
========> pt_305:  3.8514595106244087
========> pt_306:  3.4791308641433716
========> pt_307:  3.815433047711849
========> pt_308:  4.101532772183418
========> pt_309:  3.2056187838315964
========> pt_310:  3.9210909232497215
========> pt_311:  3.203566260635853
========> pt_312:  3.423072285950184
========> pt_313:  3.2960494980216026
========> pt_314:  4.396551102399826
========> pt_315:  3.853813484311104
========> pt_316:  4.70386266708374
========> pt_317:  4.310388416051865
========> pt_318:  5.4015567898750305
========> pt_319:  3.284083791077137
========> pt_320:  3.064492754638195
========> pt_321:  3.9841172844171524
========> pt_322:  3.9037280529737473
========> pt_323:  8.002653643488884
========> pt_324:  3.992200121283531
========> pt_325:  4.113324284553528
========> pt_326:  3.494080565869808
========> pt_327:  3.3848311007022858
========> pt_328:  3.7525884434580803
========> pt_329:  5.981117263436317
========> pt_330:  7.28387676179409
========> pt_331:  3.7372056022286415
========> pt_332:  3.4007420018315315
========> pt_333:  3.23746457695961
========> pt_334:  2.9115530475974083
========> pt_335:  5.168270096182823
========> pt_336:  4.29936982691288
========> pt_337:  4.010808169841766
========> pt_338:  3.4976716339588165
========> pt_339:  3.6330901831388474
========> pt_340:  3.5207106918096542
===============================================> mean Dose score: 3.9835536241531373
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.056877501510,     best is           0.056877501510
            Average val evaluation index is   -3.983553624153,     best is           -3.974332178012
    Train use time   1532.62721
    Train loader use time     76.16672
    Val use time     44.72234
    Total use time   1580.60242
    End lr is 0.000277903393, 0.000277903393
    time: 04:19:11
Epoch: 28, iter: 13999
    Begin lr is 0.000277903393, 0.000277903393
========> pt_241:  3.3066125214099884
========> pt_242:  3.160114623606205
========> pt_243:  4.733764678239822
========> pt_244:  3.602965995669365
========> pt_245:  4.10925704985857
========> pt_246:  4.173993282020092
========> pt_247:  3.6738592758774757
========> pt_248:  2.959304265677929
========> pt_249:  4.231487400829792
========> pt_250:  3.2571206614375114
========> pt_251:  3.7842561304569244
========> pt_252:  3.6205312237143517
========> pt_253:  4.208409748971462
========> pt_254:  3.9288702234625816
========> pt_255:  3.383285515010357
========> pt_256:  3.1180891394615173
========> pt_257:  3.791296146810055
========> pt_258:  3.1643550097942352
========> pt_259:  3.3981989696621895
========> pt_260:  5.4573705047369
========> pt_261:  4.310380332171917
========> pt_262:  3.6768782138824463
========> pt_263:  3.717629835009575
========> pt_264:  4.21159740537405
========> pt_265:  3.3901218697428703
========> pt_266:  3.959328718483448
========> pt_267:  3.98534394800663
========> pt_268:  4.354867748916149
========> pt_269:  3.522160053253174
========> pt_270:  5.7974207401275635
========> pt_271:  4.325718320906162
========> pt_272:  3.48385576158762
========> pt_273:  3.628702722489834
========> pt_274:  3.85450791567564
========> pt_275:  3.7095660343766212
========> pt_276:  3.159589432179928
========> pt_277:  3.1915828213095665
========> pt_278:  4.653873518109322
========> pt_279:  3.169424645602703
========> pt_280:  3.3023058995604515
========> pt_281:  3.4291818737983704
========> pt_282:  3.6944152787327766
========> pt_283:  5.463018789887428
========> pt_284:  3.081045150756836
========> pt_285:  3.2853034138679504
========> pt_286:  4.047017693519592
========> pt_287:  4.681509956717491
========> pt_288:  3.194672428071499
========> pt_289:  4.026074968278408
========> pt_290:  4.295159950852394
========> pt_291:  3.5254955664277077
========> pt_292:  3.2996945455670357
========> pt_293:  3.8301558792591095
========> pt_294:  3.430771790444851
========> pt_295:  3.506954275071621
========> pt_296:  2.9666227847337723
========> pt_297:  4.5834410190582275
========> pt_298:  3.780570402741432
========> pt_299:  3.3054205402731895
========> pt_300:  5.262425914406776
========> pt_301:  3.7481167539954185
========> pt_302:  5.298418998718262
========> pt_303:  3.5266664251685143
========> pt_304:  3.69465883821249
========> pt_305:  3.7366240844130516
========> pt_306:  3.4408727288246155
========> pt_307:  3.699195981025696
========> pt_308:  3.936825282871723
========> pt_309:  2.970391698181629
========> pt_310:  4.0904101356863976
========> pt_311:  3.241737298667431
========> pt_312:  3.3506519347429276
========> pt_313:  3.214770518243313
========> pt_314:  4.184091351926327
========> pt_315:  3.7221049144864082
========> pt_316:  4.490437805652618
========> pt_317:  4.6743132174015045
========> pt_318:  5.4535554349422455
========> pt_319:  3.218432515859604
========> pt_320:  3.0001114308834076
========> pt_321:  4.065835662186146
========> pt_322:  3.8307715579867363
========> pt_323:  7.903206795454025
========> pt_324:  3.7192129716277122
========> pt_325:  4.120111875236034
========> pt_326:  3.4798182547092438
========> pt_327:  3.3176089450716972
========> pt_328:  3.57648815959692
========> pt_329:  6.199410185217857
========> pt_330:  7.399963885545731
========> pt_331:  3.5598864778876305
========> pt_332:  3.2796699926257133
========> pt_333:  3.0837253481149673
========> pt_334:  2.7760831266641617
========> pt_335:  5.124979615211487
========> pt_336:  4.41090390086174
========> pt_337:  4.038958325982094
========> pt_338:  3.354550451040268
========> pt_339:  3.593837209045887
========> pt_340:  3.351757861673832
===============================================> mean Dose score: 3.9003611847758295
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.055422467619,     best is           0.055422467619
            Average val evaluation index is   -3.900361184776,     best is           -3.900361184776
    Train use time   1532.23643
    Train loader use time     75.97146
    Val use time     44.61147
    Total use time   1581.60525
    End lr is 0.000276340474, 0.000276340474
    time: 04:45:33
Epoch: 29, iter: 14499
    Begin lr is 0.000276340474, 0.000276340474
========> pt_241:  3.74755322933197
========> pt_242:  3.7382064387202263
========> pt_243:  4.371374770998955
========> pt_244:  3.893568702042103
========> pt_245:  4.003432281315327
========> pt_246:  4.0266139805316925
========> pt_247:  3.776763416826725
========> pt_248:  3.1871382519602776
========> pt_249:  4.034448824822903
========> pt_250:  4.012096635997295
========> pt_251:  3.367784544825554
========> pt_252:  3.4926583245396614
========> pt_253:  4.690666645765305
========> pt_254:  3.931078426539898
========> pt_255:  3.4657512605190277
========> pt_256:  3.5107463970780373
========> pt_257:  4.300613179802895
========> pt_258:  3.7354660034179688
========> pt_259:  3.325084187090397
========> pt_260:  6.668861955404282
========> pt_261:  4.1042956337332726
========> pt_262:  3.7960106134414673
========> pt_263:  4.183313995599747
========> pt_264:  4.037229157984257
========> pt_265:  3.7952322140336037
========> pt_266:  3.8891173526644707
========> pt_267:  4.213081710040569
========> pt_268:  3.9851345494389534
========> pt_269:  3.615468628704548
========> pt_270:  5.329559147357941
========> pt_271:  4.917943626642227
========> pt_272:  4.108370430767536
========> pt_273:  3.698713816702366
========> pt_274:  3.76592293381691
========> pt_275:  3.9187877997756004
========> pt_276:  3.2472463324666023
========> pt_277:  3.2021724432706833
========> pt_278:  4.77771021425724
========> pt_279:  3.722258508205414
========> pt_280:  3.4455321729183197
========> pt_281:  3.354959338903427
========> pt_282:  3.6706379801034927
========> pt_283:  6.519844233989716
========> pt_284:  3.1758732348680496
========> pt_285:  3.257681578397751
========> pt_286:  4.218728952109814
========> pt_287:  4.595120400190353
========> pt_288:  3.4482548758387566
========> pt_289:  4.093720875680447
========> pt_290:  4.177937433123589
========> pt_291:  3.8985728845000267
========> pt_292:  3.572656400501728
========> pt_293:  3.6401187255978584
========> pt_294:  3.9303310588002205
========> pt_295:  3.7497880309820175
========> pt_296:  3.271491192281246
========> pt_297:  4.457854554057121
========> pt_298:  3.6056923493742943
========> pt_299:  3.6584753915667534
========> pt_300:  4.848111942410469
========> pt_301:  4.019803963601589
========> pt_302:  4.914542660117149
========> pt_303:  3.4426485747098923
========> pt_304:  3.7671342119574547
========> pt_305:  4.3829164654016495
========> pt_306:  3.4801043197512627
========> pt_307:  3.6403466388583183
========> pt_308:  4.316539727151394
========> pt_309:  3.338148519396782
========> pt_310:  4.209602773189545
========> pt_311:  3.3350536972284317
========> pt_312:  3.4545279666781425
========> pt_313:  3.2727744430303574
========> pt_314:  4.375433400273323
========> pt_315:  3.894773982465267
========> pt_316:  4.15080975741148
========> pt_317:  4.699897393584251
========> pt_318:  5.4472416639328
========> pt_319:  3.3378752321004868
========> pt_320:  3.0051933228969574
========> pt_321:  4.870910570025444
========> pt_322:  3.871876522898674
========> pt_323:  7.786412984132767
========> pt_324:  3.9098793640732765
========> pt_325:  4.148129038512707
========> pt_326:  4.103198051452637
========> pt_327:  3.789086900651455
========> pt_328:  3.659758120775223
========> pt_329:  5.096198916435242
========> pt_330:  7.394566461443901
========> pt_331:  4.341513440012932
========> pt_332:  3.667236492037773
========> pt_333:  3.283490538597107
========> pt_334:  3.237749859690666
========> pt_335:  4.765352830290794
========> pt_336:  4.2717477306723595
========> pt_337:  4.017402529716492
========> pt_338:  3.398115523159504
========> pt_339:  3.8216623291373253
========> pt_340:  3.5687438026070595
===============================================> mean Dose score: 4.036672608926892
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.055154158689,     best is           0.055154158689
            Average val evaluation index is   -4.036672608927,     best is           -3.900361184776
    Train use time   1531.45049
    Train loader use time     75.39014
    Val use time     44.41317
    Total use time   1579.07993
    End lr is 0.000274728868, 0.000274728868
    time: 05:11:52
Epoch: 30, iter: 14999
    Begin lr is 0.000274728868, 0.000274728868
========> pt_241:  3.339681327342987
========> pt_242:  3.110446222126484
========> pt_243:  4.473903402686119
========> pt_244:  3.5109881311655045
========> pt_245:  4.141419939696789
========> pt_246:  3.8723404332995415
========> pt_247:  3.320399187505245
========> pt_248:  3.225220888853073
========> pt_249:  3.9109302684664726
========> pt_250:  3.036295659840107
========> pt_251:  3.7281792983412743
========> pt_252:  3.814372234046459
========> pt_253:  3.76559566706419
========> pt_254:  3.7510493770241737
========> pt_255:  3.5346512123942375
========> pt_256:  3.096880428493023
========> pt_257:  3.690560832619667
========> pt_258:  2.9993458092212677
========> pt_259:  3.3598287031054497
========> pt_260:  4.591360613703728
========> pt_261:  3.9782478660345078
========> pt_262:  3.833712786436081
========> pt_263:  3.6797112226486206
========> pt_264:  3.9831219241023064
========> pt_265:  3.308730758726597
========> pt_266:  3.9457689225673676
========> pt_267:  4.0204063430428505
========> pt_268:  3.8320662826299667
========> pt_269:  3.4099065139889717
========> pt_270:  5.4476625472307205
========> pt_271:  4.002941511571407
========> pt_272:  3.61847635358572
========> pt_273:  3.492683619260788
========> pt_274:  3.9385922625660896
========> pt_275:  4.044053517282009
========> pt_276:  3.066660799086094
========> pt_277:  3.1353696063160896
========> pt_278:  4.7393811494112015
========> pt_279:  3.6359959468245506
========> pt_280:  3.076796419918537
========> pt_281:  3.4588731825351715
========> pt_282:  3.429720364511013
========> pt_283:  5.277528688311577
========> pt_284:  3.2017789408564568
========> pt_285:  3.5133108124136925
========> pt_286:  3.934432454407215
========> pt_287:  4.370552562177181
========> pt_288:  3.053528405725956
========> pt_289:  3.7643830850720406
========> pt_290:  3.763887621462345
========> pt_291:  3.3190830796957016
========> pt_292:  3.4510868415236473
========> pt_293:  3.3981014415621758
========> pt_294:  3.191576302051544
========> pt_295:  3.305366300046444
========> pt_296:  2.868087589740753
========> pt_297:  4.3945301324129105
========> pt_298:  3.667721264064312
========> pt_299:  3.3142483979463577
========> pt_300:  5.064910128712654
========> pt_301:  3.5950589179992676
========> pt_302:  4.95921678841114
========> pt_303:  3.5262679681181908
========> pt_304:  3.4152669087052345
========> pt_305:  3.706454001367092
========> pt_306:  3.1634636968374252
========> pt_307:  3.583894297480583
========> pt_308:  3.6466066911816597
========> pt_309:  3.0986228957772255
========> pt_310:  3.719441667199135
========> pt_311:  2.995900772511959
========> pt_312:  3.1057338416576385
========> pt_313:  3.2272841036319733
========> pt_314:  4.294433444738388
========> pt_315:  3.823610804975033
========> pt_316:  4.188577905297279
========> pt_317:  4.521673396229744
========> pt_318:  4.661693498492241
========> pt_319:  3.1649769470095634
========> pt_320:  2.998930662870407
========> pt_321:  3.493705578148365
========> pt_322:  3.428029790520668
========> pt_323:  7.628105580806732
========> pt_324:  3.7675052881240845
========> pt_325:  4.080405943095684
========> pt_326:  3.6394907906651497
========> pt_327:  3.374018259346485
========> pt_328:  3.629366122186184
========> pt_329:  5.471002012491226
========> pt_330:  7.071098610758781
========> pt_331:  3.820198103785515
========> pt_332:  3.2306284829974174
========> pt_333:  3.048127330839634
========> pt_334:  2.7877087891101837
========> pt_335:  5.188699886202812
========> pt_336:  4.122292175889015
========> pt_337:  3.51644866168499
========> pt_338:  3.1494196504354477
========> pt_339:  3.2164590060710907
========> pt_340:  3.302338756620884
===============================================> mean Dose score: 3.7656860161572694
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.054493393905,     best is           0.054493393905
            Average val evaluation index is   -3.765686016157,     best is           -3.765686016157
    Train use time   1531.06282
    Train loader use time     74.42610
    Val use time     44.39706
    Total use time   1580.31994
    End lr is 0.000273069196, 0.000273069196
    time: 05:38:12
Epoch: 31, iter: 15499
    Begin lr is 0.000273069196, 0.000273069196
========> pt_241:  3.3501382172107697
========> pt_242:  3.466768004000187
========> pt_243:  4.609245806932449
========> pt_244:  3.5580483078956604
========> pt_245:  4.141320325434208
========> pt_246:  4.226234443485737
========> pt_247:  3.7396302446722984
========> pt_248:  3.3790552988648415
========> pt_249:  4.094387404620647
========> pt_250:  3.436252139508724
========> pt_251:  3.90194620937109
========> pt_252:  3.5944226384162903
========> pt_253:  4.535929709672928
========> pt_254:  4.008133187890053
========> pt_255:  3.448513299226761
========> pt_256:  3.535500280559063
========> pt_257:  4.144498072564602
========> pt_258:  3.044067919254303
========> pt_259:  3.8572705164551735
========> pt_260:  5.722695961594582
========> pt_261:  4.034888483583927
========> pt_262:  3.9224010333418846
========> pt_263:  4.127217084169388
========> pt_264:  3.9015141129493713
========> pt_265:  3.4461603686213493
========> pt_266:  3.921976760029793
========> pt_267:  3.7309494614601135
========> pt_268:  4.180932641029358
========> pt_269:  3.369998224079609
========> pt_270:  5.624396502971649
========> pt_271:  4.908595532178879
========> pt_272:  4.366343729197979
========> pt_273:  3.6837927997112274
========> pt_274:  3.7754715606570244
========> pt_275:  4.082297049462795
========> pt_276:  3.301868326961994
========> pt_277:  3.206399790942669
========> pt_278:  4.798711612820625
========> pt_279:  3.0833806097507477
========> pt_280:  3.7564561888575554
========> pt_281:  3.343975432217121
========> pt_282:  3.4421103447675705
========> pt_283:  6.120878159999847
========> pt_284:  3.202304132282734
========> pt_285:  3.510786294937134
========> pt_286:  3.9746111631393433
========> pt_287:  4.929945319890976
========> pt_288:  3.1330789998173714
========> pt_289:  3.9088336750864983
========> pt_290:  4.39059354364872
========> pt_291:  3.805992379784584
========> pt_292:  3.417057618498802
========> pt_293:  3.4758824482560158
========> pt_294:  3.6083748936653137
========> pt_295:  3.5772981122136116
========> pt_296:  3.01884725689888
========> pt_297:  4.3019139021635056
========> pt_298:  4.300772249698639
========> pt_299:  3.5063811019062996
========> pt_300:  6.143096312880516
========> pt_301:  3.9608651772141457
========> pt_302:  5.061503425240517
========> pt_303:  3.2242297008633614
========> pt_304:  3.43130923807621
========> pt_305:  3.9376837387681007
========> pt_306:  3.2987435162067413
========> pt_307:  3.6447646096348763
========> pt_308:  3.602566495537758
========> pt_309:  3.1533588469028473
========> pt_310:  4.944001883268356
========> pt_311:  3.2532255351543427
========> pt_312:  3.1513183191418648
========> pt_313:  3.154100999236107
========> pt_314:  4.02610182762146
========> pt_315:  3.725474588572979
========> pt_316:  4.254353307187557
========> pt_317:  5.668318048119545
========> pt_318:  5.256975814700127
========> pt_319:  3.174135722219944
========> pt_320:  3.0560818687081337
========> pt_321:  3.564622327685356
========> pt_322:  3.5143186897039413
========> pt_323:  7.756568342447281
========> pt_324:  3.629740849137306
========> pt_325:  3.842567503452301
========> pt_326:  3.397061489522457
========> pt_327:  3.7141070887446404
========> pt_328:  3.3857430145144463
========> pt_329:  5.470941513776779
========> pt_330:  8.213960006833076
========> pt_331:  3.6861738935112953
========> pt_332:  3.2937072589993477
========> pt_333:  3.1117380782961845
========> pt_334:  2.9591013863682747
========> pt_335:  4.735267758369446
========> pt_336:  5.094058513641357
========> pt_337:  3.7663771957159042
========> pt_338:  3.4797353297472
========> pt_339:  3.639175519347191
========> pt_340:  3.9571132138371468
===============================================> mean Dose score: 3.973217348381877
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.054180270419,     best is           0.054180270419
            Average val evaluation index is   -3.973217348382,     best is           -3.765686016157
    Train use time   1531.10138
    Train loader use time     74.54681
    Val use time     44.99820
    Total use time   1579.30014
    End lr is 0.000271362098, 0.000271362098
    time: 06:04:31
Epoch: 32, iter: 15999
    Begin lr is 0.000271362098, 0.000271362098
========> pt_241:  3.497895374894142
========> pt_242:  3.22951577603817
========> pt_243:  4.519927799701691
========> pt_244:  3.4872280433773994
========> pt_245:  4.009898081421852
========> pt_246:  3.8474037498235703
========> pt_247:  3.5698100924491882
========> pt_248:  3.1465595215559006
========> pt_249:  3.9390645176172256
========> pt_250:  3.193456195294857
========> pt_251:  3.8572371378540993
========> pt_252:  3.575610928237438
========> pt_253:  4.318060018122196
========> pt_254:  3.7813078612089157
========> pt_255:  3.4481972455978394
========> pt_256:  3.3555012196302414
========> pt_257:  4.229716509580612
========> pt_258:  3.0847853794693947
========> pt_259:  3.5725121945142746
========> pt_260:  6.130708679556847
========> pt_261:  3.9200058579444885
========> pt_262:  3.6464205011725426
========> pt_263:  3.8692516088485718
========> pt_264:  3.968176916241646
========> pt_265:  3.388392962515354
========> pt_266:  3.720913454890251
========> pt_267:  3.9455965533852577
========> pt_268:  3.844778575003147
========> pt_269:  3.384203687310219
========> pt_270:  5.401078015565872
========> pt_271:  4.935745373368263
========> pt_272:  4.217595122754574
========> pt_273:  3.4550974890589714
========> pt_274:  3.8358693569898605
========> pt_275:  3.6773017048835754
========> pt_276:  3.126540184020996
========> pt_277:  3.0495281890034676
========> pt_278:  4.633646085858345
========> pt_279:  3.2792524993419647
========> pt_280:  3.568531796336174
========> pt_281:  3.3139920607209206
========> pt_282:  3.2324468344449997
========> pt_283:  6.877516284584999
========> pt_284:  3.188343793153763
========> pt_285:  3.4024769067764282
========> pt_286:  3.7162310630083084
========> pt_287:  4.908976256847382
========> pt_288:  3.0948790162801743
========> pt_289:  3.8612154498696327
========> pt_290:  4.0483322367072105
========> pt_291:  3.5493863001465797
========> pt_292:  3.132057562470436
========> pt_293:  3.472241833806038
========> pt_294:  3.425123244524002
========> pt_295:  3.435608819127083
========> pt_296:  2.8070256114006042
========> pt_297:  4.702522829174995
========> pt_298:  3.724597878754139
========> pt_299:  3.2681238651275635
========> pt_300:  5.390846952795982
========> pt_301:  3.9594585821032524
========> pt_302:  5.272007659077644
========> pt_303:  3.3199097216129303
========> pt_304:  3.401499018073082
========> pt_305:  3.748026266694069
========> pt_306:  3.1410223245620728
========> pt_307:  3.5529353842139244
========> pt_308:  3.8871464505791664
========> pt_309:  2.928892970085144
========> pt_310:  4.266386032104492
========> pt_311:  3.064308650791645
========> pt_312:  3.036342076957226
========> pt_313:  2.993873283267021
========> pt_314:  4.049939624965191
========> pt_315:  3.618967644870281
========> pt_316:  4.121793061494827
========> pt_317:  4.820930287241936
========> pt_318:  5.143706575036049
========> pt_319:  2.9986148700118065
========> pt_320:  2.8863083943724632
========> pt_321:  4.017678946256638
========> pt_322:  3.363492786884308
========> pt_323:  7.469339221715927
========> pt_324:  3.5929495468735695
========> pt_325:  3.938411809504032
========> pt_326:  3.3714616671204567
========> pt_327:  3.479466736316681
========> pt_328:  3.53278461843729
========> pt_329:  5.724533349275589
========> pt_330:  7.692656144499779
========> pt_331:  3.9071916043758392
========> pt_332:  3.1055599078536034
========> pt_333:  3.0747201666235924
========> pt_334:  2.7138980105519295
========> pt_335:  4.801527410745621
========> pt_336:  4.343960247933865
========> pt_337:  3.620655871927738
========> pt_338:  3.242649994790554
========> pt_339:  3.4216758608818054
========> pt_340:  3.513583317399025
===============================================> mean Dose score: 3.853545331582427
        ==> Saving latest model successfully !
            Average train loss is             0.054231361255,     best is           0.054180270419
            Average val evaluation index is   -3.853545331582,     best is           -3.765686016157
    Train use time   1531.75720
    Train loader use time     75.56771
    Val use time     44.64286
    Total use time   1577.90687
    End lr is 0.000269608232, 0.000269608232
    time: 06:30:49
Epoch: 33, iter: 16499
    Begin lr is 0.000269608232, 0.000269608232
========> pt_241:  3.1038719415664673
========> pt_242:  2.97153752297163
========> pt_243:  4.640323370695114
========> pt_244:  3.3693551644682884
========> pt_245:  4.26916167140007
========> pt_246:  3.975306637585163
========> pt_247:  3.3785121142864227
========> pt_248:  2.889085076749325
========> pt_249:  4.0537940710783005
========> pt_250:  3.0659478530287743
========> pt_251:  3.8274459540843964
========> pt_252:  3.983670063316822
========> pt_253:  4.428933560848236
========> pt_254:  3.9198248833417892
========> pt_255:  3.2798220217227936
========> pt_256:  3.0072836577892303
========> pt_257:  3.571052923798561
========> pt_258:  3.0825617909431458
========> pt_259:  3.309578262269497
========> pt_260:  5.074799582362175
========> pt_261:  4.632213935256004
========> pt_262:  3.874754384160042
========> pt_263:  3.4197795391082764
========> pt_264:  4.4974154978990555
========> pt_265:  3.1935276463627815
========> pt_266:  3.846571370959282
========> pt_267:  3.898736648261547
========> pt_268:  4.149093367159367
========> pt_269:  3.2385188713669777
========> pt_270:  5.81554114818573
========> pt_271:  4.087905697524548
========> pt_272:  3.446575254201889
========> pt_273:  3.707435019314289
========> pt_274:  4.070738926529884
========> pt_275:  3.702026642858982
========> pt_276:  3.049791045486927
========> pt_277:  3.1902142986655235
========> pt_278:  4.413943961262703
========> pt_279:  3.6372213065624237
========> pt_280:  3.1681466102600098
========> pt_281:  3.401919901371002
========> pt_282:  3.384755738079548
========> pt_283:  5.974524989724159
========> pt_284:  2.94357068836689
========> pt_285:  3.1847957521677017
========> pt_286:  3.9860516786575317
========> pt_287:  4.849933162331581
========> pt_288:  3.109600804746151
========> pt_289:  3.6276596412062645
========> pt_290:  4.458787590265274
========> pt_291:  3.2808199897408485
========> pt_292:  3.468310199677944
========> pt_293:  3.7439754605293274
========> pt_294:  3.190458118915558
========> pt_295:  3.5347046703100204
========> pt_296:  2.81063050031662
========> pt_297:  4.711972624063492
========> pt_298:  3.829328715801239
========> pt_299:  3.2164347544312477
========> pt_300:  5.54671511054039
========> pt_301:  3.964407481253147
========> pt_302:  5.1630426943302155
========> pt_303:  3.380763605237007
========> pt_304:  3.6098485067486763
========> pt_305:  3.6740660667419434
========> pt_306:  3.249485567212105
========> pt_307:  3.6065004765987396
========> pt_308:  4.137416854500771
========> pt_309:  2.911427356302738
========> pt_310:  4.185808263719082
========> pt_311:  2.982996553182602
========> pt_312:  3.1640559062361717
========> pt_313:  3.135501556098461
========> pt_314:  4.098896905779839
========> pt_315:  3.7526432052254677
========> pt_316:  4.6054380387067795
========> pt_317:  4.39568430185318
========> pt_318:  5.725546181201935
========> pt_319:  3.070670925080776
========> pt_320:  2.973109185695648
========> pt_321:  3.5414260253310204
========> pt_322:  3.917325921356678
========> pt_323:  8.028279021382332
========> pt_324:  3.7474822998046875
========> pt_325:  4.2130715399980545
========> pt_326:  3.281373605132103
========> pt_327:  3.2793518528342247
========> pt_328:  3.435927741229534
========> pt_329:  6.187345385551453
========> pt_330:  6.875205859541893
========> pt_331:  3.6698462814092636
========> pt_332:  3.0876974016427994
========> pt_333:  2.9770585522055626
========> pt_334:  2.6123151928186417
========> pt_335:  5.0781650841236115
========> pt_336:  4.37995620071888
========> pt_337:  3.925686739385128
========> pt_338:  3.341796435415745
========> pt_339:  3.285471089184284
========> pt_340:  3.5130437836050987
===============================================> mean Dose score: 3.8466210503131153
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.053106676437,     best is           0.053106676437
            Average val evaluation index is   -3.846621050313,     best is           -3.765686016157
    Train use time   1529.73090
    Train loader use time     73.58144
    Val use time     44.70948
    Total use time   1577.68931
    End lr is 0.000267808274, 0.000267808274
    time: 06:57:07
Epoch: 34, iter: 16999
    Begin lr is 0.000267808274, 0.000267808274
========> pt_241:  3.2672633230686188
========> pt_242:  3.059183992445469
========> pt_243:  4.775634482502937
========> pt_244:  3.5193001851439476
========> pt_245:  4.105841740965843
========> pt_246:  4.090169444680214
========> pt_247:  3.4394817799329758
========> pt_248:  3.048378974199295
========> pt_249:  3.9005375280976295
========> pt_250:  3.2578742876648903
========> pt_251:  3.8995004445314407
========> pt_252:  3.7864794582128525
========> pt_253:  5.091518610715866
========> pt_254:  3.8741736486554146
========> pt_255:  3.257967382669449
========> pt_256:  3.146803341805935
========> pt_257:  3.9374834671616554
========> pt_258:  3.183487206697464
========> pt_259:  3.2608329877257347
========> pt_260:  6.3807691633701324
========> pt_261:  4.626109823584557
========> pt_262:  3.7732448428869247
========> pt_263:  3.5519058629870415
========> pt_264:  4.3873172253370285
========> pt_265:  3.136295862495899
========> pt_266:  3.7723715230822563
========> pt_267:  3.9510518684983253
========> pt_268:  4.299445189535618
========> pt_269:  3.187972456216812
========> pt_270:  5.73380634188652
========> pt_271:  4.489409849047661
========> pt_272:  3.302716352045536
========> pt_273:  3.4369324892759323
========> pt_274:  3.9776071533560753
========> pt_275:  3.9512119814753532
========> pt_276:  2.9764436557888985
========> pt_277:  3.010779283940792
========> pt_278:  4.923618510365486
========> pt_279:  3.5124004632234573
========> pt_280:  3.625105656683445
========> pt_281:  3.280898481607437
========> pt_282:  3.298056647181511
========> pt_283:  6.456121876835823
========> pt_284:  2.7732349932193756
========> pt_285:  3.239756226539612
========> pt_286:  3.9223767817020416
========> pt_287:  4.584192559123039
========> pt_288:  3.019511178135872
========> pt_289:  4.1963861510157585
========> pt_290:  4.263674803078175
========> pt_291:  3.521505258977413
========> pt_292:  3.5167808830738068
========> pt_293:  3.618457317352295
========> pt_294:  3.239704854786396
========> pt_295:  3.6381394788622856
========> pt_296:  2.7753321081399918
========> pt_297:  4.770235493779182
========> pt_298:  3.8465171307325363
========> pt_299:  3.1304042786359787
========> pt_300:  5.5983977019786835
========> pt_301:  4.1069841757416725
========> pt_302:  5.2024054527282715
========> pt_303:  3.257245048880577
========> pt_304:  3.461620919406414
========> pt_305:  3.979583792388439
========> pt_306:  3.1933310255408287
========> pt_307:  3.451201841235161
========> pt_308:  4.105302728712559
========> pt_309:  2.82375767827034
========> pt_310:  4.528748095035553
========> pt_311:  3.04341834038496
========> pt_312:  3.0368096381425858
========> pt_313:  3.2774023339152336
========> pt_314:  4.184130467474461
========> pt_315:  3.654022216796875
========> pt_316:  4.60868202149868
========> pt_317:  4.900549724698067
========> pt_318:  6.260401234030724
========> pt_319:  2.890884391963482
========> pt_320:  2.8303371742367744
========> pt_321:  4.195418693125248
========> pt_322:  3.555517792701721
========> pt_323:  8.197425603866577
========> pt_324:  3.7845169007778168
========> pt_325:  4.137088283896446
========> pt_326:  3.4688593819737434
========> pt_327:  3.293289504945278
========> pt_328:  3.605881668627262
========> pt_329:  5.077239349484444
========> pt_330:  6.7352937161922455
========> pt_331:  3.9969513565301895
========> pt_332:  3.0001286417245865
========> pt_333:  2.9861967265605927
========> pt_334:  2.736038714647293
========> pt_335:  5.083027929067612
========> pt_336:  4.720073193311691
========> pt_337:  3.7105585262179375
========> pt_338:  3.233274780213833
========> pt_339:  3.455258384346962
========> pt_340:  3.8325802609324455
===============================================> mean Dose score: 3.901995497569442
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.052885616563,     best is           0.052885616563
            Average val evaluation index is   -3.901995497569,     best is           -3.765686016157
    Train use time   1531.07196
    Train loader use time     74.74990
    Val use time     44.93903
    Total use time   1579.25207
    End lr is 0.000265962917, 0.000265962917
    time: 07:23:26
Epoch: 35, iter: 17499
    Begin lr is 0.000265962917, 0.000265962917
========> pt_241:  3.3781665936112404
========> pt_242:  3.3498265966773033
========> pt_243:  4.577090740203857
========> pt_244:  3.7424053624272346
========> pt_245:  3.9497946947813034
========> pt_246:  4.19994592666626
========> pt_247:  3.50831001996994
========> pt_248:  3.14772542566061
========> pt_249:  3.773190602660179
========> pt_250:  3.2592683658003807
========> pt_251:  3.6334815993905067
========> pt_252:  3.581940084695816
========> pt_253:  4.4408997893333435
========> pt_254:  3.6774373054504395
========> pt_255:  3.9345213770866394
========> pt_256:  3.065003603696823
========> pt_257:  3.8724736869335175
========> pt_258:  3.3525242656469345
========> pt_259:  3.939158394932747
========> pt_260:  5.664159804582596
========> pt_261:  4.199381358921528
========> pt_262:  3.735787533223629
========> pt_263:  3.833054080605507
========> pt_264:  3.711850382387638
========> pt_265:  3.187672831118107
========> pt_266:  3.7121857330203056
========> pt_267:  3.692619875073433
========> pt_268:  4.148295149207115
========> pt_269:  3.17201878875494
========> pt_270:  5.549068823456764
========> pt_271:  4.447310566902161
========> pt_272:  4.287489652633667
========> pt_273:  3.4144097566604614
========> pt_274:  3.781638778746128
========> pt_275:  4.115154109895229
========> pt_276:  3.234013542532921
========> pt_277:  2.9551512375473976
========> pt_278:  4.444263726472855
========> pt_279:  3.3795369416475296
========> pt_280:  3.150285668671131
========> pt_281:  3.1253231689333916
========> pt_282:  3.2825767993927
========> pt_283:  6.900113597512245
========> pt_284:  3.2451797276735306
========> pt_285:  3.37637796998024
========> pt_286:  3.723868764936924
========> pt_287:  4.354926161468029
========> pt_288:  3.0670365691184998
========> pt_289:  3.9458009973168373
========> pt_290:  4.1631123796105385
========> pt_291:  3.7333641946315765
========> pt_292:  3.348483368754387
========> pt_293:  3.3264052495360374
========> pt_294:  3.4437385946512222
========> pt_295:  3.6410295963287354
========> pt_296:  2.8510545939207077
========> pt_297:  4.376009702682495
========> pt_298:  4.153958819806576
========> pt_299:  3.2322687283158302
========> pt_300:  5.721615329384804
========> pt_301:  3.613826557993889
========> pt_302:  5.093371644616127
========> pt_303:  3.095085285604
========> pt_304:  3.434099741280079
========> pt_305:  3.767388202250004
========> pt_306:  3.285745158791542
========> pt_307:  3.4681962430477142
========> pt_308:  3.7513620406389236
========> pt_309:  2.9137158766388893
========> pt_310:  4.315224662423134
========> pt_311:  3.3945317566394806
========> pt_312:  3.036103993654251
========> pt_313:  3.1258108094334602
========> pt_314:  3.893773667514324
========> pt_315:  3.4999870136380196
========> pt_316:  4.207083210349083
========> pt_317:  4.955599382519722
========> pt_318:  5.6867581605911255
========> pt_319:  2.9061947390437126
========> pt_320:  2.8083132952451706
========> pt_321:  3.988214246928692
========> pt_322:  3.395700268447399
========> pt_323:  7.24413588643074
========> pt_324:  3.644159622490406
========> pt_325:  3.906931094825268
========> pt_326:  3.3973441645503044
========> pt_327:  3.034769631922245
========> pt_328:  3.3892498537898064
========> pt_329:  5.196234583854675
========> pt_330:  8.373204097151756
========> pt_331:  3.5476018488407135
========> pt_332:  2.9786672443151474
========> pt_333:  2.9800574108958244
========> pt_334:  2.80786894261837
========> pt_335:  4.869513362646103
========> pt_336:  4.5812249928712845
========> pt_337:  3.5788632556796074
========> pt_338:  3.328344337642193
========> pt_339:  3.565322495996952
========> pt_340:  3.6160438880324364
===============================================> mean Dose score: 3.84886379763484
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.052518494762,     best is           0.052518494762
            Average val evaluation index is   -3.848863797635,     best is           -3.765686016157
    Train use time   1531.67257
    Train loader use time     74.17907
    Val use time     44.95587
    Total use time   1579.87007
    End lr is 0.000264072875, 0.000264072875
    time: 07:49:46
Epoch: 36, iter: 17999
    Begin lr is 0.000264072875, 0.000264072875
========> pt_241:  3.5945728421211243
========> pt_242:  3.135458268225193
========> pt_243:  4.428253471851349
========> pt_244:  3.520026169717312
========> pt_245:  3.7371378019452095
========> pt_246:  3.98120891302824
========> pt_247:  3.6608465760946274
========> pt_248:  2.7479778230190277
========> pt_249:  3.7247642502188683
========> pt_250:  3.395378477871418
========> pt_251:  3.5012613981962204
========> pt_252:  3.2727183774113655
========> pt_253:  4.345436468720436
========> pt_254:  3.751659579575062
========> pt_255:  3.446813076734543
========> pt_256:  3.3216386288404465
========> pt_257:  4.059750065207481
========> pt_258:  3.0982398241758347
========> pt_259:  3.4350747615098953
========> pt_260:  6.068945750594139
========> pt_261:  3.8696258142590523
========> pt_262:  3.472806140780449
========> pt_263:  3.8373129814863205
========> pt_264:  3.7730946391820908
========> pt_265:  3.305848464369774
========> pt_266:  3.484499603509903
========> pt_267:  3.5326985642313957
========> pt_268:  3.993823677301407
========> pt_269:  3.150426745414734
========> pt_270:  5.366056561470032
========> pt_271:  4.87310416996479
========> pt_272:  4.088842384517193
========> pt_273:  3.382035121321678
========> pt_274:  3.704822100698948
========> pt_275:  4.014710336923599
========> pt_276:  3.0765805020928383
========> pt_277:  2.8497809916734695
========> pt_278:  4.4478195905685425
========> pt_279:  3.1071146205067635
========> pt_280:  3.3659716695547104
========> pt_281:  3.1027600169181824
========> pt_282:  3.233565278351307
========> pt_283:  6.967963948845863
========> pt_284:  3.183424361050129
========> pt_285:  2.981070764362812
========> pt_286:  3.505173996090889
========> pt_287:  4.735708981752396
========> pt_288:  3.1433238834142685
========> pt_289:  3.8003795593976974
========> pt_290:  4.2399100214242935
========> pt_291:  3.5592932254076004
========> pt_292:  3.1692418456077576
========> pt_293:  3.3002014830708504
========> pt_294:  3.4670908376574516
========> pt_295:  3.323476016521454
========> pt_296:  2.869550511240959
========> pt_297:  4.238867722451687
========> pt_298:  3.805951438844204
========> pt_299:  3.1064478307962418
========> pt_300:  5.438666492700577
========> pt_301:  3.913649059832096
========> pt_302:  4.919985458254814
========> pt_303:  3.056192174553871
========> pt_304:  3.3313142508268356
========> pt_305:  3.7328974157571793
========> pt_306:  3.088785856962204
========> pt_307:  3.3948947489261627
========> pt_308:  3.821418508887291
========> pt_309:  2.7887044101953506
========> pt_310:  4.52384926378727
========> pt_311:  3.0834437161684036
========> pt_312:  2.9621291905641556
========> pt_313:  2.945306897163391
========> pt_314:  3.6960922926664352
========> pt_315:  3.3159932121634483
========> pt_316:  4.121357835829258
========> pt_317:  5.160205513238907
========> pt_318:  5.227625072002411
========> pt_319:  2.8989727050065994
========> pt_320:  2.7438435703516006
========> pt_321:  3.994865193963051
========> pt_322:  3.2887380197644234
========> pt_323:  7.323745936155319
========> pt_324:  3.4348228573799133
========> pt_325:  3.7991422042250633
========> pt_326:  3.3445514738559723
========> pt_327:  3.4159091860055923
========> pt_328:  3.2703685760498047
========> pt_329:  5.612938776612282
========> pt_330:  8.19955088198185
========> pt_331:  3.7294620275497437
========> pt_332:  2.985842600464821
========> pt_333:  2.807498909533024
========> pt_334:  2.671891562640667
========> pt_335:  4.6914192289114
========> pt_336:  4.473007395863533
========> pt_337:  3.5741644352674484
========> pt_338:  3.1948818266391754
========> pt_339:  3.511328175663948
========> pt_340:  3.4928426891565323
===============================================> mean Dose score: 3.7763983853161336
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.051946974933,     best is           0.051946974933
            Average val evaluation index is   -3.776398385316,     best is           -3.765686016157
    Train use time   1531.41815
    Train loader use time     73.86227
    Val use time     45.15288
    Total use time   1579.82184
    End lr is 0.000262138874, 0.000262138874
    time: 08:16:06
Epoch: 37, iter: 18499
    Begin lr is 0.000262138874, 0.000262138874
========> pt_241:  3.196828216314316
========> pt_242:  2.97271516174078
========> pt_243:  4.470849260687828
========> pt_244:  3.2434137910604477
========> pt_245:  3.9052381739020348
========> pt_246:  3.95583413541317
========> pt_247:  3.294830396771431
========> pt_248:  2.762875370681286
========> pt_249:  4.071311317384243
========> pt_250:  3.0235254764556885
========> pt_251:  3.8663437590003014
========> pt_252:  3.4206784144043922
========> pt_253:  4.4658878445625305
========> pt_254:  3.777586407959461
========> pt_255:  3.2844121009111404
========> pt_256:  2.883629761636257
========> pt_257:  3.6849133297801018
========> pt_258:  2.975766435265541
========> pt_259:  3.3894767239689827
========> pt_260:  5.341596305370331
========> pt_261:  4.369157962501049
========> pt_262:  3.6121589317917824
========> pt_263:  3.4777553007006645
========> pt_264:  4.25199881196022
========> pt_265:  3.169451765716076
========> pt_266:  3.6314616724848747
========> pt_267:  3.7239595130085945
========> pt_268:  4.066215343773365
========> pt_269:  3.0905577912926674
========> pt_270:  5.517242848873138
========> pt_271:  4.335627853870392
========> pt_272:  3.7310978397727013
========> pt_273:  3.250439465045929
========> pt_274:  3.8193200901150703
========> pt_275:  3.7420793995261192
========> pt_276:  2.9184073954820633
========> pt_277:  2.886308655142784
========> pt_278:  4.8103852570056915
========> pt_279:  2.949712350964546
========> pt_280:  3.2940952852368355
========> pt_281:  3.196795880794525
========> pt_282:  3.212483562529087
========> pt_283:  5.94473771750927
========> pt_284:  2.906358763575554
========> pt_285:  3.070339486002922
========> pt_286:  3.4888453409075737
========> pt_287:  4.488687515258789
========> pt_288:  2.9074759036302567
========> pt_289:  4.101563543081284
========> pt_290:  4.11300640553236
========> pt_291:  3.394407369196415
========> pt_292:  3.2107335329055786
========> pt_293:  3.5411660373210907
========> pt_294:  3.324609585106373
========> pt_295:  3.335084468126297
========> pt_296:  2.6121556013822556
========> pt_297:  4.4362883269786835
========> pt_298:  3.7160834670066833
========> pt_299:  2.9718175902962685
========> pt_300:  5.377768278121948
========> pt_301:  3.809608221054077
========> pt_302:  5.3503138571977615
========> pt_303:  3.091098628938198
========> pt_304:  3.2362045347690582
========> pt_305:  3.807014860212803
========> pt_306:  2.9348500072956085
========> pt_307:  3.3063723519444466
========> pt_308:  3.8506652042269707
========> pt_309:  2.792796678841114
========> pt_310:  4.237263463437557
========> pt_311:  2.8780589252710342
========> pt_312:  2.82187283039093
========> pt_313:  2.8859743475914
========> pt_314:  3.8101793080568314
========> pt_315:  3.4139302000403404
========> pt_316:  4.478782936930656
========> pt_317:  4.834864288568497
========> pt_318:  5.676658004522324
========> pt_319:  2.8544949367642403
========> pt_320:  2.6737917959690094
========> pt_321:  4.8436439037323
========> pt_322:  3.2977361604571342
========> pt_323:  7.641407996416092
========> pt_324:  3.5256071761250496
========> pt_325:  3.7939463555812836
========> pt_326:  3.4174299985170364
========> pt_327:  3.0057310312986374
========> pt_328:  3.204236701130867
========> pt_329:  4.975171238183975
========> pt_330:  7.504860311746597
========> pt_331:  4.21838890761137
========> pt_332:  2.830764316022396
========> pt_333:  2.8064678236842155
========> pt_334:  2.4530014768242836
========> pt_335:  4.923801571130753
========> pt_336:  4.381612613797188
========> pt_337:  3.532697521150112
========> pt_338:  3.097723498940468
========> pt_339:  3.2875901088118553
========> pt_340:  3.466445431113243
===============================================> mean Dose score: 3.7293861381709577
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.052071356840,     best is           0.051946974933
            Average val evaluation index is   -3.729386138171,     best is           -3.729386138171
    Train use time   1530.90609
    Train loader use time     73.65515
    Val use time     44.73403
    Total use time   1578.80143
    End lr is 0.000260161660, 0.000260161660
    time: 08:42:25
Epoch: 38, iter: 18999
    Begin lr is 0.000260161660, 0.000260161660
========> pt_241:  3.2029763981699944
========> pt_242:  3.3197662979364395
========> pt_243:  4.415102303028107
========> pt_244:  3.2991839572787285
========> pt_245:  3.942515030503273
========> pt_246:  4.187234677374363
========> pt_247:  3.318578489124775
========> pt_248:  3.2244638726115227
========> pt_249:  3.88589084148407
========> pt_250:  3.078908920288086
========> pt_251:  3.5843389108777046
========> pt_252:  3.6040395870804787
========> pt_253:  4.037581980228424
========> pt_254:  3.607976958155632
========> pt_255:  3.9434605836868286
========> pt_256:  3.074590563774109
========> pt_257:  3.722684867680073
========> pt_258:  2.9917435720562935
========> pt_259:  4.010097309947014
========> pt_260:  5.0435467809438705
========> pt_261:  3.946683183312416
========> pt_262:  3.840070627629757
========> pt_263:  3.751874715089798
========> pt_264:  3.6571749299764633
========> pt_265:  3.1629006937146187
========> pt_266:  3.8229500129818916
========> pt_267:  3.4213243424892426
========> pt_268:  4.03491847217083
========> pt_269:  3.1662338599562645
========> pt_270:  5.389590561389923
========> pt_271:  4.360092543065548
========> pt_272:  4.558703824877739
========> pt_273:  3.238847441971302
========> pt_274:  4.144571088254452
========> pt_275:  3.6898424103856087
========> pt_276:  2.9354049265384674
========> pt_277:  2.8579720482230186
========> pt_278:  4.734338894486427
========> pt_279:  2.658039703965187
========> pt_280:  3.2093480601906776
========> pt_281:  3.120081163942814
========> pt_282:  3.1001319736242294
========> pt_283:  5.490666702389717
========> pt_284:  3.3293167501688004
========> pt_285:  3.261312022805214
========> pt_286:  3.516840599477291
========> pt_287:  4.506798535585403
========> pt_288:  2.7844642847776413
========> pt_289:  4.014135859906673
========> pt_290:  4.073175825178623
========> pt_291:  3.3843737095594406
========> pt_292:  3.1735075265169144
========> pt_293:  3.3110124990344048
========> pt_294:  3.36736261844635
========> pt_295:  3.3304696157574654
========> pt_296:  2.647155672311783
========> pt_297:  4.017707370221615
========> pt_298:  3.8377977535128593
========> pt_299:  2.954034097492695
========> pt_300:  5.3645675629377365
========> pt_301:  3.7067974358797073
========> pt_302:  5.199050381779671
========> pt_303:  3.0827657133340836
========> pt_304:  3.205712139606476
========> pt_305:  3.598018139600754
========> pt_306:  2.994825355708599
========> pt_307:  3.4185494855046272
========> pt_308:  3.6053408309817314
========> pt_309:  2.8510762378573418
========> pt_310:  4.241604506969452
========> pt_311:  2.996397279202938
========> pt_312:  2.8177453577518463
========> pt_313:  2.841520830988884
========> pt_314:  3.746352903544903
========> pt_315:  3.4225499629974365
========> pt_316:  4.370671734213829
========> pt_317:  4.954516664147377
========> pt_318:  5.472816973924637
========> pt_319:  2.8343866765499115
========> pt_320:  2.7499573305249214
========> pt_321:  3.6040210723876953
========> pt_322:  3.174884393811226
========> pt_323:  7.294483855366707
========> pt_324:  3.270527385175228
========> pt_325:  4.0560755506157875
========> pt_326:  3.1652121618390083
========> pt_327:  3.1622138246893883
========> pt_328:  3.0524156987667084
========> pt_329:  4.869702681899071
========> pt_330:  8.441331386566162
========> pt_331:  3.765222765505314
========> pt_332:  2.879282459616661
========> pt_333:  2.8047071024775505
========> pt_334:  2.689664103090763
========> pt_335:  4.672084152698517
========> pt_336:  4.318079315125942
========> pt_337:  3.4261204302310944
========> pt_338:  3.0638979375362396
========> pt_339:  3.460410423576832
========> pt_340:  3.451971895992756
===============================================> mean Dose score: 3.713934409245849
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.051399155118,     best is           0.051399155118
            Average val evaluation index is   -3.713934409246,     best is           -3.713934409246
    Train use time   1530.92340
    Train loader use time     73.20832
    Val use time     44.88074
    Total use time   1580.67017
    End lr is 0.000258141997, 0.000258141997
    time: 09:08:45
Epoch: 39, iter: 19499
    Begin lr is 0.000258141997, 0.000258141997
========> pt_241:  3.2965699955821037
========> pt_242:  2.8602245822548866
========> pt_243:  4.4770801067352295
========> pt_244:  3.3582573011517525
========> pt_245:  3.7649257481098175
========> pt_246:  3.8478217646479607
========> pt_247:  3.3022746071219444
========> pt_248:  2.8335003182291985
========> pt_249:  3.8533631339669228
========> pt_250:  2.9943082481622696
========> pt_251:  3.6508238688111305
========> pt_252:  3.529995158314705
========> pt_253:  3.9179564639925957
========> pt_254:  3.706052415072918
========> pt_255:  3.123399205505848
========> pt_256:  3.0445487797260284
========> pt_257:  3.4601277485489845
========> pt_258:  2.799050472676754
========> pt_259:  3.219643011689186
========> pt_260:  5.23147352039814
========> pt_261:  4.099080488085747
========> pt_262:  3.4831850603222847
========> pt_263:  3.5530923679471016
========> pt_264:  4.002612419426441
========> pt_265:  3.118310794234276
========> pt_266:  3.699222318828106
========> pt_267:  3.884553350508213
========> pt_268:  3.9548562467098236
========> pt_269:  3.172522597014904
========> pt_270:  5.286541432142258
========> pt_271:  4.090743139386177
========> pt_272:  3.358617424964905
========> pt_273:  3.3635131269693375
========> pt_274:  3.6818664893507957
========> pt_275:  3.667180947959423
========> pt_276:  2.771078944206238
========> pt_277:  2.8998783603310585
========> pt_278:  4.443345814943314
========> pt_279:  3.2760708406567574
========> pt_280:  3.0280933901667595
========> pt_281:  3.1228187307715416
========> pt_282:  3.1023117527365685
========> pt_283:  6.907041221857071
========> pt_284:  2.6756148412823677
========> pt_285:  3.2085026428103447
========> pt_286:  3.791247643530369
========> pt_287:  4.806014224886894
========> pt_288:  2.829410657286644
========> pt_289:  3.6771898344159126
========> pt_290:  4.0582506358623505
========> pt_291:  2.9010896384716034
========> pt_292:  3.4127160534262657
========> pt_293:  3.555174618959427
========> pt_294:  2.9523740336298943
========> pt_295:  3.317658230662346
========> pt_296:  2.6435486972332
========> pt_297:  4.532623663544655
========> pt_298:  3.5205763950943947
========> pt_299:  3.0522284656763077
========> pt_300:  5.256417766213417
========> pt_301:  3.920944891870022
========> pt_302:  5.139584317803383
========> pt_303:  3.189232237637043
========> pt_304:  3.288058713078499
========> pt_305:  3.5057930648326874
========> pt_306:  2.8825626894831657
========> pt_307:  3.446522317826748
========> pt_308:  3.7929778546094894
========> pt_309:  2.7041394636034966
========> pt_310:  3.8432903587818146
========> pt_311:  2.6975980401039124
========> pt_312:  2.8855349496006966
========> pt_313:  2.949778065085411
========> pt_314:  3.9068132266402245
========> pt_315:  3.4612172469496727
========> pt_316:  4.255676716566086
========> pt_317:  4.836447164416313
========> pt_318:  4.823737740516663
========> pt_319:  2.887500114738941
========> pt_320:  2.6382289826869965
========> pt_321:  3.5448528081178665
========> pt_322:  3.537474311888218
========> pt_323:  8.228622078895569
========> pt_324:  3.4114155918359756
========> pt_325:  3.771403282880783
========> pt_326:  3.29278152436018
========> pt_327:  3.3254211023449898
========> pt_328:  3.263736143708229
========> pt_329:  6.526605486869812
========> pt_330:  7.433949038386345
========> pt_331:  3.7591616809368134
========> pt_332:  3.0059216544032097
========> pt_333:  2.82054889947176
========> pt_334:  2.4767696484923363
========> pt_335:  4.547800496220589
========> pt_336:  4.114724099636078
========> pt_337:  3.4984002262353897
========> pt_338:  2.9510944336652756
========> pt_339:  3.188813701272011
========> pt_340:  3.3164406940340996
===============================================> mean Dose score: 3.6750014681369065
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.050671792768,     best is           0.050671792768
            Average val evaluation index is   -3.675001468137,     best is           -3.675001468137
    Train use time   1531.28748
    Train loader use time     73.95753
    Val use time     44.33493
    Total use time   1580.42013
    End lr is 0.000256080662, 0.000256080662
    time: 09:35:06
Epoch: 40, iter: 19999
    Begin lr is 0.000256080662, 0.000256080662
========> pt_241:  3.310905583202839
========> pt_242:  2.8166522085666656
========> pt_243:  4.640782848000526
========> pt_244:  3.3652136102318764
========> pt_245:  3.919658251106739
========> pt_246:  4.136237129569054
========> pt_247:  3.068174310028553
========> pt_248:  2.7541179209947586
========> pt_249:  3.96891750395298
========> pt_250:  3.039502874016762
========> pt_251:  3.7695784121751785
========> pt_252:  3.7407635524868965
========> pt_253:  4.079871624708176
========> pt_254:  3.982442356646061
========> pt_255:  3.567923679947853
========> pt_256:  2.8084126487374306
========> pt_257:  3.095938265323639
========> pt_258:  2.8747888654470444
========> pt_259:  3.233509734272957
========> pt_260:  4.669322595000267
========> pt_261:  4.692656844854355
========> pt_262:  3.575283922255039
========> pt_263:  3.3707189932465553
========> pt_264:  4.599391296505928
========> pt_265:  3.1095992401242256
========> pt_266:  3.789401911199093
========> pt_267:  3.8021687045693398
========> pt_268:  4.362768828868866
========> pt_269:  2.97688826918602
========> pt_270:  5.922312512993813
========> pt_271:  3.6915890499949455
========> pt_272:  3.330571837723255
========> pt_273:  3.222101293504238
========> pt_274:  3.9066332951188087
========> pt_275:  4.026640057563782
========> pt_276:  2.9337792843580246
========> pt_277:  2.8597867488861084
========> pt_278:  4.7280798852443695
========> pt_279:  3.39293010532856
========> pt_280:  3.149353675544262
========> pt_281:  3.0509918928146362
========> pt_282:  3.2253630086779594
========> pt_283:  6.1452846974134445
========> pt_284:  2.9088660702109337
========> pt_285:  3.0911505222320557
========> pt_286:  3.6924555897712708
========> pt_287:  4.72607247531414
========> pt_288:  2.846624106168747
========> pt_289:  4.247709140181541
========> pt_290:  4.271217584609985
========> pt_291:  2.9301543161273003
========> pt_292:  3.5134177282452583
========> pt_293:  3.5073021426796913
========> pt_294:  3.1455639004707336
========> pt_295:  3.419583961367607
========> pt_296:  2.6096532493829727
========> pt_297:  4.417949393391609
========> pt_298:  3.742358423769474
========> pt_299:  3.0776426196098328
========> pt_300:  5.205611363053322
========> pt_301:  3.84197685867548
========> pt_302:  5.1993247121572495
========> pt_303:  3.2957809045910835
========> pt_304:  3.3015603572130203
========> pt_305:  3.8369351252913475
========> pt_306:  2.981272339820862
========> pt_307:  3.3689483627676964
========> pt_308:  4.073406346142292
========> pt_309:  2.696221172809601
========> pt_310:  4.28438987582922
========> pt_311:  2.8069093078374863
========> pt_312:  3.0467285588383675
========> pt_313:  3.211456909775734
========> pt_314:  3.9700625464320183
========> pt_315:  3.5075728222727776
========> pt_316:  4.786542505025864
========> pt_317:  4.502795711159706
========> pt_318:  5.848444104194641
========> pt_319:  2.7909407764673233
========> pt_320:  2.722177989780903
========> pt_321:  4.420728161931038
========> pt_322:  3.4791895374655724
========> pt_323:  8.288902267813683
========> pt_324:  3.4459152445197105
========> pt_325:  3.8363822922110558
========> pt_326:  3.3665654435753822
========> pt_327:  3.1505240127444267
========> pt_328:  3.427482172846794
========> pt_329:  6.433343589305878
========> pt_330:  7.101869508624077
========> pt_331:  3.830052874982357
========> pt_332:  2.858082354068756
========> pt_333:  2.849634438753128
========> pt_334:  2.451634518802166
========> pt_335:  4.656206890940666
========> pt_336:  4.084663018584251
========> pt_337:  3.6810648813843727
========> pt_338:  2.910085953772068
========> pt_339:  3.1873129680752754
========> pt_340:  3.2741406187415123
===============================================> mean Dose score: 3.748675699532032
        ==> Saving latest model successfully !
            Average train loss is             0.051041251048,     best is           0.050671792768
            Average val evaluation index is   -3.748675699532,     best is           -3.675001468137
    Train use time   1530.91254
    Train loader use time     73.89393
    Val use time     45.13484
    Total use time   1577.71042
    End lr is 0.000253978450, 0.000253978450
    time: 10:01:23
Epoch: 41, iter: 20499
    Begin lr is 0.000253978450, 0.000253978450
========> pt_241:  2.9893799498677254
========> pt_242:  2.8017499670386314
========> pt_243:  4.559081941843033
========> pt_244:  3.1558163464069366
========> pt_245:  4.079952724277973
========> pt_246:  3.944433778524399
========> pt_247:  2.9906759783625603
========> pt_248:  2.817510664463043
========> pt_249:  3.8189518824219704
========> pt_250:  2.8131242468953133
========> pt_251:  3.79974901676178
========> pt_252:  4.109398126602173
========> pt_253:  3.92147034406662
========> pt_254:  3.9839063212275505
========> pt_255:  3.3739186450839043
========> pt_256:  2.6889798417687416
========> pt_257:  3.3867866173386574
========> pt_258:  2.755182906985283
========> pt_259:  3.110647536814213
========> pt_260:  4.435253068804741
========> pt_261:  4.47301521897316
========> pt_262:  3.948148973286152
========> pt_263:  3.233345188200474
========> pt_264:  4.224472939968109
========> pt_265:  3.0646729469299316
========> pt_266:  3.866789937019348
========> pt_267:  3.8214904814958572
========> pt_268:  4.09384161233902
========> pt_269:  3.179975152015686
========> pt_270:  5.709083750844002
========> pt_271:  3.795887790620327
========> pt_272:  3.359971083700657
========> pt_273:  3.170429915189743
========> pt_274:  4.290910959243774
========> pt_275:  3.4092697128653526
========> pt_276:  2.8215719014406204
========> pt_277:  2.8705667331814766
========> pt_278:  4.849819988012314
========> pt_279:  3.4079554304480553
========> pt_280:  3.044302351772785
========> pt_281:  3.236977458000183
========> pt_282:  3.1991780176758766
========> pt_283:  5.2545782923698425
========> pt_284:  3.0896278843283653
========> pt_285:  3.055497743189335
========> pt_286:  3.7390320375561714
========> pt_287:  4.387480467557907
========> pt_288:  2.7888113260269165
========> pt_289:  4.15090624243021
========> pt_290:  4.093948267400265
========> pt_291:  2.8705724701285362
========> pt_292:  3.570066951215267
========> pt_293:  3.3624418824911118
========> pt_294:  2.962612919509411
========> pt_295:  3.1093692407011986
========> pt_296:  2.5485844910144806
========> pt_297:  4.573677256703377
========> pt_298:  3.3960382267832756
========> pt_299:  3.0510789901018143
========> pt_300:  4.557053670287132
========> pt_301:  3.7115194648504257
========> pt_302:  5.49937017261982
========> pt_303:  3.4329069778323174
========> pt_304:  3.293565660715103
========> pt_305:  3.6128953471779823
========> pt_306:  2.901325114071369
========> pt_307:  3.2701195403933525
========> pt_308:  3.974389247596264
========> pt_309:  2.8094765916466713
========> pt_310:  3.752156086266041
========> pt_311:  2.6135997474193573
========> pt_312:  2.8959185630083084
========> pt_313:  2.908663973212242
========> pt_314:  4.052438847720623
========> pt_315:  3.5275739058852196
========> pt_316:  4.6251945197582245
========> pt_317:  4.23073273152113
========> pt_318:  5.260718911886215
========> pt_319:  2.8106607496738434
========> pt_320:  2.7761470153927803
========> pt_321:  3.8716621696949005
========> pt_322:  3.558500222861767
========> pt_323:  8.314642384648323
========> pt_324:  3.510931022465229
========> pt_325:  4.321148321032524
========> pt_326:  3.1797727942466736
========> pt_327:  3.092047832906246
========> pt_328:  3.276100568473339
========> pt_329:  6.642641499638557
========> pt_330:  6.193964779376984
========> pt_331:  3.676096685230732
========> pt_332:  2.8643669188022614
========> pt_333:  2.8261205181479454
========> pt_334:  2.494170591235161
========> pt_335:  5.079832971096039
========> pt_336:  4.0644001215696335
========> pt_337:  3.4220414608716965
========> pt_338:  2.9185638576745987
========> pt_339:  2.9439008235931396
========> pt_340:  3.223677910864353
===============================================> mean Dose score: 3.6660298243165017
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.050491686493,     best is           0.050491686493
            Average val evaluation index is   -3.666029824317,     best is           -3.666029824317
    Train use time   1532.25489
    Train loader use time     74.70580
    Val use time     45.50985
    Total use time   1582.77153
    End lr is 0.000251836172, 0.000251836172
    time: 10:27:46
Epoch: 42, iter: 20999
    Begin lr is 0.000251836172, 0.000251836172
========> pt_241:  3.220369517803192
========> pt_242:  2.714989595115185
========> pt_243:  4.401601701974869
========> pt_244:  3.1667418405413628
========> pt_245:  3.590093068778515
========> pt_246:  4.02418177574873
========> pt_247:  3.2473232597112656
========> pt_248:  2.6817364245653152
========> pt_249:  3.852175325155258
========> pt_250:  2.9740411788225174
========> pt_251:  3.501570150256157
========> pt_252:  3.5161054879426956
========> pt_253:  4.036589227616787
========> pt_254:  3.7831859290599823
========> pt_255:  3.160743862390518
========> pt_256:  2.8311898931860924
========> pt_257:  3.5124268010258675
========> pt_258:  2.9743003845214844
========> pt_259:  3.0729886516928673
========> pt_260:  5.078006014227867
========> pt_261:  3.8291996344923973
========> pt_262:  3.642936609685421
========> pt_263:  3.5391049087047577
========> pt_264:  3.9312904328107834
========> pt_265:  3.101940155029297
========> pt_266:  3.5420774295926094
========> pt_267:  3.6994241550564766
========> pt_268:  4.07320711761713
========> pt_269:  2.9425810649991035
========> pt_270:  5.332042723894119
========> pt_271:  3.993741273880005
========> pt_272:  3.3467312529683113
========> pt_273:  3.1738879904150963
========> pt_274:  3.6699099093675613
========> pt_275:  3.8525205850601196
========> pt_276:  2.8612932190299034
========> pt_277:  2.757975496351719
========> pt_278:  4.480501413345337
========> pt_279:  2.898167185485363
========> pt_280:  3.007109984755516
========> pt_281:  3.1139830499887466
========> pt_282:  3.104238323867321
========> pt_283:  6.205114796757698
========> pt_284:  2.7506736665964127
========> pt_285:  3.07270310819149
========> pt_286:  3.6548011377453804
========> pt_287:  4.447091519832611
========> pt_288:  2.870083786547184
========> pt_289:  3.628833629190922
========> pt_290:  4.157918617129326
========> pt_291:  3.1675051152706146
========> pt_292:  3.0620190873742104
========> pt_293:  3.39895311743021
========> pt_294:  3.154965713620186
========> pt_295:  3.298458233475685
========> pt_296:  2.6249059662222862
========> pt_297:  4.3815333396196365
========> pt_298:  3.585171289741993
========> pt_299:  3.0287862569093704
========> pt_300:  5.284633636474609
========> pt_301:  3.6466768383979797
========> pt_302:  5.289010927081108
========> pt_303:  3.241489566862583
========> pt_304:  3.185095377266407
========> pt_305:  3.6299074813723564
========> pt_306:  2.9692479595541954
========> pt_307:  3.3609312400221825
========> pt_308:  3.6268728971481323
========> pt_309:  2.608903795480728
========> pt_310:  4.128444008529186
========> pt_311:  2.847985066473484
========> pt_312:  2.7904919907450676
========> pt_313:  2.780146710574627
========> pt_314:  3.7339910864830017
========> pt_315:  3.2926686108112335
========> pt_316:  4.125900976359844
========> pt_317:  4.674254804849625
========> pt_318:  5.299539789557457
========> pt_319:  2.6897822320461273
========> pt_320:  2.6567215099930763
========> pt_321:  4.443608671426773
========> pt_322:  3.4841400012373924
========> pt_323:  7.844129279255867
========> pt_324:  3.257622644305229
========> pt_325:  3.7624554708600044
========> pt_326:  2.9497290402650833
========> pt_327:  3.0310317501425743
========> pt_328:  3.112657032907009
========> pt_329:  7.858559265732765
========> pt_330:  6.991891711950302
========> pt_331:  3.6168264597654343
========> pt_332:  2.7451234310865402
========> pt_333:  2.664939947426319
========> pt_334:  2.4345970898866653
========> pt_335:  4.660844951868057
========> pt_336:  4.180878661572933
========> pt_337:  3.4132182970643044
========> pt_338:  2.9131093248724937
========> pt_339:  3.3528098091483116
========> pt_340:  3.4439099207520485
===============================================> mean Dose score: 3.6374852165579794
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.049925896779,     best is           0.049925896779
            Average val evaluation index is   -3.637485216558,     best is           -3.637485216558
    Train use time   1550.35858
    Train loader use time     74.22287
    Val use time     45.17400
    Total use time   1600.67341
    End lr is 0.000249654653, 0.000249654653
    time: 10:54:27
Epoch: 43, iter: 21499
    Begin lr is 0.000249654653, 0.000249654653
========> pt_241:  2.9208552464842796
========> pt_242:  2.810625545680523
========> pt_243:  4.48180265724659
========> pt_244:  3.2345037907361984
========> pt_245:  3.6891451105475426
========> pt_246:  3.854854218661785
========> pt_247:  2.9077158123254776
========> pt_248:  2.7599620446562767
========> pt_249:  4.0979959443211555
========> pt_250:  3.0085622146725655
========> pt_251:  3.4504755958914757
========> pt_252:  3.654260039329529
========> pt_253:  4.373539946973324
========> pt_254:  3.8178803771734238
========> pt_255:  3.3042340353131294
========> pt_256:  2.647721543908119
========> pt_257:  2.9699280485510826
========> pt_258:  2.9458633810281754
========> pt_259:  2.9358093813061714
========> pt_260:  5.558254197239876
========> pt_261:  4.39332902431488
========> pt_262:  3.5136766731739044
========> pt_263:  3.2792558893561363
========> pt_264:  3.9461535587906837
========> pt_265:  2.9489636793732643
========> pt_266:  3.632277362048626
========> pt_267:  3.7269797548651695
========> pt_268:  4.190968386828899
========> pt_269:  2.986113540828228
========> pt_270:  5.1228345185518265
========> pt_271:  3.4726309031248093
========> pt_272:  3.354102447628975
========> pt_273:  3.1745323538780212
========> pt_274:  3.6667926609516144
========> pt_275:  3.561059944331646
========> pt_276:  2.6306989789009094
========> pt_277:  2.8092945739626884
========> pt_278:  5.0747766345739365
========> pt_279:  3.248675614595413
========> pt_280:  3.0941765010356903
========> pt_281:  3.0206387490034103
========> pt_282:  3.1196871399879456
========> pt_283:  6.953837499022484
========> pt_284:  2.6340024173259735
========> pt_285:  2.9547063633799553
========> pt_286:  3.8007287308573723
========> pt_287:  4.725554585456848
========> pt_288:  2.660345695912838
========> pt_289:  4.066468812525272
========> pt_290:  4.216782562434673
========> pt_291:  2.8619002923369408
========> pt_292:  3.422204442322254
========> pt_293:  3.433515354990959
========> pt_294:  2.8786054998636246
========> pt_295:  3.520658276975155
========> pt_296:  2.5345678254961967
========> pt_297:  4.4356802105903625
========> pt_298:  3.4627696126699448
========> pt_299:  2.92923666536808
========> pt_300:  4.795899465680122
========> pt_301:  3.7188150361180305
========> pt_302:  5.084376633167267
========> pt_303:  3.162086308002472
========> pt_304:  3.228449746966362
========> pt_305:  3.6007309332489967
========> pt_306:  2.955235205590725
========> pt_307:  3.1529147550463676
========> pt_308:  3.8819586858153343
========> pt_309:  2.648121304810047
========> pt_310:  3.9258064329624176
========> pt_311:  2.7381770312786102
========> pt_312:  2.9733769968152046
========> pt_313:  2.917843349277973
========> pt_314:  4.103403016924858
========> pt_315:  3.514515832066536
========> pt_316:  4.753346964716911
========> pt_317:  4.485413283109665
========> pt_318:  5.430063679814339
========> pt_319:  2.825314737856388
========> pt_320:  2.598189003765583
========> pt_321:  4.105580449104309
========> pt_322:  3.4985433891415596
========> pt_323:  8.040192052721977
========> pt_324:  3.330385647714138
========> pt_325:  3.770674169063568
========> pt_326:  3.1863702833652496
========> pt_327:  3.054971508681774
========> pt_328:  3.274444416165352
========> pt_329:  7.348829433321953
========> pt_330:  6.899437680840492
========> pt_331:  3.7545111030340195
========> pt_332:  2.9133640974760056
========> pt_333:  2.6719098165631294
========> pt_334:  2.5093985348939896
========> pt_335:  4.544497057795525
========> pt_336:  4.31338258087635
========> pt_337:  3.470410704612732
========> pt_338:  2.917100675404072
========> pt_339:  3.0392147228121758
========> pt_340:  3.3559857308864594
===============================================> mean Dose score: 3.6537641532719136
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.049368772283,     best is           0.049368772283
            Average val evaluation index is   -3.653764153272,     best is           -3.637485216558
    Train use time   1532.95583
    Train loader use time     74.82075
    Val use time     45.57435
    Total use time   1581.88576
    End lr is 0.000247434735, 0.000247434735
    time: 11:20:49
Epoch: 44, iter: 21999
    Begin lr is 0.000247434735, 0.000247434735
========> pt_241:  3.1973901763558388
========> pt_242:  3.0051690712571144
========> pt_243:  4.34197474271059
========> pt_244:  3.1293898820877075
========> pt_245:  3.5813304036855698
========> pt_246:  3.81684772670269
========> pt_247:  2.977128177881241
========> pt_248:  2.8610992059111595
========> pt_249:  3.6161312460899353
========> pt_250:  2.8071914613246918
========> pt_251:  3.3763988316059113
========> pt_252:  3.5255156457424164
========> pt_253:  3.761257752776146
========> pt_254:  3.5311362892389297
========> pt_255:  3.479435443878174
========> pt_256:  2.5880833715200424
========> pt_257:  3.4795068949460983
========> pt_258:  2.655281536281109
========> pt_259:  3.387191593647003
========> pt_260:  4.85417902469635
========> pt_261:  3.785458542406559
========> pt_262:  3.6073555424809456
========> pt_263:  3.262753300368786
========> pt_264:  3.4969620779156685
========> pt_265:  2.90800292044878
========> pt_266:  3.537442237138748
========> pt_267:  3.4287727251648903
========> pt_268:  3.818923458456993
========> pt_269:  2.9801304265856743
========> pt_270:  5.148859396576881
========> pt_271:  3.733118548989296
========> pt_272:  3.9564748480916023
========> pt_273:  2.916140779852867
========> pt_274:  3.770933113992214
========> pt_275:  3.579542823135853
========> pt_276:  2.6757342740893364
========> pt_277:  2.7206921204924583
========> pt_278:  4.352420940995216
========> pt_279:  2.7724414691329002
========> pt_280:  3.0157164484262466
========> pt_281:  2.972084619104862
========> pt_282:  2.912059724330902
========> pt_283:  5.443312376737595
========> pt_284:  2.9083148017525673
========> pt_285:  3.0799462646245956
========> pt_286:  3.4319765493273735
========> pt_287:  4.145652242004871
========> pt_288:  2.5234707444906235
========> pt_289:  3.7930378317832947
========> pt_290:  3.7860864773392677
========> pt_291:  3.0003437772393227
========> pt_292:  3.225593790411949
========> pt_293:  2.9407572373747826
========> pt_294:  2.8525485470891
========> pt_295:  3.2876041904091835
========> pt_296:  2.341662719845772
========> pt_297:  4.14641760289669
========> pt_298:  3.8543863967061043
========> pt_299:  2.7932660654187202
========> pt_300:  5.323975011706352
========> pt_301:  3.668973743915558
========> pt_302:  5.0223951786756516
========> pt_303:  3.097640573978424
========> pt_304:  3.0194929242134094
========> pt_305:  3.5246994346380234
========> pt_306:  2.775009274482727
========> pt_307:  3.0998456478118896
========> pt_308:  3.403223752975464
========> pt_309:  2.6706041395664215
========> pt_310:  4.114832319319248
========> pt_311:  2.712728977203369
========> pt_312:  2.6868942007422447
========> pt_313:  2.7570215985178947
========> pt_314:  3.6361685767769814
========> pt_315:  3.2699054479599
========> pt_316:  3.809174560010433
========> pt_317:  5.261392220854759
========> pt_318:  4.942266196012497
========> pt_319:  2.6385025307536125
========> pt_320:  2.6965437456965446
========> pt_321:  3.706510066986084
========> pt_322:  3.1314486637711525
========> pt_323:  7.511187121272087
========> pt_324:  3.219631277024746
========> pt_325:  3.7569401785731316
========> pt_326:  3.116266094148159
========> pt_327:  2.9058554768562317
========> pt_328:  3.0267712846398354
========> pt_329:  5.389067977666855
========> pt_330:  7.5507089495658875
========> pt_331:  3.5831409320235252
========> pt_332:  2.7829445153474808
========> pt_333:  2.5777792930603027
========> pt_334:  2.4857372790575027
========> pt_335:  4.825246036052704
========> pt_336:  4.326160326600075
========> pt_337:  2.937711179256439
========> pt_338:  2.783656418323517
========> pt_339:  3.0163678526878357
========> pt_340:  3.370763063430786
===============================================> mean Dose score: 3.5101322047412395
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.049964015469,     best is           0.049368772283
            Average val evaluation index is   -3.510132204741,     best is           -3.510132204741
    Train use time   1539.03018
    Train loader use time     75.15443
    Val use time     44.89079
    Total use time   1587.18237
    End lr is 0.000245177273, 0.000245177273
    time: 11:47:16
Epoch: 45, iter: 22499
    Begin lr is 0.000245177273, 0.000245177273
========> pt_241:  3.368402309715748
========> pt_242:  3.202214166522026
========> pt_243:  4.034883007407188
========> pt_244:  3.3845390379428864
========> pt_245:  3.4366432949900627
========> pt_246:  3.5924778133630753
========> pt_247:  3.593919612467289
========> pt_248:  2.5815604627132416
========> pt_249:  3.8179755583405495
========> pt_250:  3.2854004204273224
========> pt_251:  3.0255279317498207
========> pt_252:  3.5259704291820526
========> pt_253:  4.176418706774712
========> pt_254:  3.4119869396090508
========> pt_255:  3.9271368831396103
========> pt_256:  3.024001382291317
========> pt_257:  3.480132482945919
========> pt_258:  2.998270131647587
========> pt_259:  3.5479525849223137
========> pt_260:  5.673737898468971
========> pt_261:  3.6984598264098167
========> pt_262:  3.3847132325172424
========> pt_263:  3.5038278996944427
========> pt_264:  3.305377773940563
========> pt_265:  2.862110733985901
========> pt_266:  3.459959030151367
========> pt_267:  3.4514636546373367
========> pt_268:  3.6793584004044533
========> pt_269:  3.073311746120453
========> pt_270:  4.923949688673019
========> pt_271:  4.21472143381834
========> pt_272:  4.330884180963039
========> pt_273:  3.0287427082657814
========> pt_274:  3.8977759703993797
========> pt_275:  3.8504602387547493
========> pt_276:  2.957409769296646
========> pt_277:  2.552504390478134
========> pt_278:  4.441253915429115
========> pt_279:  2.9746659845113754
========> pt_280:  3.1583797186613083
========> pt_281:  2.886539176106453
========> pt_282:  3.15209724009037
========> pt_283:  6.500081494450569
========> pt_284:  3.214126154780388
========> pt_285:  2.822330743074417
========> pt_286:  3.2668234035372734
========> pt_287:  4.169658496975899
========> pt_288:  2.72755715996027
========> pt_289:  3.8227857276797295
========> pt_290:  3.6214501783251762
========> pt_291:  3.266066387295723
========> pt_292:  2.9849163442850113
========> pt_293:  3.0580175668001175
========> pt_294:  3.317106179893017
========> pt_295:  3.5454601421952248
========> pt_296:  2.750203497707844
========> pt_297:  3.9197299629449844
========> pt_298:  3.5202084481716156
========> pt_299:  2.7898426726460457
========> pt_300:  4.968244656920433
========> pt_301:  3.710823990404606
========> pt_302:  4.852970615029335
========> pt_303:  2.9663286358118057
========> pt_304:  2.9905080422759056
========> pt_305:  3.5352741926908493
========> pt_306:  2.8410151973366737
========> pt_307:  3.038671538233757
========> pt_308:  3.5991457104682922
========> pt_309:  2.693749852478504
========> pt_310:  3.9323103055357933
========> pt_311:  3.086286634206772
========> pt_312:  2.621748298406601
========> pt_313:  2.8358665481209755
========> pt_314:  3.475237563252449
========> pt_315:  3.0183809995651245
========> pt_316:  3.91832884401083
========> pt_317:  4.777858331799507
========> pt_318:  5.005255788564682
========> pt_319:  2.6782671362161636
========> pt_320:  2.4495908617973328
========> pt_321:  3.8729657605290413
========> pt_322:  3.0903491750359535
========> pt_323:  7.197409495711327
========> pt_324:  3.165404349565506
========> pt_325:  3.730950765311718
========> pt_326:  3.203338608145714
========> pt_327:  2.932813912630081
========> pt_328:  3.1557610630989075
========> pt_329:  4.841064363718033
========> pt_330:  8.603453859686852
========> pt_331:  4.015981070697308
========> pt_332:  2.874966971576214
========> pt_333:  2.5909560173749924
========> pt_334:  2.652997449040413
========> pt_335:  4.658332169055939
========> pt_336:  4.1562966257333755
========> pt_337:  3.168773762881756
========> pt_338:  2.9994774982333183
========> pt_339:  3.2637929916381836
========> pt_340:  3.28615665435791
===============================================> mean Dose score: 3.5763058863580226
        ==> Saving latest model successfully !
            Average train loss is             0.049593908548,     best is           0.049368772283
            Average val evaluation index is   -3.576305886358,     best is           -3.510132204741
    Train use time   1538.13975
    Train loader use time     73.32757
    Val use time     45.51514
    Total use time   1585.31865
    End lr is 0.000242883138, 0.000242883138
    time: 12:13:41
Epoch: 46, iter: 22999
    Begin lr is 0.000242883138, 0.000242883138
========> pt_241:  3.9586366340517998
========> pt_242:  3.822424039244652
========> pt_243:  4.717905148863792
========> pt_244:  3.717784732580185
========> pt_245:  5.021028742194176
========> pt_246:  4.911387339234352
========> pt_247:  4.718513786792755
========> pt_248:  3.3141211420297623
========> pt_249:  4.2317356541752815
========> pt_250:  4.211981780827045
========> pt_251:  4.0659623965620995
========> pt_252:  3.7069932743906975
========> pt_253:  6.002126485109329
========> pt_254:  4.327654540538788
========> pt_255:  3.8494547083973885
========> pt_256:  4.762699231505394
========> pt_257:  4.569090306758881
========> pt_258:  4.043900445103645
========> pt_259:  3.5324787348508835
========> pt_260:  8.659139797091484
========> pt_261:  4.406695067882538
========> pt_262:  3.721993826329708
========> pt_263:  4.287015832960606
========> pt_264:  4.280736744403839
========> pt_265:  3.8534971699118614
========> pt_266:  3.892269805073738
========> pt_267:  4.345494620501995
========> pt_268:  4.508843496441841
========> pt_269:  3.4087038412690163
========> pt_270:  6.386899873614311
========> pt_271:  5.58168962597847
========> pt_272:  4.7268229722976685
========> pt_273:  3.6406929418444633
========> pt_274:  3.8421685248613358
========> pt_275:  4.608936011791229
========> pt_276:  3.3922328054904938
========> pt_277:  3.295222595334053
========> pt_278:  4.676977246999741
========> pt_279:  4.459821283817291
========> pt_280:  5.262318477034569
========> pt_281:  3.2221518829464912
========> pt_282:  3.5967961698770523
========> pt_283:  7.6653748750686646
========> pt_284:  3.573724515736103
========> pt_285:  3.7744861096143723
========> pt_286:  4.261800646781921
========> pt_287:  5.996079742908478
========> pt_288:  3.7989192456007004
========> pt_289:  4.481396898627281
========> pt_290:  5.0351473689079285
========> pt_291:  3.655526079237461
========> pt_292:  4.508872181177139
========> pt_293:  3.6994966492056847
========> pt_294:  4.191494360566139
========> pt_295:  3.778977356851101
========> pt_296:  3.439653106033802
========> pt_297:  4.605092778801918
========> pt_298:  4.406363368034363
========> pt_299:  4.331204146146774
========> pt_300:  6.276439651846886
========> pt_301:  4.992342442274094
========> pt_302:  5.178033858537674
========> pt_303:  3.3159327134490013
========> pt_304:  3.9834580570459366
========> pt_305:  4.660768806934357
========> pt_306:  3.738678954541683
========> pt_307:  4.153731428086758
========> pt_308:  4.782998636364937
========> pt_309:  3.3876416832208633
========> pt_310:  6.280645877122879
========> pt_311:  3.3395731076598167
========> pt_312:  3.275061398744583
========> pt_313:  3.0601243302226067
========> pt_314:  4.245850369334221
========> pt_315:  3.7737056240439415
========> pt_316:  4.069453328847885
========> pt_317:  5.730671361088753
========> pt_318:  6.875539124011993
========> pt_319:  3.1571869552135468
========> pt_320:  2.915668524801731
========> pt_321:  4.725410640239716
========> pt_322:  4.349816367030144
========> pt_323:  8.0556171387434
========> pt_324:  4.001264236867428
========> pt_325:  4.164460301399231
========> pt_326:  3.626492954790592
========> pt_327:  5.762505158782005
========> pt_328:  3.8618240877985954
========> pt_329:  8.189221248030663
========> pt_330:  7.730254530906677
========> pt_331:  5.141023248434067
========> pt_332:  3.320346511900425
========> pt_333:  3.371594399213791
========> pt_334:  3.583260625600815
========> pt_335:  4.542388468980789
========> pt_336:  5.258967578411102
========> pt_337:  3.8528960943222046
========> pt_338:  3.429686464369297
========> pt_339:  4.811967611312866
========> pt_340:  4.603831171989441
===============================================> mean Dose score: 4.463229442387819
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.048844680168,     best is           0.048844680168
            Average val evaluation index is   -4.463229442388,     best is           -3.510132204741
    Train use time   1536.34136
    Train loader use time     76.13589
    Val use time     45.32834
    Total use time   1585.44925
    End lr is 0.000240553213, 0.000240553213
    time: 12:40:07
Epoch: 47, iter: 23499
    Begin lr is 0.000240553213, 0.000240553213
========> pt_241:  3.063485138118267
========> pt_242:  2.67726369202137
========> pt_243:  4.792639836668968
========> pt_244:  3.108736090362072
========> pt_245:  3.8853158429265022
========> pt_246:  3.863338641822338
========> pt_247:  2.8889544308185577
========> pt_248:  2.6821893826127052
========> pt_249:  4.61588554084301
========> pt_250:  2.6876913756132126
========> pt_251:  3.836335353553295
========> pt_252:  3.6589160934090614
========> pt_253:  4.207574762403965
========> pt_254:  4.265071228146553
========> pt_255:  3.159342482686043
========> pt_256:  2.737532928586006
========> pt_257:  3.8299381360411644
========> pt_258:  2.775999940931797
========> pt_259:  3.0867964401841164
========> pt_260:  4.512617886066437
========> pt_261:  4.412456527352333
========> pt_262:  3.861139565706253
========> pt_263:  3.4224558249115944
========> pt_264:  4.2242880538105965
========> pt_265:  3.140428029000759
========> pt_266:  3.6446407437324524
========> pt_267:  3.8019540905952454
========> pt_268:  4.113249443471432
========> pt_269:  3.1990426778793335
========> pt_270:  5.36206990480423
========> pt_271:  4.10582322627306
========> pt_272:  3.549419939517975
========> pt_273:  3.1270937994122505
========> pt_274:  3.9715948328375816
========> pt_275:  3.7021244317293167
========> pt_276:  2.78943482786417
========> pt_277:  2.949015572667122
========> pt_278:  4.955751150846481
========> pt_279:  3.0167605727910995
========> pt_280:  3.086135648190975
========> pt_281:  3.380187042057514
========> pt_282:  3.128029964864254
========> pt_283:  5.552359223365784
========> pt_284:  2.6674532517790794
========> pt_285:  3.078523501753807
========> pt_286:  3.6341606453061104
========> pt_287:  4.768003299832344
========> pt_288:  2.7932345122098923
========> pt_289:  4.205610118806362
========> pt_290:  4.5568836480379105
========> pt_291:  2.8392552584409714
========> pt_292:  3.4034563601017
========> pt_293:  3.642096668481827
========> pt_294:  3.0327648296952248
========> pt_295:  3.215094916522503
========> pt_296:  2.6187551766633987
========> pt_297:  4.891683533787727
========> pt_298:  3.3960090205073357
========> pt_299:  3.031647689640522
========> pt_300:  4.822396859526634
========> pt_301:  3.666120395064354
========> pt_302:  5.670537203550339
========> pt_303:  3.478581942617893
========> pt_304:  3.2290883734822273
========> pt_305:  3.7156477198004723
========> pt_306:  2.8120162338018417
========> pt_307:  3.3306818827986717
========> pt_308:  4.040503390133381
========> pt_309:  2.743251882493496
========> pt_310:  3.623826317489147
========> pt_311:  2.4588296934962273
========> pt_312:  2.7883030846714973
========> pt_313:  2.7098409458994865
========> pt_314:  3.9163094386458397
========> pt_315:  3.466305136680603
========> pt_316:  4.460751190781593
========> pt_317:  4.157867506146431
========> pt_318:  5.220157653093338
========> pt_319:  2.9214469343423843
========> pt_320:  2.7529092505574226
========> pt_321:  4.539421424269676
========> pt_322:  3.5681041330099106
========> pt_323:  8.09860147535801
========> pt_324:  3.4020747989416122
========> pt_325:  3.8153808936476707
========> pt_326:  3.0588804557919502
========> pt_327:  3.05978637188673
========> pt_328:  2.9522384330630302
========> pt_329:  7.316524162888527
========> pt_330:  6.529011353850365
========> pt_331:  3.631388656795025
========> pt_332:  2.873038835823536
========> pt_333:  2.752637527883053
========> pt_334:  2.23556075245142
========> pt_335:  5.250385627150536
========> pt_336:  3.9306053891777992
========> pt_337:  3.2641804963350296
========> pt_338:  2.872738689184189
========> pt_339:  3.187972977757454
========> pt_340:  3.4535062685608864
===============================================> mean Dose score: 3.6838312450796367
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.048610739410,     best is           0.048610739410
            Average val evaluation index is   -3.683831245080,     best is           -3.510132204741
    Train use time   1534.55887
    Train loader use time     73.89439
    Val use time     45.05047
    Total use time   1582.90083
    End lr is 0.000238188399, 0.000238188399
    time: 13:06:30
Epoch: 48, iter: 23999
    Begin lr is 0.000238188399, 0.000238188399
========> pt_241:  3.016367591917515
========> pt_242:  2.7603044360876083
========> pt_243:  4.0804630517959595
========> pt_244:  3.145620748400688
========> pt_245:  3.524533584713936
========> pt_246:  3.588845282793045
========> pt_247:  2.8964781761169434
========> pt_248:  2.8012876212596893
========> pt_249:  3.714700862765312
========> pt_250:  2.96891026198864
========> pt_251:  3.534684330224991
========> pt_252:  3.780147172510624
========> pt_253:  4.303102232515812
========> pt_254:  3.6179201304912567
========> pt_255:  3.418051414191723
========> pt_256:  2.707260102033615
========> pt_257:  3.1261153891682625
========> pt_258:  2.824164479970932
========> pt_259:  3.0351201072335243
========> pt_260:  4.9053677171468735
========> pt_261:  4.21599842607975
========> pt_262:  3.6399226263165474
========> pt_263:  3.2379018887877464
========> pt_264:  3.82069930434227
========> pt_265:  2.9180042445659637
========> pt_266:  3.757328726351261
========> pt_267:  3.614150434732437
========> pt_268:  3.73150072991848
========> pt_269:  2.8212670609354973
========> pt_270:  5.1389699429273605
========> pt_271:  3.3344698324799538
========> pt_272:  3.525601178407669
========> pt_273:  3.0937686562538147
========> pt_274:  3.6718443036079407
========> pt_275:  3.6465438455343246
========> pt_276:  2.579573392868042
========> pt_277:  2.8075674921274185
========> pt_278:  4.441248178482056
========> pt_279:  3.352673426270485
========> pt_280:  2.9112831503152847
========> pt_281:  3.032745011150837
========> pt_282:  2.9212213680148125
========> pt_283:  6.211958974599838
========> pt_284:  2.682146355509758
========> pt_285:  2.9919206351041794
========> pt_286:  3.6274265125393867
========> pt_287:  4.639672487974167
========> pt_288:  2.6933276653289795
========> pt_289:  3.629731461405754
========> pt_290:  4.105318374931812
========> pt_291:  2.7265597134828568
========> pt_292:  3.3541783317923546
========> pt_293:  3.1818201020359993
========> pt_294:  2.8180204704403877
========> pt_295:  3.4132566303014755
========> pt_296:  2.4504219368100166
========> pt_297:  4.231613874435425
========> pt_298:  3.580648750066757
========> pt_299:  2.932245694100857
========> pt_300:  5.134495124220848
========> pt_301:  3.6029965057969093
========> pt_302:  4.727283492684364
========> pt_303:  2.8912752866744995
========> pt_304:  3.1278036162257195
========> pt_305:  3.541891761124134
========> pt_306:  2.927868925035
========> pt_307:  3.153361976146698
========> pt_308:  3.4741730988025665
========> pt_309:  2.5370266288518906
========> pt_310:  4.165191240608692
========> pt_311:  2.7511193230748177
========> pt_312:  2.8274037688970566
========> pt_313:  2.9221653565764427
========> pt_314:  3.864493854343891
========> pt_315:  3.38765237480402
========> pt_316:  4.226657673716545
========> pt_317:  4.43299688398838
========> pt_318:  5.318695455789566
========> pt_319:  2.689712606370449
========> pt_320:  2.644423320889473
========> pt_321:  3.623918630182743
========> pt_322:  3.219180926680565
========> pt_323:  8.043067306280136
========> pt_324:  3.288508541882038
========> pt_325:  3.8749877735972404
========> pt_326:  3.153766952455044
========> pt_327:  3.1252973526716232
========> pt_328:  3.3243774995207787
========> pt_329:  5.455889850854874
========> pt_330:  6.928755566477776
========> pt_331:  3.6320914328098297
========> pt_332:  2.6983310654759407
========> pt_333:  2.6465824991464615
========> pt_334:  2.410663329064846
========> pt_335:  4.399846717715263
========> pt_336:  4.048459753394127
========> pt_337:  3.1920503824949265
========> pt_338:  2.8192202746868134
========> pt_339:  3.062225617468357
========> pt_340:  3.4024138003587723
===============================================> mean Dose score: 3.5393231943249703
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.048408130072,     best is           0.048408130072
            Average val evaluation index is   -3.539323194325,     best is           -3.510132204741
    Train use time   1536.39928
    Train loader use time     73.71121
    Val use time     44.69005
    Total use time   1584.44570
    End lr is 0.000235789605, 0.000235789605
    time: 13:32:54
Epoch: 49, iter: 24499
    Begin lr is 0.000235789605, 0.000235789605
========> pt_241:  3.0797499045729637
========> pt_242:  2.71310918033123
========> pt_243:  4.281739667057991
========> pt_244:  2.94343039393425
========> pt_245:  3.6813493818044662
========> pt_246:  3.7992222607135773
========> pt_247:  3.1164585426449776
========> pt_248:  2.6629846915602684
========> pt_249:  3.7260806187987328
========> pt_250:  2.6193713769316673
========> pt_251:  3.589578829705715
========> pt_252:  3.6067813262343407
========> pt_253:  3.772827610373497
========> pt_254:  3.624393753707409
========> pt_255:  3.4976674616336823
========> pt_256:  2.630322426557541
========> pt_257:  3.4100833162665367
========> pt_258:  2.71335456520319
========> pt_259:  3.2515307888388634
========> pt_260:  5.129295885562897
========> pt_261:  3.849552758038044
========> pt_262:  3.6569274589419365
========> pt_263:  3.232925869524479
========> pt_264:  3.755584955215454
========> pt_265:  2.9581305384635925
========> pt_266:  3.48953690379858
========> pt_267:  3.4474696964025497
========> pt_268:  3.9021164923906326
========> pt_269:  2.8162652254104614
========> pt_270:  5.04573255777359
========> pt_271:  3.8303178176283836
========> pt_272:  3.6834968253970146
========> pt_273:  2.9471229016780853
========> pt_274:  3.9163459464907646
========> pt_275:  3.7160881608724594
========> pt_276:  2.7636438608169556
========> pt_277:  2.7033623680472374
========> pt_278:  4.17678065598011
========> pt_279:  2.793791778385639
========> pt_280:  2.924574352800846
========> pt_281:  3.092074431478977
========> pt_282:  2.899252250790596
========> pt_283:  6.4138296246528625
========> pt_284:  2.848268784582615
========> pt_285:  3.024296313524246
========> pt_286:  3.352809026837349
========> pt_287:  4.425938352942467
========> pt_288:  2.7288122475147247
========> pt_289:  3.788759373128414
========> pt_290:  3.814663253724575
========> pt_291:  2.924431972205639
========> pt_292:  2.9772410914301872
========> pt_293:  3.0188099667429924
========> pt_294:  2.988377809524536
========> pt_295:  3.1479155272245407
========> pt_296:  2.37709853798151
========> pt_297:  4.1977038234472275
========> pt_298:  3.5268935561180115
========> pt_299:  2.8025226294994354
========> pt_300:  5.040677264332771
========> pt_301:  3.438304141163826
========> pt_302:  4.892286956310272
========> pt_303:  3.242107853293419
========> pt_304:  2.953297942876816
========> pt_305:  3.5275259241461754
========> pt_306:  2.7074309065937996
========> pt_307:  3.1072259694337845
========> pt_308:  3.573383167386055
========> pt_309:  2.497890219092369
========> pt_310:  4.030309617519379
========> pt_311:  2.6531022787094116
========> pt_312:  2.5375473871827126
========> pt_313:  2.586713805794716
========> pt_314:  3.5356799513101578
========> pt_315:  3.1848421692848206
========> pt_316:  3.9694127067923546
========> pt_317:  4.407526925206184
========> pt_318:  5.116801336407661
========> pt_319:  2.581985257565975
========> pt_320:  2.6185139641165733
========> pt_321:  3.847641311585903
========> pt_322:  3.184584267437458
========> pt_323:  7.553893998265266
========> pt_324:  3.2061053812503815
========> pt_325:  3.6859266832470894
========> pt_326:  2.8666940331459045
========> pt_327:  2.897840179502964
========> pt_328:  2.835061550140381
========> pt_329:  6.164741292595863
========> pt_330:  7.339391112327576
========> pt_331:  3.561753071844578
========> pt_332:  2.6496702805161476
========> pt_333:  2.5182662904262543
========> pt_334:  2.2790799289941788
========> pt_335:  4.8919521272182465
========> pt_336:  3.8490844145417213
========> pt_337:  3.1013257801532745
========> pt_338:  2.713179588317871
========> pt_339:  2.993593215942383
========> pt_340:  3.138830028474331
===============================================> mean Dose score: 3.4928997796028853
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.048089156777,     best is           0.048089156777
            Average val evaluation index is   -3.492899779603,     best is           -3.492899779603
    Train use time   1539.58365
    Train loader use time     75.20217
    Val use time     44.83988
    Total use time   1590.27264
    End lr is 0.000233357756, 0.000233357756
    time: 13:59:24
Epoch: 50, iter: 24999
    Begin lr is 0.000233357756, 0.000233357756
========> pt_241:  3.004927337169647
========> pt_242:  2.68060103058815
========> pt_243:  4.777970463037491
========> pt_244:  3.1084442883729935
========> pt_245:  4.118590801954269
========> pt_246:  4.217702820897102
========> pt_247:  3.0924898386001587
========> pt_248:  2.85702858120203
========> pt_249:  4.422149881720543
========> pt_250:  2.9479704052209854
========> pt_251:  3.717757873237133
========> pt_252:  3.8851385191082954
========> pt_253:  4.40926156938076
========> pt_254:  4.190421812236309
========> pt_255:  3.284597247838974
========> pt_256:  2.5107991322875023
========> pt_257:  3.5344647616147995
========> pt_258:  2.881566807627678
========> pt_259:  3.1999600678682327
========> pt_260:  4.202004708349705
========> pt_261:  4.977134317159653
========> pt_262:  3.6615970730781555
========> pt_263:  3.1081128492951393
========> pt_264:  4.245839677751064
========> pt_265:  2.956497333943844
========> pt_266:  3.8256294280290604
========> pt_267:  3.556486815214157
========> pt_268:  4.476165845990181
========> pt_269:  3.013632893562317
========> pt_270:  5.432683378458023
========> pt_271:  3.9405902847647667
========> pt_272:  3.376459851861
========> pt_273:  3.2234687730669975
========> pt_274:  4.084829390048981
========> pt_275:  3.769359104335308
========> pt_276:  2.62909471988678
========> pt_277:  2.810436226427555
========> pt_278:  5.09630061686039
========> pt_279:  3.2459020614624023
========> pt_280:  3.2119695842266083
========> pt_281:  3.2446029037237167
========> pt_282:  3.1391507759690285
========> pt_283:  5.453053712844849
========> pt_284:  2.7473365887999535
========> pt_285:  3.039124496281147
========> pt_286:  3.8171742111444473
========> pt_287:  4.975349605083466
========> pt_288:  2.7806492149829865
========> pt_289:  4.081017449498177
========> pt_290:  4.393513649702072
========> pt_291:  3.092753477394581
========> pt_292:  3.4786132350564003
========> pt_293:  3.628312088549137
========> pt_294:  2.8177737817168236
========> pt_295:  3.6735181882977486
========> pt_296:  2.4497324600815773
========> pt_297:  5.002747178077698
========> pt_298:  3.5191505029797554
========> pt_299:  2.7964938804507256
========> pt_300:  5.381244868040085
========> pt_301:  3.928774781525135
========> pt_302:  5.4394639283418655
========> pt_303:  3.3999286592006683
========> pt_304:  3.2510992139577866
========> pt_305:  3.749849572777748
========> pt_306:  2.8209277987480164
========> pt_307:  3.1379426270723343
========> pt_308:  4.073703624308109
========> pt_309:  2.6095181703567505
========> pt_310:  4.091584123671055
========> pt_311:  2.7573788538575172
========> pt_312:  2.817727103829384
========> pt_313:  2.824147008359432
========> pt_314:  3.791414797306061
========> pt_315:  3.2466569915413857
========> pt_316:  4.896757081151009
========> pt_317:  5.0308483093976974
========> pt_318:  5.600870847702026
========> pt_319:  2.8458863869309425
========> pt_320:  2.712325043976307
========> pt_321:  3.7503570318222046
========> pt_322:  3.6317096650600433
========> pt_323:  8.211987540125847
========> pt_324:  3.303283266723156
========> pt_325:  4.070719629526138
========> pt_326:  3.0949288234114647
========> pt_327:  2.991139106452465
========> pt_328:  3.1958985701203346
========> pt_329:  6.842528209090233
========> pt_330:  6.997765824198723
========> pt_331:  3.6563537642359734
========> pt_332:  2.9337558150291443
========> pt_333:  2.6029981300234795
========> pt_334:  2.3767921328544617
========> pt_335:  5.3474318236112595
========> pt_336:  4.768783003091812
========> pt_337:  3.43048807233572
========> pt_338:  2.9299498721957207
========> pt_339:  3.119504600763321
========> pt_340:  3.62563893198967
===============================================> mean Dose score: 3.7310616917908193
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.047929168947,     best is           0.047929168947
            Average val evaluation index is   -3.731061691791,     best is           -3.492899779603
    Train use time   1544.90863
    Train loader use time     77.88055
    Val use time     44.44176
    Total use time   1592.54113
    End lr is 0.000230893792, 0.000230893792
    time: 14:25:57
Epoch: 51, iter: 25499
    Begin lr is 0.000230893792, 0.000230893792
========> pt_241:  3.017108179628849
========> pt_242:  2.6575298979878426
========> pt_243:  4.5146555453538895
========> pt_244:  3.086485080420971
========> pt_245:  3.377387672662735
========> pt_246:  3.6943448707461357
========> pt_247:  2.8792615979909897
========> pt_248:  2.5871628522872925
========> pt_249:  3.711913749575615
========> pt_250:  2.8669827058911324
========> pt_251:  3.57036579400301
========> pt_252:  3.6292722448706627
========> pt_253:  4.362118728458881
========> pt_254:  3.68154339492321
========> pt_255:  3.2450970634818077
========> pt_256:  2.5713153183460236
========> pt_257:  3.4457306191325188
========> pt_258:  2.7631371840834618
========> pt_259:  2.9829712584614754
========> pt_260:  4.5962855219841
========> pt_261:  4.043505899608135
========> pt_262:  3.4222466871142387
========> pt_263:  3.1173595041036606
========> pt_264:  3.848307318985462
========> pt_265:  2.8137443587183952
========> pt_266:  3.3893703296780586
========> pt_267:  3.6142482236027718
========> pt_268:  3.7612321972846985
========> pt_269:  2.8959253430366516
========> pt_270:  5.111340805888176
========> pt_271:  4.014883749186993
========> pt_272:  3.5942135006189346
========> pt_273:  2.9119176045060158
========> pt_274:  3.884655050933361
========> pt_275:  3.356686681509018
========> pt_276:  2.559504508972168
========> pt_277:  2.6545628532767296
========> pt_278:  4.373411126434803
========> pt_279:  2.7881940826773643
========> pt_280:  3.008495718240738
========> pt_281:  2.956126257777214
========> pt_282:  2.7572255209088326
========> pt_283:  5.4779645800590515
========> pt_284:  2.7229683846235275
========> pt_285:  2.890561819076538
========> pt_286:  3.4159055352211
========> pt_287:  4.40789096057415
========> pt_288:  2.59284608066082
========> pt_289:  3.810001462697983
========> pt_290:  4.163733534514904
========> pt_291:  2.7876149117946625
========> pt_292:  3.0836940556764603
========> pt_293:  3.2074274867773056
========> pt_294:  2.7986087277531624
========> pt_295:  3.2843127474188805
========> pt_296:  2.2923875600099564
========> pt_297:  4.531949833035469
========> pt_298:  3.42325821518898
========> pt_299:  2.784450985491276
========> pt_300:  4.991121515631676
========> pt_301:  3.610924445092678
========> pt_302:  4.9387989938259125
========> pt_303:  3.162878006696701
========> pt_304:  3.093024156987667
========> pt_305:  3.6506864428520203
========> pt_306:  2.685825824737549
========> pt_307:  3.0195296928286552
========> pt_308:  3.6352764815092087
========> pt_309:  2.3878641799092293
========> pt_310:  3.797897808253765
========> pt_311:  2.584889195859432
========> pt_312:  2.6182252913713455
========> pt_313:  2.6143984869122505
========> pt_314:  3.713722974061966
========> pt_315:  3.2557909935712814
========> pt_316:  4.134727530181408
========> pt_317:  4.453819394111633
========> pt_318:  5.294344201683998
========> pt_319:  2.4949249997735023
========> pt_320:  2.4943919852375984
========> pt_321:  4.429758638143539
========> pt_322:  3.1392889842391014
========> pt_323:  7.275688052177429
========> pt_324:  3.13142754137516
========> pt_325:  3.874923624098301
========> pt_326:  3.098014257848263
========> pt_327:  2.914571203291416
========> pt_328:  3.078255169093609
========> pt_329:  5.62347024679184
========> pt_330:  7.0100246369838715
========> pt_331:  3.7144815549254417
========> pt_332:  2.5854263827204704
========> pt_333:  2.5492163375020027
========> pt_334:  2.2754687815904617
========> pt_335:  4.859065338969231
========> pt_336:  4.156046025454998
========> pt_337:  3.0131416022777557
========> pt_338:  2.6952771842479706
========> pt_339:  3.0243531614542007
========> pt_340:  3.31685584038496
===============================================> mean Dose score: 3.4819122064858674
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.048538741142,     best is           0.047929168947
            Average val evaluation index is   -3.481912206486,     best is           -3.481912206486
    Train use time   1543.14735
    Train loader use time     72.91216
    Val use time     44.57680
    Total use time   1590.72449
    End lr is 0.000228398660, 0.000228398660
    time: 14:52:28
Epoch: 52, iter: 25999
    Begin lr is 0.000228398660, 0.000228398660
========> pt_241:  3.0194926634430885
========> pt_242:  2.7106355130672455
========> pt_243:  4.742633476853371
========> pt_244:  3.275127373635769
========> pt_245:  3.6923085153102875
========> pt_246:  3.7955717369914055
========> pt_247:  2.9967065528035164
========> pt_248:  2.5463734194636345
========> pt_249:  4.475653693079948
========> pt_250:  2.9439889639616013
========> pt_251:  3.518422171473503
========> pt_252:  3.8214803114533424
========> pt_253:  4.403569996356964
========> pt_254:  4.293188266456127
========> pt_255:  3.8828348740935326
========> pt_256:  2.6682621613144875
========> pt_257:  3.1149891018867493
========> pt_258:  3.0182287096977234
========> pt_259:  3.45047689974308
========> pt_260:  4.215966090559959
========> pt_261:  4.401598572731018
========> pt_262:  3.53387463837862
========> pt_263:  3.30108679831028
========> pt_264:  4.229270331561565
========> pt_265:  2.927703335881233
========> pt_266:  3.5048462077975273
========> pt_267:  3.6957253888249397
========> pt_268:  4.364326149225235
========> pt_269:  2.9139328375458717
========> pt_270:  5.301882028579712
========> pt_271:  3.898261785507202
========> pt_272:  3.9035702869296074
========> pt_273:  3.0116476491093636
========> pt_274:  4.171429388225079
========> pt_275:  3.603891208767891
========> pt_276:  2.7226098254323006
========> pt_277:  2.606690637767315
========> pt_278:  5.3060585260391235
========> pt_279:  2.781303748488426
========> pt_280:  3.274255096912384
========> pt_281:  2.971673905849457
========> pt_282:  3.0163754150271416
========> pt_283:  5.9925807267427444
========> pt_284:  2.993007004261017
========> pt_285:  2.882130853831768
========> pt_286:  3.4088068455457687
========> pt_287:  4.602686911821365
========> pt_288:  2.6323170587420464
========> pt_289:  4.548742398619652
========> pt_290:  4.646727368235588
========> pt_291:  2.766241915524006
========> pt_292:  3.231019377708435
========> pt_293:  3.466189354658127
========> pt_294:  3.053955025970936
========> pt_295:  3.34236778318882
========> pt_296:  2.529017850756645
========> pt_297:  4.598236605525017
========> pt_298:  3.500729165971279
========> pt_299:  2.9098452627658844
========> pt_300:  4.721199721097946
========> pt_301:  3.851427175104618
========> pt_302:  5.542929247021675
========> pt_303:  3.024693988263607
========> pt_304:  3.1778855994343758
========> pt_305:  3.6554212495684624
========> pt_306:  2.8696589916944504
========> pt_307:  3.160196505486965
========> pt_308:  4.087320268154144
========> pt_309:  2.5744132697582245
========> pt_310:  3.835231512784958
========> pt_311:  2.6516899466514587
========> pt_312:  2.7997610718011856
========> pt_313:  2.7454058453440666
========> pt_314:  3.7624695524573326
========> pt_315:  3.139316104352474
========> pt_316:  4.66420941054821
========> pt_317:  4.27670992910862
========> pt_318:  5.310354456305504
========> pt_319:  2.6733164116740227
========> pt_320:  2.3474831134080887
========> pt_321:  4.491085037589073
========> pt_322:  3.4204838797450066
========> pt_323:  7.109117358922958
========> pt_324:  3.1537677347660065
========> pt_325:  4.037469327449799
========> pt_326:  3.0619705840945244
========> pt_327:  2.931024506688118
========> pt_328:  3.017275333404541
========> pt_329:  6.607373356819153
========> pt_330:  7.8894829750061035
========> pt_331:  3.8353266939520836
========> pt_332:  2.6575854420661926
========> pt_333:  2.6863551884889603
========> pt_334:  2.315925993025303
========> pt_335:  4.899337664246559
========> pt_336:  4.2908775806427
========> pt_337:  3.2107671722769737
========> pt_338:  2.8534315153956413
========> pt_339:  3.1845561042428017
========> pt_340:  3.4091098606586456
===============================================> mean Dose score: 3.6506594244390724
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.047809681289,     best is           0.047809681289
            Average val evaluation index is   -3.650659424439,     best is           -3.481912206486
    Train use time   1540.38407
    Train loader use time     74.23263
    Val use time     44.22094
    Total use time   1587.55656
    End lr is 0.000225873323, 0.000225873323
    time: 15:18:55
Epoch: 53, iter: 26499
    Begin lr is 0.000225873323, 0.000225873323
========> pt_241:  3.2332434877753258
========> pt_242:  2.6215801015496254
========> pt_243:  4.732042551040649
========> pt_244:  3.053489290177822
========> pt_245:  3.5750607028603554
========> pt_246:  4.058033414185047
========> pt_247:  2.820621132850647
========> pt_248:  2.5407282635569572
========> pt_249:  4.051830992102623
========> pt_250:  2.716192528605461
========> pt_251:  3.5274161398410797
========> pt_252:  3.8174180313944817
========> pt_253:  4.2001960054039955
========> pt_254:  4.004053696990013
========> pt_255:  3.463447093963623
========> pt_256:  2.5704537332057953
========> pt_257:  3.225877769291401
========> pt_258:  2.719542905688286
========> pt_259:  3.137759305536747
========> pt_260:  4.746166914701462
========> pt_261:  4.49106939136982
========> pt_262:  3.641218915581703
========> pt_263:  3.2209312170743942
========> pt_264:  4.257230646908283
========> pt_265:  2.8238533809781075
========> pt_266:  3.4793512150645256
========> pt_267:  3.7557385489344597
========> pt_268:  4.243537075817585
========> pt_269:  2.9167496785521507
========> pt_270:  5.460379794239998
========> pt_271:  3.8792983070015907
========> pt_272:  3.4221945330500603
========> pt_273:  2.8964296728372574
========> pt_274:  3.9981701970100403
========> pt_275:  3.8150661438703537
========> pt_276:  2.698151655495167
========> pt_277:  2.5924911722540855
========> pt_278:  5.078588575124741
========> pt_279:  2.651607282459736
========> pt_280:  3.075091242790222
========> pt_281:  2.9497360810637474
========> pt_282:  2.9311053454875946
========> pt_283:  5.904048681259155
========> pt_284:  2.677122876048088
========> pt_285:  3.012085221707821
========> pt_286:  3.620043583214283
========> pt_287:  4.509269595146179
========> pt_288:  2.575891837477684
========> pt_289:  4.339750371873379
========> pt_290:  4.279147349298
========> pt_291:  2.6880014315247536
========> pt_292:  3.160567581653595
========> pt_293:  3.463175892829895
========> pt_294:  2.897496223449707
========> pt_295:  3.4763604402542114
========> pt_296:  2.377614602446556
========> pt_297:  4.433130919933319
========> pt_298:  3.446236252784729
========> pt_299:  2.8359176591038704
========> pt_300:  5.11536605656147
========> pt_301:  3.666261211037636
========> pt_302:  5.365794748067856
========> pt_303:  3.285756893455982
========> pt_304:  3.117564469575882
========> pt_305:  3.671567365527153
========> pt_306:  2.6937049999833107
========> pt_307:  3.086419366300106
========> pt_308:  3.6775635182857513
========> pt_309:  2.5968168303370476
========> pt_310:  4.024594835937023
========> pt_311:  2.426721826195717
========> pt_312:  2.6797563955187798
========> pt_313:  2.688969150185585
========> pt_314:  3.8006551936268806
========> pt_315:  3.2459982857108116
========> pt_316:  4.651123955845833
========> pt_317:  4.6328554302453995
========> pt_318:  5.4040612280368805
========> pt_319:  2.68583495169878
========> pt_320:  2.440403923392296
========> pt_321:  4.630955457687378
========> pt_322:  3.352918289601803
========> pt_323:  7.582962587475777
========> pt_324:  3.101154714822769
========> pt_325:  3.8517961651086807
========> pt_326:  3.115214928984642
========> pt_327:  2.9004622250795364
========> pt_328:  2.9338791593909264
========> pt_329:  6.770448163151741
========> pt_330:  7.325856611132622
========> pt_331:  3.8549507036805153
========> pt_332:  2.5633391365408897
========> pt_333:  2.6010144501924515
========> pt_334:  2.192820757627487
========> pt_335:  4.818612039089203
========> pt_336:  4.119732715189457
========> pt_337:  3.101235553622246
========> pt_338:  2.717409022152424
========> pt_339:  3.0427851900458336
========> pt_340:  3.2788031920790672
===============================================> mean Dose score: 3.596031463518739
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.047006361835,     best is           0.047006361835
            Average val evaluation index is   -3.596031463519,     best is           -3.481912206486
    Train use time   1535.21507
    Train loader use time     71.70341
    Val use time     43.95307
    Total use time   1582.34302
    End lr is 0.000223318755, 0.000223318755
    time: 15:45:18
Epoch: 54, iter: 26999
    Begin lr is 0.000223318755, 0.000223318755
========> pt_241:  3.006606437265873
========> pt_242:  2.6475390046834946
========> pt_243:  4.647471085190773
========> pt_244:  3.0608737841248512
========> pt_245:  3.7677937000989914
========> pt_246:  3.957921341061592
========> pt_247:  2.7589333057403564
========> pt_248:  2.6799123361706734
========> pt_249:  3.7831024825572968
========> pt_250:  2.855069935321808
========> pt_251:  3.772123008966446
========> pt_252:  3.7282875180244446
========> pt_253:  4.023122526705265
========> pt_254:  3.769974522292614
========> pt_255:  3.2114076241850853
========> pt_256:  2.499348968267441
========> pt_257:  3.111943304538727
========> pt_258:  2.7995897457003593
========> pt_259:  2.8684886544942856
========> pt_260:  4.404076412320137
========> pt_261:  4.32412214577198
========> pt_262:  3.5738160461187363
========> pt_263:  3.0492470785975456
========> pt_264:  4.091767445206642
========> pt_265:  2.7935703843832016
========> pt_266:  3.6821022257208824
========> pt_267:  3.5428787767887115
========> pt_268:  4.138554595410824
========> pt_269:  2.72254828363657
========> pt_270:  5.468626394867897
========> pt_271:  3.622201196849346
========> pt_272:  3.081836849451065
========> pt_273:  2.959204912185669
========> pt_274:  3.784656412899494
========> pt_275:  3.9138060435652733
========> pt_276:  2.584693357348442
========> pt_277:  2.6490825042128563
========> pt_278:  4.499666467308998
========> pt_279:  3.2868630811572075
========> pt_280:  3.053274415433407
========> pt_281:  2.9220227152109146
========> pt_282:  2.8923259302973747
========> pt_283:  6.005949378013611
========> pt_284:  2.5759687647223473
========> pt_285:  2.9987215250730515
========> pt_286:  3.4311527758836746
========> pt_287:  4.585563167929649
========> pt_288:  2.560741864144802
========> pt_289:  3.817381262779236
========> pt_290:  4.03663981705904
========> pt_291:  2.7697231993079185
========> pt_292:  3.428954742848873
========> pt_293:  3.2514968886971474
========> pt_294:  2.833426259458065
========> pt_295:  3.4575429931282997
========> pt_296:  2.322244457900524
========> pt_297:  4.460653141140938
========> pt_298:  3.563866876065731
========> pt_299:  2.768279053270817
========> pt_300:  4.999672695994377
========> pt_301:  3.5554202646017075
========> pt_302:  5.077393725514412
========> pt_303:  3.1009726971387863
========> pt_304:  3.0901919305324554
========> pt_305:  3.8521096110343933
========> pt_306:  2.663339599967003
========> pt_307:  2.9986195638775826
========> pt_308:  3.8221171125769615
========> pt_309:  2.4868713691830635
========> pt_310:  4.204744882881641
========> pt_311:  2.6148905605077744
========> pt_312:  2.6710443198680878
========> pt_313:  2.817869745194912
========> pt_314:  3.790959231555462
========> pt_315:  3.290932923555374
========> pt_316:  4.396868720650673
========> pt_317:  4.256618097424507
========> pt_318:  5.601516515016556
========> pt_319:  2.512420602142811
========> pt_320:  2.571292370557785
========> pt_321:  4.1724443063139915
========> pt_322:  3.3817456662654877
========> pt_323:  8.466762229800224
========> pt_324:  3.270888291299343
========> pt_325:  3.862428292632103
========> pt_326:  2.9662347584962845
========> pt_327:  2.818441614508629
========> pt_328:  3.285086713731289
========> pt_329:  5.616079494357109
========> pt_330:  6.608020067214966
========> pt_331:  3.5068541392683983
========> pt_332:  2.645375393331051
========> pt_333:  2.6546533405780792
========> pt_334:  2.2795456647872925
========> pt_335:  4.931445270776749
========> pt_336:  4.007465094327927
========> pt_337:  3.298284038901329
========> pt_338:  2.6384631544351578
========> pt_339:  2.9387592151761055
========> pt_340:  3.311007283627987
===============================================> mean Dose score: 3.538946096971631
        ==> Saving latest model successfully !
            Average train loss is             0.047168629184,     best is           0.047006361835
            Average val evaluation index is   -3.538946096972,     best is           -3.481912206486
    Train use time   1529.51257
    Train loader use time     72.16930
    Val use time     44.82730
    Total use time   1576.02747
    End lr is 0.000220735941, 0.000220735941
    time: 16:11:34
Epoch: 55, iter: 27499
    Begin lr is 0.000220735941, 0.000220735941
========> pt_241:  3.02699763327837
========> pt_242:  2.7194753661751747
========> pt_243:  4.0328555181622505
========> pt_244:  3.1153878197073936
========> pt_245:  3.8083601742982864
========> pt_246:  3.8389258459210396
========> pt_247:  3.149697631597519
========> pt_248:  2.580708786845207
========> pt_249:  3.9228519052267075
========> pt_250:  2.940121218562126
========> pt_251:  3.5998915135860443
========> pt_252:  3.488124832510948
========> pt_253:  4.54503059387207
========> pt_254:  3.665635883808136
========> pt_255:  3.5382235050201416
========> pt_256:  3.0859173834323883
========> pt_257:  3.1665778160095215
========> pt_258:  2.743774466216564
========> pt_259:  2.948300801217556
========> pt_260:  6.079574748873711
========> pt_261:  3.7887682393193245
========> pt_262:  3.325349912047386
========> pt_263:  3.281017132103443
========> pt_264:  3.6783192306756973
========> pt_265:  2.734963297843933
========> pt_266:  3.404802456498146
========> pt_267:  3.4357866644859314
========> pt_268:  3.7224658206105232
========> pt_269:  2.7741432562470436
========> pt_270:  5.312444791197777
========> pt_271:  3.777613267302513
========> pt_272:  3.466617539525032
========> pt_273:  2.9422887414693832
========> pt_274:  3.8841528072953224
========> pt_275:  3.668687157332897
========> pt_276:  2.640228308737278
========> pt_277:  2.550720199942589
========> pt_278:  4.113188944756985
========> pt_279:  3.1650864705443382
========> pt_280:  3.40888898819685
========> pt_281:  2.9211407899856567
========> pt_282:  2.9104583337903023
========> pt_283:  6.173633560538292
========> pt_284:  2.714657112956047
========> pt_285:  2.8276697546243668
========> pt_286:  3.201810233294964
========> pt_287:  5.321253091096878
========> pt_288:  2.4876724556088448
========> pt_289:  3.6419621109962463
========> pt_290:  3.8978977501392365
========> pt_291:  2.7337634935975075
========> pt_292:  3.1753649935126305
========> pt_293:  3.1885140761733055
========> pt_294:  2.825687639415264
========> pt_295:  3.1461521983146667
========> pt_296:  2.4093545228242874
========> pt_297:  4.185289591550827
========> pt_298:  3.4105899930000305
========> pt_299:  2.677859030663967
========> pt_300:  4.8205626010894775
========> pt_301:  3.8602683320641518
========> pt_302:  4.544664993882179
========> pt_303:  2.879254035651684
========> pt_304:  2.8201892971992493
========> pt_305:  3.7900976464152336
========> pt_306:  2.6119785383343697
========> pt_307:  3.001251518726349
========> pt_308:  3.3612965792417526
========> pt_309:  2.5316790118813515
========> pt_310:  4.563752859830856
========> pt_311:  2.605961002409458
========> pt_312:  2.6192378625273705
========> pt_313:  2.7536261081695557
========> pt_314:  3.550637476146221
========> pt_315:  3.2079458981752396
========> pt_316:  3.977248854935169
========> pt_317:  4.498259872198105
========> pt_318:  5.021291077136993
========> pt_319:  2.638039141893387
========> pt_320:  2.403368540108204
========> pt_321:  3.60957782715559
========> pt_322:  3.017963506281376
========> pt_323:  8.086754158139229
========> pt_324:  3.161472976207733
========> pt_325:  3.6458468064665794
========> pt_326:  3.121516965329647
========> pt_327:  3.502163663506508
========> pt_328:  3.1483444944024086
========> pt_329:  5.535614639520645
========> pt_330:  7.155299261212349
========> pt_331:  4.120591953396797
========> pt_332:  2.728177011013031
========> pt_333:  2.4806327000260353
========> pt_334:  2.295575737953186
========> pt_335:  4.290620982646942
========> pt_336:  4.099375680088997
========> pt_337:  3.173642084002495
========> pt_338:  2.674175128340721
========> pt_339:  2.8834662586450577
========> pt_340:  3.3995740115642548
===============================================> mean Dose score: 3.511136944964528
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.046952564918,     best is           0.046952564918
            Average val evaluation index is   -3.511136944965,     best is           -3.481912206486
    Train use time   1527.69802
    Train loader use time     71.33172
    Val use time     44.38103
    Total use time   1575.80316
    End lr is 0.000218125875, 0.000218125875
    time: 16:37:49
Epoch: 56, iter: 27999
    Begin lr is 0.000218125875, 0.000218125875
========> pt_241:  3.3262571319937706
========> pt_242:  2.7333885058760643
========> pt_243:  4.802395775914192
========> pt_244:  3.2136982306838036
========> pt_245:  3.6329543218016624
========> pt_246:  3.9324328675866127
========> pt_247:  2.8966669738292694
========> pt_248:  2.648720294237137
========> pt_249:  3.961581513285637
========> pt_250:  2.6621580496430397
========> pt_251:  3.9913607016205788
========> pt_252:  3.6918023601174355
========> pt_253:  4.114920720458031
========> pt_254:  4.241225607693195
========> pt_255:  3.3949273452162743
========> pt_256:  2.617178298532963
========> pt_257:  3.8602477312088013
========> pt_258:  2.656586952507496
========> pt_259:  3.3184919133782387
========> pt_260:  4.383634105324745
========> pt_261:  4.576951488852501
========> pt_262:  3.6878759413957596
========> pt_263:  3.353991359472275
========> pt_264:  4.454832226037979
========> pt_265:  3.076006807386875
========> pt_266:  3.6152341961860657
========> pt_267:  3.5483359172940254
========> pt_268:  4.218607693910599
========> pt_269:  3.0144376307725906
========> pt_270:  5.888066068291664
========> pt_271:  4.276469759643078
========> pt_272:  3.8442223519086838
========> pt_273:  2.9808616265654564
========> pt_274:  4.134848266839981
========> pt_275:  3.7047962844371796
========> pt_276:  2.7602332457900047
========> pt_277:  2.7583444863557816
========> pt_278:  4.777854681015015
========> pt_279:  2.967465855181217
========> pt_280:  2.9947450384497643
========> pt_281:  3.1900622695684433
========> pt_282:  3.019244410097599
========> pt_283:  5.637326017022133
========> pt_284:  2.925744690001011
========> pt_285:  3.098958507180214
========> pt_286:  3.35050355643034
========> pt_287:  4.465722516179085
========> pt_288:  2.752130851149559
========> pt_289:  4.2774442583322525
========> pt_290:  4.451644569635391
========> pt_291:  3.022484742105007
========> pt_292:  3.21948055177927
========> pt_293:  3.514716625213623
========> pt_294:  3.1563305854797363
========> pt_295:  3.375113755464554
========> pt_296:  2.4977603554725647
========> pt_297:  4.785286635160446
========> pt_298:  3.5451500862836838
========> pt_299:  2.917056865990162
========> pt_300:  5.139746516942978
========> pt_301:  3.7381673231720924
========> pt_302:  5.994793623685837
========> pt_303:  3.461715318262577
========> pt_304:  3.078693002462387
========> pt_305:  3.496548756957054
========> pt_306:  2.681235745549202
========> pt_307:  3.2042114064097404
========> pt_308:  4.146237149834633
========> pt_309:  2.6896408945322037
========> pt_310:  3.8305629417300224
========> pt_311:  2.421916089951992
========> pt_312:  2.585809715092182
========> pt_313:  2.706674672663212
========> pt_314:  3.5257305204868317
========> pt_315:  3.0868032202124596
========> pt_316:  4.733132049441338
========> pt_317:  4.3728528171777725
========> pt_318:  5.742912441492081
========> pt_319:  2.6747819408774376
========> pt_320:  2.7318082377314568
========> pt_321:  4.57853801548481
========> pt_322:  3.427267298102379
========> pt_323:  7.576542943716049
========> pt_324:  3.203379288315773
========> pt_325:  3.786008507013321
========> pt_326:  2.9101399332284927
========> pt_327:  2.919146940112114
========> pt_328:  2.8735512495040894
========> pt_329:  7.108066454529762
========> pt_330:  7.230843901634216
========> pt_331:  3.8219689950346947
========> pt_332:  2.5856759399175644
========> pt_333:  2.70619485527277
========> pt_334:  2.2108470275998116
========> pt_335:  5.421047806739807
========> pt_336:  4.0647391229867935
========> pt_337:  3.054131828248501
========> pt_338:  2.7511876448988914
========> pt_339:  3.1788546219468117
========> pt_340:  3.528471738100052
===============================================> mean Dose score: 3.6689755067229273
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.046664187677,     best is           0.046664187677
            Average val evaluation index is   -3.668975506723,     best is           -3.481912206486
    Train use time   1527.44078
    Train loader use time     70.14818
    Val use time     44.35488
    Total use time   1575.50222
    End lr is 0.000215489566, 0.000215489566
    time: 17:04:05
Epoch: 57, iter: 28499
    Begin lr is 0.000215489566, 0.000215489566
========> pt_241:  2.98650573939085
========> pt_242:  2.7009596303105354
========> pt_243:  4.618810340762138
========> pt_244:  2.9012729600071907
========> pt_245:  3.471125215291977
========> pt_246:  3.9472443610429764
========> pt_247:  2.729419581592083
========> pt_248:  2.540266439318657
========> pt_249:  3.8307316601276398
========> pt_250:  2.551504336297512
========> pt_251:  3.629041202366352
========> pt_252:  3.463885448873043
========> pt_253:  3.916405141353607
========> pt_254:  3.916202001273632
========> pt_255:  3.0080078169703484
========> pt_256:  2.507697269320488
========> pt_257:  3.6068830266594887
========> pt_258:  2.462201453745365
========> pt_259:  2.9338937625288963
========> pt_260:  4.206031523644924
========> pt_261:  4.048778414726257
========> pt_262:  3.4049179777503014
========> pt_263:  3.094222918152809
========> pt_264:  3.7120024114847183
========> pt_265:  2.806774489581585
========> pt_266:  3.5678167641162872
========> pt_267:  3.4940271079540253
========> pt_268:  3.733110725879669
========> pt_269:  2.798619158565998
========> pt_270:  5.268488824367523
========> pt_271:  3.806164488196373
========> pt_272:  3.5201766341924667
========> pt_273:  2.7261046692728996
========> pt_274:  3.7871266901493073
========> pt_275:  3.388635739684105
========> pt_276:  2.58814986795187
========> pt_277:  2.522941380739212
========> pt_278:  4.492738842964172
========> pt_279:  2.7705946937203407
========> pt_280:  2.793792560696602
========> pt_281:  2.9801082611083984
========> pt_282:  2.913304902613163
========> pt_283:  5.620131343603134
========> pt_284:  2.7070796489715576
========> pt_285:  2.912992760539055
========> pt_286:  3.1823228672146797
========> pt_287:  4.177143387496471
========> pt_288:  2.4586445465683937
========> pt_289:  3.838336765766144
========> pt_290:  3.8628674298524857
========> pt_291:  2.770201973617077
========> pt_292:  2.921932488679886
========> pt_293:  3.0979443714022636
========> pt_294:  2.8611185029149055
========> pt_295:  3.078751154243946
========> pt_296:  2.3182249441742897
========> pt_297:  4.371854327619076
========> pt_298:  3.2898035272955894
========> pt_299:  2.7362095192074776
========> pt_300:  4.784154891967773
========> pt_301:  3.3979887887835503
========> pt_302:  5.1160237193107605
========> pt_303:  3.038497343659401
========> pt_304:  2.882447950541973
========> pt_305:  3.5393625497817993
========> pt_306:  2.579004392027855
========> pt_307:  3.098733201622963
========> pt_308:  3.617909699678421
========> pt_309:  2.5951990112662315
========> pt_310:  3.6915647983551025
========> pt_311:  2.378881685435772
========> pt_312:  2.489548698067665
========> pt_313:  2.552267089486122
========> pt_314:  3.6127224564552307
========> pt_315:  3.103230968117714
========> pt_316:  3.9543334022164345
========> pt_317:  4.465808570384979
========> pt_318:  4.958723932504654
========> pt_319:  2.4706175550818443
========> pt_320:  2.407499924302101
========> pt_321:  4.140746630728245
========> pt_322:  3.130602464079857
========> pt_323:  7.768051102757454
========> pt_324:  3.08072779327631
========> pt_325:  3.5391122102737427
========> pt_326:  2.871042899787426
========> pt_327:  2.827768065035343
========> pt_328:  2.6920298114418983
========> pt_329:  6.6323718428611755
========> pt_330:  6.949220821261406
========> pt_331:  3.460712395608425
========> pt_332:  2.5539537519216537
========> pt_333:  2.4633634462952614
========> pt_334:  2.088080793619156
========> pt_335:  4.804301485419273
========> pt_336:  3.7231341749429703
========> pt_337:  2.8443551436066628
========> pt_338:  2.5412411987781525
========> pt_339:  2.8867271915078163
========> pt_340:  3.233138658106327
===============================================> mean Dose score: 3.413174485042691
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.046807523854,     best is           0.046664187677
            Average val evaluation index is   -3.413174485043,     best is           -3.413174485043
    Train use time   1529.24313
    Train loader use time     72.05627
    Val use time     44.10844
    Total use time   1576.54005
    End lr is 0.000212828028, 0.000212828028
    time: 17:30:21
Epoch: 58, iter: 28999
    Begin lr is 0.000212828028, 0.000212828028
========> pt_241:  3.219662830233574
========> pt_242:  2.664405107498169
========> pt_243:  4.184138290584087
========> pt_244:  2.8906377032399178
========> pt_245:  3.4034571424126625
========> pt_246:  3.6553941294550896
========> pt_247:  2.978818491101265
========> pt_248:  2.4384254589676857
========> pt_249:  3.6394287273287773
========> pt_250:  2.7467939257621765
========> pt_251:  3.5128312557935715
========> pt_252:  3.354230225086212
========> pt_253:  4.148753583431244
========> pt_254:  3.5584337264299393
========> pt_255:  3.0153411999344826
========> pt_256:  2.5145860388875008
========> pt_257:  3.3604229986667633
========> pt_258:  2.6457031816244125
========> pt_259:  2.8077328205108643
========> pt_260:  4.362749271094799
========> pt_261:  3.9608023315668106
========> pt_262:  3.428618349134922
========> pt_263:  3.194182440638542
========> pt_264:  3.836764842271805
========> pt_265:  2.8467539697885513
========> pt_266:  3.407004401087761
========> pt_267:  3.386221006512642
========> pt_268:  3.8683918491005898
========> pt_269:  2.7414895966649055
========> pt_270:  5.398545414209366
========> pt_271:  4.045238196849823
========> pt_272:  3.5263415053486824
========> pt_273:  2.803276516497135
========> pt_274:  3.6534039303660393
========> pt_275:  3.5824256390333176
========> pt_276:  2.607550397515297
========> pt_277:  2.4613117054104805
========> pt_278:  4.423901736736298
========> pt_279:  2.5585052371025085
========> pt_280:  2.87001833319664
========> pt_281:  2.92754452675581
========> pt_282:  2.829311825335026
========> pt_283:  5.264312848448753
========> pt_284:  2.6193299144506454
========> pt_285:  2.84463781863451
========> pt_286:  3.2452840358018875
========> pt_287:  4.188474640250206
========> pt_288:  2.5268479809165
========> pt_289:  3.8432421162724495
========> pt_290:  4.028374440968037
========> pt_291:  2.8445087373256683
========> pt_292:  2.926066219806671
========> pt_293:  3.16124115139246
========> pt_294:  2.9428566992282867
========> pt_295:  3.28593160957098
========> pt_296:  2.305249273777008
========> pt_297:  4.074973054230213
========> pt_298:  3.4465812519192696
========> pt_299:  2.712229862809181
========> pt_300:  5.071535259485245
========> pt_301:  3.6032888293266296
========> pt_302:  4.887797012925148
========> pt_303:  3.076346330344677
========> pt_304:  2.865566201508045
========> pt_305:  3.492690399289131
========> pt_306:  2.5864971056580544
========> pt_307:  2.9508566111326218
========> pt_308:  3.5110152512788773
========> pt_309:  2.4178068712353706
========> pt_310:  4.106109030544758
========> pt_311:  2.5516099482774734
========> pt_312:  2.501738667488098
========> pt_313:  2.4742748588323593
========> pt_314:  3.4000832960009575
========> pt_315:  3.0001604557037354
========> pt_316:  4.225821644067764
========> pt_317:  4.575521424412727
========> pt_318:  5.729865059256554
========> pt_319:  2.435668334364891
========> pt_320:  2.4406980723142624
========> pt_321:  4.271678104996681
========> pt_322:  3.007042706012726
========> pt_323:  7.927455306053162
========> pt_324:  2.9862642660737038
========> pt_325:  3.4482303634285927
========> pt_326:  2.8341292962431908
========> pt_327:  2.820497006177902
========> pt_328:  2.7922266349196434
========> pt_329:  5.859156548976898
========> pt_330:  7.246997579932213
========> pt_331:  3.661603592336178
========> pt_332:  2.435367926955223
========> pt_333:  2.3886188492178917
========> pt_334:  2.151919063180685
========> pt_335:  4.985301122069359
========> pt_336:  4.064921401441097
========> pt_337:  2.950110547244549
========> pt_338:  2.658652514219284
========> pt_339:  3.1143786385655403
========> pt_340:  3.3120540156960487
===============================================> mean Dose score: 3.425412476621568
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.046165104017,     best is           0.046165104017
            Average val evaluation index is   -3.425412476622,     best is           -3.413174485043
    Train use time   1529.47928
    Train loader use time     72.20164
    Val use time     43.94383
    Total use time   1576.47837
    End lr is 0.000210142288, 0.000210142288
    time: 17:56:38
Epoch: 59, iter: 29499
    Begin lr is 0.000210142288, 0.000210142288
========> pt_241:  2.9526757448911667
========> pt_242:  2.6938731968402863
========> pt_243:  4.307951517403126
========> pt_244:  2.9958827793598175
========> pt_245:  3.4704991057515144
========> pt_246:  3.863704241812229
========> pt_247:  2.727840095758438
========> pt_248:  2.5487281754612923
========> pt_249:  3.8324975967407227
========> pt_250:  2.6711009070277214
========> pt_251:  3.5696306824684143
========> pt_252:  3.711596131324768
========> pt_253:  3.81140049546957
========> pt_254:  3.710992708802223
========> pt_255:  3.2646282389760017
========> pt_256:  2.4558939412236214
========> pt_257:  2.854955457150936
========> pt_258:  2.484011761844158
========> pt_259:  2.9235320538282394
========> pt_260:  4.223053567111492
========> pt_261:  4.110749959945679
========> pt_262:  3.2631587982177734
========> pt_263:  3.1107262894511223
========> pt_264:  3.717484325170517
========> pt_265:  2.6350517570972443
========> pt_266:  3.5427549108862877
========> pt_267:  3.49617350846529
========> pt_268:  3.952009677886963
========> pt_269:  2.7855616062879562
========> pt_270:  5.333846211433411
========> pt_271:  3.2048403844237328
========> pt_272:  3.4034360200166702
========> pt_273:  2.818884663283825
========> pt_274:  3.651023618876934
========> pt_275:  3.4830674529075623
========> pt_276:  2.583370991051197
========> pt_277:  2.5422555953264236
========> pt_278:  4.673322290182114
========> pt_279:  2.596866376698017
========> pt_280:  2.7117278799414635
========> pt_281:  2.7529992163181305
========> pt_282:  2.876468747854233
========> pt_283:  5.678698271512985
========> pt_284:  2.657203935086727
========> pt_285:  2.856297641992569
========> pt_286:  3.3764103055000305
========> pt_287:  4.384162947535515
========> pt_288:  2.4291668087244034
========> pt_289:  3.7658512219786644
========> pt_290:  3.9720305800437927
========> pt_291:  2.5239695981144905
========> pt_292:  3.162821941077709
========> pt_293:  3.1488636881113052
========> pt_294:  2.7947190776467323
========> pt_295:  3.2440824061632156
========> pt_296:  2.210073322057724
========> pt_297:  4.08942885696888
========> pt_298:  3.2674526423215866
========> pt_299:  2.7841127663850784
========> pt_300:  4.840842187404633
========> pt_301:  3.5577163472771645
========> pt_302:  5.027510449290276
========> pt_303:  2.9331453517079353
========> pt_304:  2.9486189410090446
========> pt_305:  3.435726948082447
========> pt_306:  2.6653803884983063
========> pt_307:  3.000793345272541
========> pt_308:  3.2636573910713196
========> pt_309:  2.4238627403974533
========> pt_310:  3.8324636965990067
========> pt_311:  2.4035369977355003
========> pt_312:  2.591521106660366
========> pt_313:  2.680864930152893
========> pt_314:  3.5376571118831635
========> pt_315:  3.107440061867237
========> pt_316:  4.318117126822472
========> pt_317:  4.657691195607185
========> pt_318:  5.170387551188469
========> pt_319:  2.5437219068408012
========> pt_320:  2.3412619158625603
========> pt_321:  3.476353920996189
========> pt_322:  3.3025189489126205
========> pt_323:  8.04835207760334
========> pt_324:  2.96741995960474
========> pt_325:  3.6835020408034325
========> pt_326:  2.9026081040501595
========> pt_327:  2.8827598318457603
========> pt_328:  3.0175019428133965
========> pt_329:  6.331204026937485
========> pt_330:  7.277882695198059
========> pt_331:  3.5356124117970467
========> pt_332:  2.5539182871580124
========> pt_333:  2.571970894932747
========> pt_334:  2.169123776257038
========> pt_335:  4.632246270775795
========> pt_336:  3.979218453168869
========> pt_337:  2.9805059358477592
========> pt_338:  2.6221178099513054
========> pt_339:  2.880975902080536
========> pt_340:  3.148770071566105
===============================================> mean Dose score: 3.4091205574572085
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.046044140473,     best is           0.046044140473
            Average val evaluation index is   -3.409120557457,     best is           -3.409120557457
    Train use time   1528.00935
    Train loader use time     71.14538
    Val use time     44.27341
    Total use time   1576.98799
    End lr is 0.000207433381, 0.000207433381
    time: 18:22:55
Epoch: 60, iter: 29999
    Begin lr is 0.000207433381, 0.000207433381
========> pt_241:  3.035675548017025
========> pt_242:  2.6509519666433334
========> pt_243:  4.288456588983536
========> pt_244:  2.7877163514494896
========> pt_245:  3.5620594769716263
========> pt_246:  3.7241313606500626
========> pt_247:  2.6214004307985306
========> pt_248:  2.4664702638983727
========> pt_249:  3.5676725581288338
========> pt_250:  2.5736703351140022
========> pt_251:  3.655482791364193
========> pt_252:  3.6392560973763466
========> pt_253:  3.846351280808449
========> pt_254:  3.5585688054561615
========> pt_255:  3.258412256836891
========> pt_256:  2.4276621639728546
========> pt_257:  3.2776597142219543
========> pt_258:  2.5622454658150673
========> pt_259:  2.9145198315382004
========> pt_260:  4.186169430613518
========> pt_261:  4.197581522166729
========> pt_262:  3.476588614284992
========> pt_263:  2.967614494264126
========> pt_264:  3.7479230016469955
========> pt_265:  2.7670662105083466
========> pt_266:  3.540942296385765
========> pt_267:  3.312857449054718
========> pt_268:  3.8936863094568253
========> pt_269:  2.717408500611782
========> pt_270:  5.203133523464203
========> pt_271:  3.737897425889969
========> pt_272:  3.495275154709816
========> pt_273:  2.7386753633618355
========> pt_274:  3.7702626734972
========> pt_275:  3.5590355843305588
========> pt_276:  2.5357311218976974
========> pt_277:  2.5267350673675537
========> pt_278:  4.278913959860802
========> pt_279:  2.5685665383934975
========> pt_280:  2.6607848331332207
========> pt_281:  2.9368149116635323
========> pt_282:  2.925337627530098
========> pt_283:  5.605078637599945
========> pt_284:  2.721153423190117
========> pt_285:  2.817060314118862
========> pt_286:  3.2909120619297028
========> pt_287:  4.03014350682497
========> pt_288:  2.524753212928772
========> pt_289:  3.6099309101700783
========> pt_290:  3.9058736711740494
========> pt_291:  2.677709087729454
========> pt_292:  3.1485507637262344
========> pt_293:  2.980974279344082
========> pt_294:  2.7887239679694176
========> pt_295:  3.3261356130242348
========> pt_296:  2.2041770443320274
========> pt_297:  4.1179680824279785
========> pt_298:  3.5435526072978973
========> pt_299:  2.6984896138310432
========> pt_300:  5.1798707246780396
========> pt_301:  3.716898113489151
========> pt_302:  4.843058213591576
========> pt_303:  3.2742569223046303
========> pt_304:  2.9467565193772316
========> pt_305:  3.38736891746521
========> pt_306:  2.5592927634716034
========> pt_307:  2.8572100773453712
========> pt_308:  3.4506304934620857
========> pt_309:  2.485252767801285
========> pt_310:  4.105067253112793
========> pt_311:  2.4301180988550186
========> pt_312:  2.5800464302301407
========> pt_313:  2.47897420078516
========> pt_314:  3.584042936563492
========> pt_315:  3.1758836656808853
========> pt_316:  4.135877266526222
========> pt_317:  4.485142081975937
========> pt_318:  5.510315224528313
========> pt_319:  2.4285808578133583
========> pt_320:  2.6962316036224365
========> pt_321:  3.5923077911138535
========> pt_322:  3.107425980269909
========> pt_323:  7.9501016438007355
========> pt_324:  3.007882386445999
========> pt_325:  3.6842209845781326
========> pt_326:  2.9289503395557404
========> pt_327:  2.8565002605319023
========> pt_328:  2.821383625268936
========> pt_329:  6.645315438508987
========> pt_330:  7.013876736164093
========> pt_331:  3.489222414791584
========> pt_332:  2.530614025890827
========> pt_333:  2.4953821301460266
========> pt_334:  2.1219917573034763
========> pt_335:  5.021748468279839
========> pt_336:  4.125930443406105
========> pt_337:  2.794332355260849
========> pt_338:  2.4783092364668846
========> pt_339:  2.8078486025333405
========> pt_340:  3.2196620479226112
===============================================> mean Dose score: 3.4115843350067734
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.045935954511,     best is           0.045935954511
            Average val evaluation index is   -3.411584335007,     best is           -3.409120557457
    Train use time   1530.15196
    Train loader use time     75.34197
    Val use time     44.36005
    Total use time   1577.55372
    End lr is 0.000204702351, 0.000204702351
    time: 18:49:12
Epoch: 61, iter: 30499
    Begin lr is 0.000204702351, 0.000204702351
========> pt_241:  3.0708933621644974
========> pt_242:  2.7040283754467964
========> pt_243:  4.299487955868244
========> pt_244:  3.0450523272156715
========> pt_245:  3.386307582259178
========> pt_246:  3.5518878698349
========> pt_247:  2.875455915927887
========> pt_248:  2.5131192058324814
========> pt_249:  3.7742524594068527
========> pt_250:  2.7146732807159424
========> pt_251:  3.139289766550064
========> pt_252:  3.4406954050064087
========> pt_253:  4.186295121908188
========> pt_254:  3.6278072372078896
========> pt_255:  3.323286436498165
========> pt_256:  2.390965260565281
========> pt_257:  3.101652003824711
========> pt_258:  2.9908042773604393
========> pt_259:  2.9708050191402435
========> pt_260:  4.377212375402451
========> pt_261:  3.8879790902137756
========> pt_262:  3.445887863636017
========> pt_263:  2.890673950314522
========> pt_264:  3.261127658188343
========> pt_265:  2.640424147248268
========> pt_266:  3.3106398582458496
========> pt_267:  3.2423435896635056
========> pt_268:  3.765338808298111
========> pt_269:  2.568204589188099
========> pt_270:  5.1008860021829605
========> pt_271:  3.5569100454449654
========> pt_272:  3.5487573221325874
========> pt_273:  2.953510992228985
========> pt_274:  3.606758378446102
========> pt_275:  3.3890806138515472
========> pt_276:  2.4802086874842644
========> pt_277:  2.5163770094513893
========> pt_278:  4.348933659493923
========> pt_279:  2.8452039510011673
========> pt_280:  2.8218942135572433
========> pt_281:  2.754845470190048
========> pt_282:  2.830495722591877
========> pt_283:  7.254408672451973
========> pt_284:  2.6323864236474037
========> pt_285:  2.6332318410277367
========> pt_286:  3.2899099215865135
========> pt_287:  4.163006506860256
========> pt_288:  2.3603886365890503
========> pt_289:  3.4261905774474144
========> pt_290:  4.005773477256298
========> pt_291:  2.9142676666378975
========> pt_292:  2.9677198454737663
========> pt_293:  2.8825652971863747
========> pt_294:  2.6373348012566566
========> pt_295:  3.4023814648389816
========> pt_296:  2.251691222190857
========> pt_297:  4.087703339755535
========> pt_298:  3.607163354754448
========> pt_299:  2.6157649233937263
========> pt_300:  5.1930709183216095
========> pt_301:  3.548227958381176
========> pt_302:  4.862866848707199
========> pt_303:  2.90350541472435
========> pt_304:  2.8477060422301292
========> pt_305:  3.5289403423666954
========> pt_306:  2.558801732957363
========> pt_307:  2.7453356981277466
========> pt_308:  3.42560488730669
========> pt_309:  2.5756850466132164
========> pt_310:  3.8366099447011948
========> pt_311:  2.671511098742485
========> pt_312:  2.3743513226509094
========> pt_313:  2.745877057313919
========> pt_314:  3.408536948263645
========> pt_315:  3.0225196853280067
========> pt_316:  3.970216140151024
========> pt_317:  4.768835678696632
========> pt_318:  5.364920645952225
========> pt_319:  2.4528486654162407
========> pt_320:  2.2971440106630325
========> pt_321:  3.25717281550169
========> pt_322:  3.15297394990921
========> pt_323:  7.542511895298958
========> pt_324:  3.096710406243801
========> pt_325:  3.4513569995760918
========> pt_326:  2.9793306440114975
========> pt_327:  2.64294371008873
========> pt_328:  2.703883908689022
========> pt_329:  6.04545921087265
========> pt_330:  7.776395231485367
========> pt_331:  3.3570671454072
========> pt_332:  2.597363665699959
========> pt_333:  2.3314791172742844
========> pt_334:  2.128882221877575
========> pt_335:  4.7750722616910934
========> pt_336:  4.44246232509613
========> pt_337:  3.0479685217142105
========> pt_338:  2.699619010090828
========> pt_339:  2.805970013141632
========> pt_340:  3.3822254836559296
===============================================> mean Dose score: 3.397023054584861
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.045526091807,     best is           0.045526091807
            Average val evaluation index is   -3.397023054585,     best is           -3.397023054585
    Train use time   1536.06531
    Train loader use time     80.98065
    Val use time     46.19188
    Total use time   1587.75326
    End lr is 0.000201950253, 0.000201950253
    time: 19:15:40
Epoch: 62, iter: 30999
    Begin lr is 0.000201950253, 0.000201950253
========> pt_241:  3.087909147143364
========> pt_242:  2.6748359203338623
========> pt_243:  4.11890085786581
========> pt_244:  2.8881147503852844
========> pt_245:  3.20400957018137
========> pt_246:  3.652234636247158
========> pt_247:  2.6829563081264496
========> pt_248:  2.5701452419161797
========> pt_249:  3.867143541574478
========> pt_250:  2.5030140951275826
========> pt_251:  3.226713538169861
========> pt_252:  3.358617164194584
========> pt_253:  3.5714850202202797
========> pt_254:  3.5282013192772865
========> pt_255:  3.438890613615513
========> pt_256:  2.3683353513479233
========> pt_257:  3.075893111526966
========> pt_258:  2.436756268143654
========> pt_259:  3.199058063328266
========> pt_260:  4.152610115706921
========> pt_261:  3.645169325172901
========> pt_262:  3.3995286375284195
========> pt_263:  3.0316724628210068
========> pt_264:  3.399418070912361
========> pt_265:  2.7005045861005783
========> pt_266:  3.4295931085944176
========> pt_267:  3.2149019464850426
========> pt_268:  3.729865700006485
========> pt_269:  2.7844512462615967
========> pt_270:  4.941911548376083
========> pt_271:  3.431536890566349
========> pt_272:  4.052882678806782
========> pt_273:  2.64922384172678
========> pt_274:  4.10729605704546
========> pt_275:  3.6007530987262726
========> pt_276:  2.5219953060150146
========> pt_277:  2.3823504522442818
========> pt_278:  4.428821951150894
========> pt_279:  2.4639467895030975
========> pt_280:  2.8014469519257545
========> pt_281:  2.8689369186758995
========> pt_282:  2.815796621143818
========> pt_283:  6.807705461978912
========> pt_284:  2.996097132563591
========> pt_285:  2.8511544689536095
========> pt_286:  3.04633479565382
========> pt_287:  4.167765565216541
========> pt_288:  2.3487822711467743
========> pt_289:  3.8100455328822136
========> pt_290:  3.5472944006323814
========> pt_291:  2.817375585436821
========> pt_292:  2.8206753730773926
========> pt_293:  2.961432673037052
========> pt_294:  2.948835641145706
========> pt_295:  3.2127177342772484
========> pt_296:  2.1929913014173508
========> pt_297:  3.8629600033164024
========> pt_298:  3.2866234332323074
========> pt_299:  2.597098723053932
========> pt_300:  4.590154811739922
========> pt_301:  3.46520833671093
========> pt_302:  4.852118939161301
========> pt_303:  3.181583844125271
========> pt_304:  2.6470018178224564
========> pt_305:  3.186207562685013
========> pt_306:  2.3917921632528305
========> pt_307:  2.8495387360453606
========> pt_308:  3.213851824402809
========> pt_309:  2.600381560623646
========> pt_310:  3.633684478700161
========> pt_311:  2.4112265929579735
========> pt_312:  2.3428716510534286
========> pt_313:  2.691674642264843
========> pt_314:  3.26387669891119
========> pt_315:  2.9551932215690613
========> pt_316:  3.9573022723197937
========> pt_317:  4.729448929429054
========> pt_318:  4.46399413049221
========> pt_319:  2.496427036821842
========> pt_320:  2.283502072095871
========> pt_321:  4.102819673717022
========> pt_322:  2.9725125432014465
========> pt_323:  7.2083936631679535
========> pt_324:  2.7813751995563507
========> pt_325:  3.396489880979061
========> pt_326:  2.8142421692609787
========> pt_327:  2.726174294948578
========> pt_328:  2.610134109854698
========> pt_329:  6.918800920248032
========> pt_330:  7.770015746355057
========> pt_331:  3.3999578654766083
========> pt_332:  2.5855523347854614
========> pt_333:  2.320481650531292
========> pt_334:  2.1400907821953297
========> pt_335:  4.662870615720749
========> pt_336:  3.9268646389245987
========> pt_337:  2.6744017377495766
========> pt_338:  2.4301089718937874
========> pt_339:  2.8381096944212914
========> pt_340:  3.1910495460033417
===============================================> mean Dose score: 3.3396120628342034
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.045744066939,     best is           0.045526091807
            Average val evaluation index is   -3.339612062834,     best is           -3.339612062834
    Train use time   1531.85785
    Train loader use time     76.14196
    Val use time     44.89650
    Total use time   1580.25600
    End lr is 0.000199178145, 0.000199178145
    time: 19:42:00
Epoch: 63, iter: 31499
    Begin lr is 0.000199178145, 0.000199178145
========> pt_241:  2.9593446850776672
========> pt_242:  2.758021131157875
========> pt_243:  4.497685134410858
========> pt_244:  2.8521234914660454
========> pt_245:  3.5743605345487595
========> pt_246:  3.750271238386631
========> pt_247:  2.742065116763115
========> pt_248:  2.444831281900406
========> pt_249:  3.8959818705916405
========> pt_250:  2.5610391423106194
========> pt_251:  3.5482438653707504
========> pt_252:  3.520353175699711
========> pt_253:  4.048491045832634
========> pt_254:  3.7833770737051964
========> pt_255:  3.4030988439917564
========> pt_256:  2.378893420100212
========> pt_257:  3.2571524754166603
========> pt_258:  2.5836633145809174
========> pt_259:  3.0870989337563515
========> pt_260:  4.780282974243164
========> pt_261:  4.41541314125061
========> pt_262:  3.350163772702217
========> pt_263:  2.986699752509594
========> pt_264:  3.977626971900463
========> pt_265:  2.64683336019516
========> pt_266:  3.529806099832058
========> pt_267:  3.2381436228752136
========> pt_268:  4.001507796347141
========> pt_269:  2.698757164180279
========> pt_270:  5.540485307574272
========> pt_271:  3.6783958971500397
========> pt_272:  3.4677059948444366
========> pt_273:  2.841387316584587
========> pt_274:  3.9951762929558754
========> pt_275:  3.37493434548378
========> pt_276:  2.4927055835723877
========> pt_277:  2.394315116107464
========> pt_278:  4.5331066101789474
========> pt_279:  2.7999702095985413
========> pt_280:  2.7636610716581345
========> pt_281:  2.779410034418106
========> pt_282:  2.862415835261345
========> pt_283:  8.355464935302734
========> pt_284:  2.7626698836684227
========> pt_285:  2.734634205698967
========> pt_286:  3.2210806384682655
========> pt_287:  4.610631540417671
========> pt_288:  2.443830706179142
========> pt_289:  3.8041526451706886
========> pt_290:  4.1340430080890656
========> pt_291:  2.915710248053074
========> pt_292:  3.1160569563508034
========> pt_293:  3.1947068497538567
========> pt_294:  2.86163117736578
========> pt_295:  3.304835371673107
========> pt_296:  2.175315897911787
========> pt_297:  4.252142496407032
========> pt_298:  3.4440695121884346
========> pt_299:  2.579215355217457
========> pt_300:  4.964499473571777
========> pt_301:  3.5723815485835075
========> pt_302:  5.12537494301796
========> pt_303:  3.0789824575185776
========> pt_304:  2.8478479012846947
========> pt_305:  3.400619700551033
========> pt_306:  2.5586403161287308
========> pt_307:  2.870218865573406
========> pt_308:  3.563106469810009
========> pt_309:  2.450844384729862
========> pt_310:  4.0034956485033035
========> pt_311:  2.4169254675507545
========> pt_312:  2.4480880424380302
========> pt_313:  2.5333255156874657
========> pt_314:  3.4243325889110565
========> pt_315:  2.981395684182644
========> pt_316:  4.543703272938728
========> pt_317:  4.488926380872726
========> pt_318:  5.706510469317436
========> pt_319:  2.4111538380384445
========> pt_320:  2.3750267177820206
========> pt_321:  4.125978425145149
========> pt_322:  3.0114100873470306
========> pt_323:  7.696246951818466
========> pt_324:  3.0600208044052124
========> pt_325:  3.599778860807419
========> pt_326:  2.8950199484825134
========> pt_327:  2.7203309535980225
========> pt_328:  2.598455771803856
========> pt_329:  6.667833998799324
========> pt_330:  7.281345725059509
========> pt_331:  3.445119895040989
========> pt_332:  2.4913962557911873
========> pt_333:  2.3566849157214165
========> pt_334:  2.066758908331394
========> pt_335:  4.971671178936958
========> pt_336:  3.9859481528401375
========> pt_337:  2.9034968093037605
========> pt_338:  2.5301409885287285
========> pt_339:  2.84964382648468
========> pt_340:  3.2485178485512733
===============================================> mean Dose score: 3.469763914681971
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.045198526427,     best is           0.045198526427
            Average val evaluation index is   -3.469763914682,     best is           -3.339612062834
    Train use time   1528.87289
    Train loader use time     72.98982
    Val use time     44.68219
    Total use time   1576.77778
    End lr is 0.000196387098, 0.000196387098
    time: 20:08:17
Epoch: 64, iter: 31999
    Begin lr is 0.000196387098, 0.000196387098
========> pt_241:  3.2081490382552147
========> pt_242:  2.6448410749435425
========> pt_243:  4.930293187499046
========> pt_244:  2.99860130995512
========> pt_245:  3.7529874220490456
========> pt_246:  3.956158794462681
========> pt_247:  2.86958284676075
========> pt_248:  2.666645124554634
========> pt_249:  4.361388050019741
========> pt_250:  2.7334875985980034
========> pt_251:  3.8748547807335854
========> pt_252:  3.6819269880652428
========> pt_253:  4.256456159055233
========> pt_254:  4.104792922735214
========> pt_255:  3.667847476899624
========> pt_256:  2.460106946527958
========> pt_257:  3.6251231282949448
========> pt_258:  2.778223790228367
========> pt_259:  3.3562545850872993
========> pt_260:  4.058728367090225
========> pt_261:  4.700724557042122
========> pt_262:  3.4212875738739967
========> pt_263:  3.197024054825306
========> pt_264:  4.278375469148159
========> pt_265:  2.8801213577389717
========> pt_266:  3.5085517540574074
========> pt_267:  3.303545080125332
========> pt_268:  4.708940386772156
========> pt_269:  2.9112980142235756
========> pt_270:  6.195027157664299
========> pt_271:  4.266696348786354
========> pt_272:  4.1016582027077675
========> pt_273:  2.9087429866194725
========> pt_274:  4.243392087519169
========> pt_275:  3.7278376892209053
========> pt_276:  2.6743751391768456
========> pt_277:  2.5999677181243896
========> pt_278:  5.315412878990173
========> pt_279:  2.762319929897785
========> pt_280:  3.016108386218548
========> pt_281:  2.905443459749222
========> pt_282:  3.0483007431030273
========> pt_283:  5.191822350025177
========> pt_284:  3.162280321121216
========> pt_285:  2.921261265873909
========> pt_286:  3.2643163576722145
========> pt_287:  5.315605327486992
========> pt_288:  2.5304346159100533
========> pt_289:  4.2069340497255325
========> pt_290:  4.460377246141434
========> pt_291:  3.0220093578100204
========> pt_292:  3.2829327508807182
========> pt_293:  3.5441727191209793
========> pt_294:  3.085830807685852
========> pt_295:  3.6831919848918915
========> pt_296:  2.433510459959507
========> pt_297:  4.730278700590134
========> pt_298:  3.5031071305274963
========> pt_299:  2.6927070319652557
========> pt_300:  5.171317458152771
========> pt_301:  3.8999708741903305
========> pt_302:  5.560221448540688
========> pt_303:  3.3500464260578156
========> pt_304:  3.0177992209792137
========> pt_305:  3.6877455562353134
========> pt_306:  2.6050250977277756
========> pt_307:  2.963574379682541
========> pt_308:  4.01656623929739
========> pt_309:  2.537926807999611
========> pt_310:  4.095850326120853
========> pt_311:  2.5754185393452644
========> pt_312:  2.545001246035099
========> pt_313:  2.6505449041724205
========> pt_314:  3.4102653339505196
========> pt_315:  2.958613745868206
========> pt_316:  5.084559693932533
========> pt_317:  4.888252317905426
========> pt_318:  5.505482628941536
========> pt_319:  2.576935961842537
========> pt_320:  2.6489515975117683
========> pt_321:  4.188335910439491
========> pt_322:  3.390098139643669
========> pt_323:  7.160855755209923
========> pt_324:  3.011278919875622
========> pt_325:  3.7993716821074486
========> pt_326:  2.973223142325878
========> pt_327:  2.8731613978743553
========> pt_328:  2.652350999414921
========> pt_329:  8.333929479122162
========> pt_330:  7.650776952505112
========> pt_331:  3.479813300073147
========> pt_332:  2.5432373955845833
========> pt_333:  2.4759896844625473
========> pt_334:  2.189275063574314
========> pt_335:  5.497163534164429
========> pt_336:  4.292778857052326
========> pt_337:  3.236687481403351
========> pt_338:  2.7458181232213974
========> pt_339:  3.2432682812213898
========> pt_340:  3.513481356203556
===============================================> mean Dose score: 3.6669333830475805
        ==> Saving latest model successfully !
            Average train loss is             0.045686320759,     best is           0.045198526427
            Average val evaluation index is   -3.666933383048,     best is           -3.339612062834
    Train use time   1527.12350
    Train loader use time     71.56378
    Val use time     45.06843
    Total use time   1573.64851
    End lr is 0.000193578187, 0.000193578187
    time: 20:34:31
Epoch: 65, iter: 32499
    Begin lr is 0.000193578187, 0.000193578187
========> pt_241:  3.0849992111325264
========> pt_242:  2.660851329565048
========> pt_243:  4.808614104986191
========> pt_244:  2.9809628054499626
========> pt_245:  3.6028429120779037
========> pt_246:  3.897707127034664
========> pt_247:  2.578365243971348
========> pt_248:  2.691929154098034
========> pt_249:  4.249453954398632
========> pt_250:  2.6046639308333397
========> pt_251:  4.040030092000961
========> pt_252:  3.9025764912366867
========> pt_253:  4.033826105296612
========> pt_254:  4.249305836856365
========> pt_255:  3.178366981446743
========> pt_256:  2.460082955658436
========> pt_257:  3.624178357422352
========> pt_258:  2.490515373647213
========> pt_259:  2.966729961335659
========> pt_260:  4.011090323328972
========> pt_261:  4.590684697031975
========> pt_262:  3.8525578752160072
========> pt_263:  3.1113435328006744
========> pt_264:  4.1302720084786415
========> pt_265:  3.0069803819060326
========> pt_266:  3.8534048572182655
========> pt_267:  3.6304861307144165
========> pt_268:  4.144938252866268
========> pt_269:  2.813165970146656
========> pt_270:  5.701209008693695
========> pt_271:  3.7004202976822853
========> pt_272:  3.5344673693180084
========> pt_273:  2.861765995621681
========> pt_274:  4.137822091579437
========> pt_275:  3.849574401974678
========> pt_276:  2.600819915533066
========> pt_277:  2.798311449587345
========> pt_278:  4.932254180312157
========> pt_279:  3.102998360991478
========> pt_280:  2.9226861149072647
========> pt_281:  3.334885500371456
========> pt_282:  3.154672347009182
========> pt_283:  5.524830222129822
========> pt_284:  2.684887573122978
========> pt_285:  3.0854028835892677
========> pt_286:  3.645576387643814
========> pt_287:  4.766286909580231
========> pt_288:  2.6940593868494034
========> pt_289:  4.016176387667656
========> pt_290:  4.527448415756226
========> pt_291:  2.6940499991178513
========> pt_292:  3.4409186244010925
========> pt_293:  3.258773162961006
========> pt_294:  2.7873048558831215
========> pt_295:  3.3585723116993904
========> pt_296:  2.4379678070545197
========> pt_297:  4.776277542114258
========> pt_298:  3.4505950286984444
========> pt_299:  2.9286329820752144
========> pt_300:  4.881650656461716
========> pt_301:  3.638719953596592
========> pt_302:  5.42798325419426
========> pt_303:  3.6645985394716263
========> pt_304:  3.0648643523454666
========> pt_305:  3.8685785606503487
========> pt_306:  2.7325383946299553
========> pt_307:  3.2238998264074326
========> pt_308:  4.141000360250473
========> pt_309:  2.5390856713056564
========> pt_310:  3.733697198331356
========> pt_311:  2.2878673672676086
========> pt_312:  2.7003436908125877
========> pt_313:  2.8815605491399765
========> pt_314:  3.80618117749691
========> pt_315:  3.331913501024246
========> pt_316:  4.611442536115646
========> pt_317:  4.303815178573132
========> pt_318:  5.482977107167244
========> pt_319:  2.6164572685956955
========> pt_320:  2.820860780775547
========> pt_321:  4.600256532430649
========> pt_322:  3.253641724586487
========> pt_323:  7.960500121116638
========> pt_324:  3.1899209320545197
========> pt_325:  3.769064173102379
========> pt_326:  3.0218294262886047
========> pt_327:  3.005169853568077
========> pt_328:  3.1301184743642807
========> pt_329:  8.231563046574593
========> pt_330:  6.533078849315643
========> pt_331:  3.6800066754221916
========> pt_332:  2.5989121198654175
========> pt_333:  2.6658153533935547
========> pt_334:  2.232937663793564
========> pt_335:  5.566483065485954
========> pt_336:  3.913070410490036
========> pt_337:  2.970656640827656
========> pt_338:  2.5849778577685356
========> pt_339:  2.8594761714339256
========> pt_340:  3.394218049943447
===============================================> mean Dose score: 3.6288230653852223
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.045131537519,     best is           0.045131537519
            Average val evaluation index is   -3.628823065385,     best is           -3.339612062834
    Train use time   1527.56332
    Train loader use time     72.12939
    Val use time     45.08714
    Total use time   1575.76862
    End lr is 0.000190752495, 0.000190752495
    time: 21:00:47
Epoch: 66, iter: 32999
    Begin lr is 0.000190752495, 0.000190752495
========> pt_241:  3.281714953482151
========> pt_242:  2.4908285588026047
========> pt_243:  4.382502883672714
========> pt_244:  3.0629098787903786
========> pt_245:  3.3944133669137955
========> pt_246:  3.8077830895781517
========> pt_247:  2.7407461404800415
========> pt_248:  2.494157813489437
========> pt_249:  3.8369009643793106
========> pt_250:  2.77987577021122
========> pt_251:  3.4602276235818863
========> pt_252:  3.3519359678030014
========> pt_253:  4.013148844242096
========> pt_254:  3.621298670768738
========> pt_255:  3.1473739072680473
========> pt_256:  2.453201226890087
========> pt_257:  3.071478009223938
========> pt_258:  2.741609551012516
========> pt_259:  2.643638923764229
========> pt_260:  4.418632090091705
========> pt_261:  4.010999575257301
========> pt_262:  3.277764543890953
========> pt_263:  3.131881020963192
========> pt_264:  3.698810562491417
========> pt_265:  2.645321413874626
========> pt_266:  3.368842750787735
========> pt_267:  3.4793926775455475
========> pt_268:  3.9417799189686775
========> pt_269:  2.730965167284012
========> pt_270:  5.059509053826332
========> pt_271:  3.8622765243053436
========> pt_272:  3.280556872487068
========> pt_273:  2.744620405137539
========> pt_274:  3.767254687845707
========> pt_275:  3.9772655442357063
========> pt_276:  2.4447204545140266
========> pt_277:  2.460097298026085
========> pt_278:  4.854102358222008
========> pt_279:  3.0301842465996742
========> pt_280:  2.956574521958828
========> pt_281:  2.676926515996456
========> pt_282:  2.7800165861845016
========> pt_283:  6.310720518231392
========> pt_284:  2.577562592923641
========> pt_285:  2.958889380097389
========> pt_286:  3.259577639400959
========> pt_287:  4.467001333832741
========> pt_288:  2.3845602199435234
========> pt_289:  4.054714068770409
========> pt_290:  3.882160000503063
========> pt_291:  2.623041719198227
========> pt_292:  3.2731249183416367
========> pt_293:  3.0670391768217087
========> pt_294:  2.7846989780664444
========> pt_295:  3.344382755458355
========> pt_296:  2.267655059695244
========> pt_297:  4.366648569703102
========> pt_298:  3.261502906680107
========> pt_299:  2.657659240067005
========> pt_300:  4.648190289735794
========> pt_301:  3.6722657084465027
========> pt_302:  5.162426233291626
========> pt_303:  2.952781096100807
========> pt_304:  2.9152359068393707
========> pt_305:  3.8634729385375977
========> pt_306:  2.537863701581955
========> pt_307:  2.8240307047963142
========> pt_308:  3.5605916008353233
========> pt_309:  2.3227328807115555
========> pt_310:  3.8620710372924805
========> pt_311:  2.5206074863672256
========> pt_312:  2.49026607722044
========> pt_313:  2.555377297103405
========> pt_314:  3.5018038004636765
========> pt_315:  3.106273375451565
========> pt_316:  4.366319216787815
========> pt_317:  4.622032418847084
========> pt_318:  5.234087482094765
========> pt_319:  2.468345984816551
========> pt_320:  2.3167474195361137
========> pt_321:  4.13954135030508
========> pt_322:  3.0350418761372566
========> pt_323:  8.003871440887451
========> pt_324:  2.909725569188595
========> pt_325:  3.6851347237825394
========> pt_326:  2.955891825258732
========> pt_327:  2.756916768848896
========> pt_328:  2.898898907005787
========> pt_329:  6.015655770897865
========> pt_330:  7.089107409119606
========> pt_331:  3.8057569041848183
========> pt_332:  2.508854568004608
========> pt_333:  2.530984841287136
========> pt_334:  2.128357421606779
========> pt_335:  4.374500103294849
========> pt_336:  4.077642820775509
========> pt_337:  3.0483171716332436
========> pt_338:  2.5738153234124184
========> pt_339:  2.8728028386831284
========> pt_340:  3.199908435344696
===============================================> mean Dose score: 3.430294287391007
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.045056266665,     best is           0.045056266665
            Average val evaluation index is   -3.430294287391,     best is           -3.339612062834
    Train use time   1529.18917
    Train loader use time     72.41968
    Val use time     45.01936
    Total use time   1577.46116
    End lr is 0.000187911112, 0.000187911112
    time: 21:27:04
Epoch: 67, iter: 33499
    Begin lr is 0.000187911112, 0.000187911112
========> pt_241:  2.8862977027893066
========> pt_242:  2.6588981598615646
========> pt_243:  4.546957686543465
========> pt_244:  3.001556359231472
========> pt_245:  3.5995595529675484
========> pt_246:  3.835378587245941
========> pt_247:  2.7574675157666206
========> pt_248:  2.426285035908222
========> pt_249:  4.161436147987843
========> pt_250:  2.6435059309005737
========> pt_251:  3.6299507692456245
========> pt_252:  3.4494956210255623
========> pt_253:  4.37729112803936
========> pt_254:  4.1532182320952415
========> pt_255:  2.9279643669724464
========> pt_256:  2.3601987957954407
========> pt_257:  3.2770341262221336
========> pt_258:  2.717820778489113
========> pt_259:  2.6942768692970276
========> pt_260:  4.218665584921837
========> pt_261:  4.5223268866539
========> pt_262:  3.439100533723831
========> pt_263:  2.884129136800766
========> pt_264:  4.001383408904076
========> pt_265:  2.6961836218833923
========> pt_266:  3.5640165582299232
========> pt_267:  3.380138799548149
========> pt_268:  4.196196831762791
========> pt_269:  2.6443982869386673
========> pt_270:  5.637618079781532
========> pt_271:  3.746090568602085
========> pt_272:  3.325338438153267
========> pt_273:  2.843930870294571
========> pt_274:  3.8529302552342415
========> pt_275:  3.7382155656814575
========> pt_276:  2.511659152805805
========> pt_277:  2.4548615515232086
========> pt_278:  4.942552000284195
========> pt_279:  2.9684333130717278
========> pt_280:  2.9947541654109955
========> pt_281:  2.9036441445350647
========> pt_282:  2.9475033655762672
========> pt_283:  5.631152540445328
========> pt_284:  2.4697843939065933
========> pt_285:  2.716955803334713
========> pt_286:  3.26628465205431
========> pt_287:  4.732531234622002
========> pt_288:  2.3980681225657463
========> pt_289:  4.177153818309307
========> pt_290:  4.19280681759119
========> pt_291:  2.921619303524494
========> pt_292:  3.0585790053009987
========> pt_293:  3.1880707666277885
========> pt_294:  2.789839282631874
========> pt_295:  3.4554709121584892
========> pt_296:  2.285960093140602
========> pt_297:  4.669509828090668
========> pt_298:  3.4432590380311012
========> pt_299:  2.6192446425557137
========> pt_300:  4.938576817512512
========> pt_301:  3.601306453347206
========> pt_302:  5.1544878631830215
========> pt_303:  3.184417113661766
========> pt_304:  2.8798801451921463
========> pt_305:  3.5876712948083878
========> pt_306:  2.5037940591573715
========> pt_307:  2.857787422835827
========> pt_308:  3.742796517908573
========> pt_309:  2.410227321088314
========> pt_310:  3.844825252890587
========> pt_311:  2.4650023877620697
========> pt_312:  2.4597447365522385
========> pt_313:  2.6495086029171944
========> pt_314:  3.4258750453591347
========> pt_315:  2.9862919077277184
========> pt_316:  4.7028060257434845
========> pt_317:  4.485270902514458
========> pt_318:  5.4572077840566635
========> pt_319:  2.382100634276867
========> pt_320:  2.3824748396873474
========> pt_321:  4.317013807594776
========> pt_322:  3.101300224661827
========> pt_323:  8.157683163881302
========> pt_324:  3.0793331936001778
========> pt_325:  3.608263023197651
========> pt_326:  2.817356027662754
========> pt_327:  2.7130387723445892
========> pt_328:  2.6676691696047783
========> pt_329:  6.8798136711120605
========> pt_330:  6.722867488861084
========> pt_331:  3.591952621936798
========> pt_332:  2.4490200355648994
========> pt_333:  2.3886535316705704
========> pt_334:  2.0384242571890354
========> pt_335:  5.31871423125267
========> pt_336:  4.104957208037376
========> pt_337:  2.9995327815413475
========> pt_338:  2.628067024052143
========> pt_339:  2.950567677617073
========> pt_340:  3.4993019700050354
===============================================> mean Dose score: 3.4867053775116803
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.044826688044,     best is           0.044826688044
            Average val evaluation index is   -3.486705377512,     best is           -3.339612062834
    Train use time   1527.39394
    Train loader use time     71.07038
    Val use time     44.23943
    Total use time   1574.83697
    End lr is 0.000185055132, 0.000185055132
    time: 21:53:19
Epoch: 68, iter: 33999
    Begin lr is 0.000185055132, 0.000185055132
========> pt_241:  2.9901106283068657
========> pt_242:  2.581569328904152
========> pt_243:  4.436681568622589
========> pt_244:  2.8223051875829697
========> pt_245:  3.3481738343834877
========> pt_246:  3.790813460946083
========> pt_247:  2.6476117596030235
========> pt_248:  2.548070251941681
========> pt_249:  3.671238534152508
========> pt_250:  2.5697115808725357
========> pt_251:  3.5431403294205666
========> pt_252:  3.3137542381882668
========> pt_253:  3.991668149828911
========> pt_254:  3.693317174911499
========> pt_255:  3.017352521419525
========> pt_256:  2.2828253731131554
========> pt_257:  3.4027814865112305
========> pt_258:  2.5992638990283012
========> pt_259:  2.814495638012886
========> pt_260:  4.397159218788147
========> pt_261:  3.680834099650383
========> pt_262:  3.4032245352864265
========> pt_263:  2.869645431637764
========> pt_264:  3.557366132736206
========> pt_265:  2.677890323102474
========> pt_266:  3.3723292499780655
========> pt_267:  3.253886327147484
========> pt_268:  3.8175898790359497
========> pt_269:  2.6104798913002014
========> pt_270:  5.109773576259613
========> pt_271:  3.736935965716839
========> pt_272:  3.579201214015484
========> pt_273:  2.684742845594883
========> pt_274:  3.6599151045084
========> pt_275:  3.4586257115006447
========> pt_276:  2.488551251590252
========> pt_277:  2.529948279261589
========> pt_278:  4.493047073483467
========> pt_279:  2.7281809225678444
========> pt_280:  2.7604225650429726
========> pt_281:  2.8998874872922897
========> pt_282:  2.7793435379862785
========> pt_283:  5.648080185055733
========> pt_284:  2.7023766562342644
========> pt_285:  2.848966605961323
========> pt_286:  3.1176168844103813
========> pt_287:  4.218990504741669
========> pt_288:  2.4114106968045235
========> pt_289:  3.5465897992253304
========> pt_290:  3.9990218728780746
========> pt_291:  2.783270478248596
========> pt_292:  3.124876469373703
========> pt_293:  2.9140256717801094
========> pt_294:  2.7683789283037186
========> pt_295:  3.2627543434500694
========> pt_296:  2.1321212500333786
========> pt_297:  4.312735348939896
========> pt_298:  3.5169044882059097
========> pt_299:  2.5993911549448967
========> pt_300:  4.990344420075417
========> pt_301:  3.4255725517868996
========> pt_302:  4.927385076880455
========> pt_303:  3.145664818584919
========> pt_304:  2.759162001311779
========> pt_305:  3.48047461360693
========> pt_306:  2.44309950619936
========> pt_307:  2.7372674643993378
========> pt_308:  3.443089798092842
========> pt_309:  2.386367619037628
========> pt_310:  3.977782391011715
========> pt_311:  2.4550172314047813
========> pt_312:  2.4050499871373177
========> pt_313:  2.40194421261549
========> pt_314:  3.311847485601902
========> pt_315:  3.0354585871100426
========> pt_316:  4.03158999979496
========> pt_317:  4.804777130484581
========> pt_318:  5.009202808141708
========> pt_319:  2.4080415442585945
========> pt_320:  2.516755387187004
========> pt_321:  4.336196854710579
========> pt_322:  3.0922486260533333
========> pt_323:  7.505635321140289
========> pt_324:  2.99454215914011
========> pt_325:  3.530358672142029
========> pt_326:  2.8044330328702927
========> pt_327:  2.6576999202370644
========> pt_328:  2.6367272064089775
========> pt_329:  6.514270007610321
========> pt_330:  6.944427862763405
========> pt_331:  3.327982909977436
========> pt_332:  2.4072175100445747
========> pt_333:  2.3529598116874695
========> pt_334:  2.159745041280985
========> pt_335:  5.098407119512558
========> pt_336:  4.116240479052067
========> pt_337:  2.8183790296316147
========> pt_338:  2.461237385869026
========> pt_339:  2.8555964305996895
========> pt_340:  3.372342810034752
===============================================> mean Dose score: 3.366039197333157
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.044484120049,     best is           0.044484120049
            Average val evaluation index is   -3.366039197333,     best is           -3.339612062834
    Train use time   1526.23405
    Train loader use time     72.18909
    Val use time     44.28435
    Total use time   1573.75196
    End lr is 0.000182185658, 0.000182185658
    time: 22:19:33
Epoch: 69, iter: 34499
    Begin lr is 0.000182185658, 0.000182185658
========> pt_241:  3.0625904351472855
========> pt_242:  2.965928092598915
========> pt_243:  4.225944206118584
========> pt_244:  2.8914763405919075
========> pt_245:  3.448897674679756
========> pt_246:  4.136888533830643
========> pt_247:  3.160155303776264
========> pt_248:  2.3978248238563538
========> pt_249:  3.7954285740852356
========> pt_250:  2.848723828792572
========> pt_251:  3.660621792078018
========> pt_252:  3.3024756610393524
========> pt_253:  4.553518146276474
========> pt_254:  3.584893047809601
========> pt_255:  3.27694833278656
========> pt_256:  2.799900583922863
========> pt_257:  3.1926772743463516
========> pt_258:  2.7361302450299263
========> pt_259:  2.8293248638510704
========> pt_260:  6.3810184597969055
========> pt_261:  3.788469135761261
========> pt_262:  3.221874423325062
========> pt_263:  2.965657152235508
========> pt_264:  3.4360703825950623
========> pt_265:  2.66493733972311
========> pt_266:  3.3835796639323235
========> pt_267:  3.171732723712921
========> pt_268:  3.9741287380456924
========> pt_269:  2.6460614800453186
========> pt_270:  5.013156086206436
========> pt_271:  3.642045557498932
========> pt_272:  3.646330274641514
========> pt_273:  2.8463205695152283
========> pt_274:  3.778368979692459
========> pt_275:  3.7140630185604095
========> pt_276:  2.587001956999302
========> pt_277:  2.3715902864933014
========> pt_278:  4.122314341366291
========> pt_279:  2.6991861313581467
========> pt_280:  3.664293698966503
========> pt_281:  2.7157752960920334
========> pt_282:  2.776045575737953
========> pt_283:  8.387382701039314
========> pt_284:  2.856363095343113
========> pt_285:  2.6223809272050858
========> pt_286:  3.062652498483658
========> pt_287:  4.315081499516964
========> pt_288:  2.409319579601288
========> pt_289:  3.805452845990658
========> pt_290:  4.231666810810566
========> pt_291:  3.0435990542173386
========> pt_292:  3.1191176176071167
========> pt_293:  2.8552159667015076
========> pt_294:  2.9384084790945053
========> pt_295:  3.403027132153511
========> pt_296:  2.1812793239951134
========> pt_297:  3.931235931813717
========> pt_298:  3.6740921437740326
========> pt_299:  2.597079947590828
========> pt_300:  5.3756242245435715
========> pt_301:  4.19104166328907
========> pt_302:  4.580455720424652
========> pt_303:  2.8081023320555687
========> pt_304:  2.746012657880783
========> pt_305:  3.8803614675998688
========> pt_306:  2.581031881272793
========> pt_307:  2.940073236823082
========> pt_308:  3.307216204702854
========> pt_309:  2.411891557276249
========> pt_310:  4.8948826640844345
========> pt_311:  2.6055174320936203
========> pt_312:  2.3733893409371376
========> pt_313:  2.6041408255696297
========> pt_314:  3.1433233618736267
========> pt_315:  2.936507984995842
========> pt_316:  3.818535692989826
========> pt_317:  5.444216728210449
========> pt_318:  5.56683249771595
========> pt_319:  2.4411849305033684
========> pt_320:  2.3770521208643913
========> pt_321:  3.953327350318432
========> pt_322:  2.7804192155599594
========> pt_323:  7.714109197258949
========> pt_324:  2.9811184853315353
========> pt_325:  3.5062723606824875
========> pt_326:  2.934252582490444
========> pt_327:  3.095855340361595
========> pt_328:  2.7973703294992447
========> pt_329:  5.22535115480423
========> pt_330:  7.414765730500221
========> pt_331:  4.026535488665104
========> pt_332:  2.4877819791436195
========> pt_333:  2.413083277642727
========> pt_334:  2.1632274985313416
========> pt_335:  4.28156703710556
========> pt_336:  4.462726265192032
========> pt_337:  2.941521033644676
========> pt_338:  2.6387961581349373
========> pt_339:  2.9092786088585854
========> pt_340:  3.7969334796071053
===============================================> mean Dose score: 3.490654116868973
        ==> Saving latest model successfully !
            Average train loss is             0.044674061261,     best is           0.044484120049
            Average val evaluation index is   -3.490654116869,     best is           -3.339612062834
    Train use time   1526.84372
    Train loader use time     72.05981
    Val use time     44.70056
    Total use time   1573.11515
    End lr is 0.000179303794, 0.000179303794
    time: 22:45:46
Epoch: 70, iter: 34999
    Begin lr is 0.000179303794, 0.000179303794
========> pt_241:  2.9594143107533455
========> pt_242:  2.581731528043747
========> pt_243:  4.614609852433205
========> pt_244:  2.8017158061265945
========> pt_245:  3.634829521179199
========> pt_246:  3.9764748886227608
========> pt_247:  2.550066187977791
========> pt_248:  2.4336105957627296
========> pt_249:  3.9522313326597214
========> pt_250:  2.461753450334072
========> pt_251:  3.7127088382840157
========> pt_252:  3.69876179844141
========> pt_253:  3.832291066646576
========> pt_254:  4.0351323038339615
========> pt_255:  3.5483765974640846
========> pt_256:  2.405373342335224
========> pt_257:  3.1709978729486465
========> pt_258:  2.4859096482396126
========> pt_259:  3.042178377509117
========> pt_260:  4.158047176897526
========> pt_261:  4.215918630361557
========> pt_262:  3.7863686308264732
========> pt_263:  3.0814702063798904
========> pt_264:  4.127551130950451
========> pt_265:  2.77790904045105
========> pt_266:  3.6159520968794823
========> pt_267:  3.5368192568421364
========> pt_268:  4.124685525894165
========> pt_269:  2.8230249136686325
========> pt_270:  5.2968961000442505
========> pt_271:  3.4863748028874397
========> pt_272:  3.41219712048769
========> pt_273:  2.7421321347355843
========> pt_274:  4.0720174834132195
========> pt_275:  3.6517073586583138
========> pt_276:  2.5289956852793694
========> pt_277:  2.5420422852039337
========> pt_278:  4.588955789804459
========> pt_279:  2.5911901891231537
========> pt_280:  2.9402805492281914
========> pt_281:  3.0410800129175186
========> pt_282:  2.990904673933983
========> pt_283:  5.539598688483238
========> pt_284:  2.751736305654049
========> pt_285:  2.9126378521323204
========> pt_286:  3.3167577907443047
========> pt_287:  4.370043538510799
========> pt_288:  2.4972375109791756
========> pt_289:  4.090650826692581
========> pt_290:  4.275239445269108
========> pt_291:  2.4980930984020233
========> pt_292:  3.1971677392721176
========> pt_293:  3.1074338033795357
========> pt_294:  2.8462277352809906
========> pt_295:  3.3136016875505447
========> pt_296:  2.1912866458296776
========> pt_297:  4.767928719520569
========> pt_298:  3.5132398828864098
========> pt_299:  2.908720038831234
========> pt_300:  4.712138473987579
========> pt_301:  3.581504337489605
========> pt_302:  5.476176738739014
========> pt_303:  3.4379685297608376
========> pt_304:  2.961312457919121
========> pt_305:  3.6517931520938873
========> pt_306:  2.521141804754734
========> pt_307:  3.0021441355347633
========> pt_308:  3.811390846967697
========> pt_309:  2.4880195409059525
========> pt_310:  4.02092345058918
========> pt_311:  2.2422336041927338
========> pt_312:  2.5353068485856056
========> pt_313:  2.544260397553444
========> pt_314:  3.5727179422974586
========> pt_315:  3.0890148133039474
========> pt_316:  4.430608749389648
========> pt_317:  4.41628098487854
========> pt_318:  5.354188904166222
========> pt_319:  2.4340596422553062
========> pt_320:  2.5094718113541603
========> pt_321:  4.9043793976306915
========> pt_322:  3.2707305252552032
========> pt_323:  7.744912430644035
========> pt_324:  2.9202990233898163
========> pt_325:  3.5784726217389107
========> pt_326:  2.971258759498596
========> pt_327:  2.8766611963510513
========> pt_328:  2.819140739738941
========> pt_329:  8.14051665365696
========> pt_330:  6.7308225482702255
========> pt_331:  3.8579148799180984
========> pt_332:  2.4774226173758507
========> pt_333:  2.5754626095294952
========> pt_334:  2.089629117399454
========> pt_335:  5.217641219496727
========> pt_336:  3.839590810239315
========> pt_337:  2.8333034366369247
========> pt_338:  2.4063293263316154
========> pt_339:  2.7961457520723343
========> pt_340:  3.3527427911758423
===============================================> mean Dose score: 3.5032432304695247
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.044156202912,     best is           0.044156202912
            Average val evaluation index is   -3.503243230470,     best is           -3.339612062834
    Train use time   1524.06661
    Train loader use time     70.70612
    Val use time     44.86218
    Total use time   1571.97921
    End lr is 0.000176410652, 0.000176410652
    time: 23:11:58
Epoch: 71, iter: 35499
    Begin lr is 0.000176410652, 0.000176410652
========> pt_241:  3.1872352585196495
========> pt_242:  2.320067547261715
========> pt_243:  4.396019130945206
========> pt_244:  3.068791814148426
========> pt_245:  3.685360811650753
========> pt_246:  3.8032837584614754
========> pt_247:  2.639305703341961
========> pt_248:  2.447775900363922
========> pt_249:  4.367401674389839
========> pt_250:  2.7070415765047073
========> pt_251:  4.1082340478897095
========> pt_252:  3.7630729749798775
========> pt_253:  4.395393803715706
========> pt_254:  4.192680343985558
========> pt_255:  3.295179568231106
========> pt_256:  2.3133250698447227
========> pt_257:  3.002012185752392
========> pt_258:  2.7334417030215263
========> pt_259:  2.578713372349739
========> pt_260:  4.3127090111374855
========> pt_261:  4.509619027376175
========> pt_262:  3.4189333394169807
========> pt_263:  2.9047897085547447
========> pt_264:  4.269765354692936
========> pt_265:  2.6582344993948936
========> pt_266:  3.7730909883975983
========> pt_267:  3.49340308457613
========> pt_268:  4.173467308282852
========> pt_269:  2.703290395438671
========> pt_270:  5.827472433447838
========> pt_271:  3.6149906367063522
========> pt_272:  2.943274974822998
========> pt_273:  2.8170034661889076
========> pt_274:  4.185640066862106
========> pt_275:  3.541439324617386
========> pt_276:  2.4280329793691635
========> pt_277:  2.472292482852936
========> pt_278:  5.1075080037117
========> pt_279:  3.1098010763525963
========> pt_280:  2.8805148601531982
========> pt_281:  2.996986359357834
========> pt_282:  2.9808368533849716
========> pt_283:  5.810338780283928
========> pt_284:  2.5070594251155853
========> pt_285:  2.7680300176143646
========> pt_286:  3.3634544536471367
========> pt_287:  5.333069637417793
========> pt_288:  2.477436438202858
========> pt_289:  4.005597196519375
========> pt_290:  4.472862929105759
========> pt_291:  2.5230472534894943
========> pt_292:  3.2751550152897835
========> pt_293:  3.2071732357144356
========> pt_294:  2.630433775484562
========> pt_295:  3.490072265267372
========> pt_296:  2.215559408068657
========> pt_297:  4.654208868741989
========> pt_298:  3.3746279403567314
========> pt_299:  2.6755838096141815
========> pt_300:  4.598230347037315
========> pt_301:  3.7197379022836685
========> pt_302:  5.147113800048828
========> pt_303:  3.249506428837776
========> pt_304:  2.936902791261673
========> pt_305:  3.8216333836317062
========> pt_306:  2.616346701979637
========> pt_307:  2.7940530702471733
========> pt_308:  4.02125958353281
========> pt_309:  2.460000552237034
========> pt_310:  3.857894279062748
========> pt_311:  2.4266373366117477
========> pt_312:  2.5726259499788284
========> pt_313:  2.893945574760437
========> pt_314:  3.599005416035652
========> pt_315:  3.1757133826613426
========> pt_316:  4.721944481134415
========> pt_317:  4.158896505832672
========> pt_318:  6.232345998287201
========> pt_319:  2.492663860321045
========> pt_320:  2.4760713055729866
========> pt_321:  4.211982823908329
========> pt_322:  3.273724429309368
========> pt_323:  8.274182826280594
========> pt_324:  3.0448340624570847
========> pt_325:  3.6339979246258736
========> pt_326:  2.8178710490465164
========> pt_327:  2.757176496088505
========> pt_328:  2.999972179532051
========> pt_329:  5.974715352058411
========> pt_330:  6.276836022734642
========> pt_331:  3.7069838866591454
========> pt_332:  2.4796047434210777
========> pt_333:  2.3857834935188293
========> pt_334:  2.0524507015943527
========> pt_335:  5.277177169919014
========> pt_336:  4.156123995780945
========> pt_337:  3.146668002009392
========> pt_338:  2.6337898895144463
========> pt_339:  2.7893318235874176
========> pt_340:  3.359806537628174
===============================================> mean Dose score: 3.5213668499141932
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.044038155362,     best is           0.044038155362
            Average val evaluation index is   -3.521366849914,     best is           -3.339612062834
    Train use time   1525.16769
    Train loader use time     72.25039
    Val use time     45.41132
    Total use time   1573.70739
    End lr is 0.000173507348, 0.000173507348
    time: 23:38:12
Epoch: 72, iter: 35999
    Begin lr is 0.000173507348, 0.000173507348
========> pt_241:  3.042830564081669
========> pt_242:  2.3572228848934174
========> pt_243:  4.43526454269886
========> pt_244:  2.9764501750469208
========> pt_245:  3.592502847313881
========> pt_246:  3.8275841623544693
========> pt_247:  2.6859601214528084
========> pt_248:  2.313196249306202
========> pt_249:  4.056330844759941
========> pt_250:  2.70250391215086
========> pt_251:  3.6008597537875175
========> pt_252:  3.667721003293991
========> pt_253:  4.161602780222893
========> pt_254:  3.9990124851465225
========> pt_255:  3.2486028596758842
========> pt_256:  2.3898595944046974
========> pt_257:  2.9555241391062737
========> pt_258:  2.744792513549328
========> pt_259:  2.6242496073246
========> pt_260:  4.324635602533817
========> pt_261:  4.461625814437866
========> pt_262:  3.4350327774882317
========> pt_263:  3.0244027078151703
========> pt_264:  4.203783683478832
========> pt_265:  2.690522037446499
========> pt_266:  3.535630665719509
========> pt_267:  3.506598584353924
========> pt_268:  4.239589273929596
========> pt_269:  2.682420164346695
========> pt_270:  5.543776229023933
========> pt_271:  3.574310466647148
========> pt_272:  3.065408319234848
========> pt_273:  2.7968910336494446
========> pt_274:  3.8783684000372887
========> pt_275:  3.8940516486763954
========> pt_276:  2.5069723278284073
========> pt_277:  2.4740028753876686
========> pt_278:  4.97040331363678
========> pt_279:  2.803787887096405
========> pt_280:  2.9839058592915535
========> pt_281:  2.8434020280838013
========> pt_282:  2.8401077166199684
========> pt_283:  6.383349224925041
========> pt_284:  2.5430843234062195
========> pt_285:  2.6986009627580643
========> pt_286:  3.3345311135053635
========> pt_287:  4.721839651465416
========> pt_288:  2.4749651178717613
========> pt_289:  4.276173263788223
========> pt_290:  4.471220597624779
========> pt_291:  2.6670240238308907
========> pt_292:  3.214372582733631
========> pt_293:  3.2965783402323723
========> pt_294:  2.8055141866207123
========> pt_295:  3.518252409994602
========> pt_296:  2.1964000910520554
========> pt_297:  4.63242881000042
========> pt_298:  3.41518085449934
========> pt_299:  2.7183225005865097
========> pt_300:  4.833535924553871
========> pt_301:  3.7834910303354263
========> pt_302:  5.399089902639389
========> pt_303:  3.0096905678510666
========> pt_304:  2.9380162805318832
========> pt_305:  3.6582091450691223
========> pt_306:  2.672540880739689
========> pt_307:  2.863178066909313
========> pt_308:  3.870907761156559
========> pt_309:  2.400238774716854
========> pt_310:  4.062141589820385
========> pt_311:  2.543962597846985
========> pt_312:  2.527935653924942
========> pt_313:  2.6428107172250748
========> pt_314:  3.5321470350027084
========> pt_315:  3.141823671758175
========> pt_316:  4.795938581228256
========> pt_317:  4.241190925240517
========> pt_318:  6.189012750983238
========> pt_319:  2.422291338443756
========> pt_320:  2.363637052476406
========> pt_321:  4.470594227313995
========> pt_322:  3.198835104703903
========> pt_323:  8.036318570375443
========> pt_324:  3.009253516793251
========> pt_325:  3.557775802910328
========> pt_326:  2.8520937636494637
========> pt_327:  2.6928389817476273
========> pt_328:  2.809734493494034
========> pt_329:  6.3655198365449905
========> pt_330:  6.574318632483482
========> pt_331:  3.78977220505476
========> pt_332:  2.4134108051657677
========> pt_333:  2.4157001078128815
========> pt_334:  2.0503578893840313
========> pt_335:  4.842829778790474
========> pt_336:  4.093780592083931
========> pt_337:  3.0205440893769264
========> pt_338:  2.583611160516739
========> pt_339:  2.8744177892804146
========> pt_340:  3.5291166231036186
===============================================> mean Dose score: 3.501021527312696
        ==> Saving latest model successfully !
            Average train loss is             0.044094663389,     best is           0.044038155362
            Average val evaluation index is   -3.501021527313,     best is           -3.339612062834
    Train use time   1526.31541
    Train loader use time     72.94552
    Val use time     45.30465
    Total use time   1573.40055
    End lr is 0.000170595001, 0.000170595001
    time: 00:04:25
Epoch: 73, iter: 36499
    Begin lr is 0.000170595001, 0.000170595001
========> pt_241:  2.9489394277334213
========> pt_242:  2.6521697640419006
========> pt_243:  4.242268167436123
========> pt_244:  2.660563178360462
========> pt_245:  3.3468668535351753
========> pt_246:  3.6285431310534477
========> pt_247:  2.693764977157116
========> pt_248:  2.4188676849007607
========> pt_249:  4.085630215704441
========> pt_250:  2.554634101688862
========> pt_251:  3.3451462909579277
========> pt_252:  3.5077032074332237
========> pt_253:  4.119172319769859
========> pt_254:  3.470306657254696
========> pt_255:  3.2313672453165054
========> pt_256:  2.3288891464471817
========> pt_257:  3.2441309094429016
========> pt_258:  2.4080295488238335
========> pt_259:  2.5960324332118034
========> pt_260:  4.893319606781006
========> pt_261:  3.8172347098588943
========> pt_262:  3.3507950976490974
========> pt_263:  2.838175408542156
========> pt_264:  3.5089457780122757
========> pt_265:  2.6067378371953964
========> pt_266:  3.2889899238944054
========> pt_267:  3.25033750385046
========> pt_268:  3.848749063909054
========> pt_269:  2.7117586508393288
========> pt_270:  4.866218268871307
========> pt_271:  3.669613152742386
========> pt_272:  3.5676011070609093
========> pt_273:  2.6563183590769768
========> pt_274:  3.8943395391106606
========> pt_275:  3.572356514632702
========> pt_276:  2.402973212301731
========> pt_277:  2.3753884062170982
========> pt_278:  4.888112023472786
========> pt_279:  2.5439435616135597
========> pt_280:  2.9788224026560783
========> pt_281:  2.8441110625863075
========> pt_282:  2.7229196205735207
========> pt_283:  6.9794712215662
========> pt_284:  2.637253701686859
========> pt_285:  2.7294759079813957
========> pt_286:  3.0727628245949745
========> pt_287:  4.313755482435226
========> pt_288:  2.1888163685798645
========> pt_289:  3.9320239797234535
========> pt_290:  3.7940097227692604
========> pt_291:  2.715963050723076
========> pt_292:  2.9293088987469673
========> pt_293:  3.005913309752941
========> pt_294:  2.7199142426252365
========> pt_295:  3.357674479484558
========> pt_296:  2.235039733350277
========> pt_297:  4.1866448149085045
========> pt_298:  3.3285237476229668
========> pt_299:  2.438862770795822
========> pt_300:  4.655192494392395
========> pt_301:  3.644045665860176
========> pt_302:  4.863193854689598
========> pt_303:  3.014930747449398
========> pt_304:  2.7117521315813065
========> pt_305:  3.342274948954582
========> pt_306:  2.4233146011829376
========> pt_307:  2.805669605731964
========> pt_308:  3.4802182763814926
========> pt_309:  2.4054228886961937
========> pt_310:  3.8033703342080116
========> pt_311:  2.337915189564228
========> pt_312:  2.2783896699547768
========> pt_313:  2.3284664377570152
========> pt_314:  3.128916062414646
========> pt_315:  2.870299182832241
========> pt_316:  4.238296635448933
========> pt_317:  4.704920873045921
========> pt_318:  4.969649687409401
========> pt_319:  2.4162612855434418
========> pt_320:  2.295526973903179
========> pt_321:  4.306870885193348
========> pt_322:  2.99127209931612
========> pt_323:  7.747529000043869
========> pt_324:  2.8055347874760628
========> pt_325:  3.462030068039894
========> pt_326:  2.76683047413826
========> pt_327:  2.649768330156803
========> pt_328:  2.325531467795372
========> pt_329:  5.931168273091316
========> pt_330:  7.025260925292969
========> pt_331:  3.6402321606874466
========> pt_332:  2.3168326914310455
========> pt_333:  2.1750377863645554
========> pt_334:  2.01553201302886
========> pt_335:  4.778301641345024
========> pt_336:  4.101391434669495
========> pt_337:  2.7476776763796806
========> pt_338:  2.464537173509598
========> pt_339:  2.79189832508564
========> pt_340:  3.444925881922245
===============================================> mean Dose score: 3.3435239097103477
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.043816749789,     best is           0.043816749789
            Average val evaluation index is   -3.343523909710,     best is           -3.339612062834
    Train use time   1525.60520
    Train loader use time     71.36445
    Val use time     44.95868
    Total use time   1573.76102
    End lr is 0.000167674733, 0.000167674733
    time: 00:30:39
Epoch: 74, iter: 36999
    Begin lr is 0.000167674733, 0.000167674733
========> pt_241:  2.820923365652561
========> pt_242:  2.508765384554863
========> pt_243:  4.45709154009819
========> pt_244:  2.710237316787243
========> pt_245:  3.4321053698658943
========> pt_246:  3.8072005286812782
========> pt_247:  2.5542213022708893
========> pt_248:  2.3659566044807434
========> pt_249:  3.818148449063301
========> pt_250:  2.432636097073555
========> pt_251:  3.5985302925109863
========> pt_252:  3.613322749733925
========> pt_253:  3.612748011946678
========> pt_254:  3.7834589555859566
========> pt_255:  3.368525914847851
========> pt_256:  2.306208908557892
========> pt_257:  3.061254508793354
========> pt_258:  2.343870662152767
========> pt_259:  2.826128341257572
========> pt_260:  4.162300080060959
========> pt_261:  3.7123090773820877
========> pt_262:  3.4323150292038918
========> pt_263:  2.910100817680359
========> pt_264:  3.830357976257801
========> pt_265:  2.6794956251978874
========> pt_266:  3.530447855591774
========> pt_267:  3.427644893527031
========> pt_268:  3.9800477027893066
========> pt_269:  2.7342965081334114
========> pt_270:  5.113943815231323
========> pt_271:  3.446746841073036
========> pt_272:  3.3852162584662437
========> pt_273:  2.6990708708763123
========> pt_274:  4.098510183393955
========> pt_275:  3.761243149638176
========> pt_276:  2.416420094668865
========> pt_277:  2.5200163200497627
========> pt_278:  4.348078332841396
========> pt_279:  2.446012571454048
========> pt_280:  2.7006639167666435
========> pt_281:  2.8634709119796753
========> pt_282:  2.8390461206436157
========> pt_283:  7.236237674951553
========> pt_284:  2.5790857523679733
========> pt_285:  2.6693469658493996
========> pt_286:  3.196396119892597
========> pt_287:  4.316973388195038
========> pt_288:  2.3864109069108963
========> pt_289:  3.8432465493679047
========> pt_290:  3.9745647460222244
========> pt_291:  2.5184501335024834
========> pt_292:  3.0357175320386887
========> pt_293:  3.04785143584013
========> pt_294:  2.7751125395298004
========> pt_295:  3.1479697674512863
========> pt_296:  2.1329549327492714
========> pt_297:  4.3103983253240585
========> pt_298:  3.3600563555955887
========> pt_299:  2.669098451733589
========> pt_300:  4.443742707371712
========> pt_301:  3.511691428720951
========> pt_302:  5.10028675198555
========> pt_303:  3.1876185908913612
========> pt_304:  2.9037150740623474
========> pt_305:  3.4025374054908752
========> pt_306:  2.423843964934349
========> pt_307:  2.8774677589535713
========> pt_308:  3.484135828912258
========> pt_309:  2.3939568176865578
========> pt_310:  3.779786005616188
========> pt_311:  2.2189121320843697
========> pt_312:  2.414412945508957
========> pt_313:  2.375081479549408
========> pt_314:  3.3578460663557053
========> pt_315:  3.007805459201336
========> pt_316:  4.1499317437410355
========> pt_317:  4.349635913968086
========> pt_318:  4.89403672516346
========> pt_319:  2.493348903954029
========> pt_320:  2.3300370573997498
========> pt_321:  4.349642172455788
========> pt_322:  2.999945841729641
========> pt_323:  7.7628883719444275
========> pt_324:  2.8407231345772743
========> pt_325:  3.469904288649559
========> pt_326:  2.753480337560177
========> pt_327:  2.723301388323307
========> pt_328:  2.511986419558525
========> pt_329:  7.009080648422241
========> pt_330:  7.105087414383888
========> pt_331:  3.4930844232439995
========> pt_332:  2.462835907936096
========> pt_333:  2.3531006276607513
========> pt_334:  2.0034844242036343
========> pt_335:  4.940862730145454
========> pt_336:  3.594157174229622
========> pt_337:  2.7900343388319016
========> pt_338:  2.3482216149568558
========> pt_339:  2.6656474173069
========> pt_340:  3.188789188861847
===============================================> mean Dose score: 3.361250494606793
        ==> Saving latest model successfully !
            Average train loss is             0.043856571481,     best is           0.043816749789
            Average val evaluation index is   -3.361250494607,     best is           -3.339612062834
    Train use time   1527.22488
    Train loader use time     73.91366
    Val use time     45.02151
    Total use time   1573.91448
    End lr is 0.000164747670, 0.000164747670
    time: 00:56:53
Epoch: 75, iter: 37499
    Begin lr is 0.000164747670, 0.000164747670
========> pt_241:  3.001372776925564
========> pt_242:  2.5043080374598503
========> pt_243:  4.36004925519228
========> pt_244:  2.8524338081479073
========> pt_245:  3.3931460231542587
========> pt_246:  3.8603095337748528
========> pt_247:  2.6320189982652664
========> pt_248:  2.3641040921211243
========> pt_249:  3.788958340883255
========> pt_250:  2.467571496963501
========> pt_251:  3.7421292066574097
========> pt_252:  3.590649291872978
========> pt_253:  3.819083049893379
========> pt_254:  3.816579654812813
========> pt_255:  3.8362805917859077
========> pt_256:  2.3502152040600777
========> pt_257:  3.200036995112896
========> pt_258:  2.458796314895153
========> pt_259:  3.197435289621353
========> pt_260:  4.28827092051506
========> pt_261:  3.825496695935726
========> pt_262:  3.548453524708748
========> pt_263:  3.12906913459301
========> pt_264:  3.7510452046990395
========> pt_265:  2.7362697571516037
========> pt_266:  3.4434913843870163
========> pt_267:  3.3605043590068817
========> pt_268:  3.9186887070536613
========> pt_269:  2.663598023355007
========> pt_270:  5.250861793756485
========> pt_271:  3.791632279753685
========> pt_272:  3.834921717643738
========> pt_273:  2.735121324658394
========> pt_274:  4.188097044825554
========> pt_275:  3.4418610483407974
========> pt_276:  2.5684450194239616
========> pt_277:  2.4857597053050995
========> pt_278:  4.4714537262916565
========> pt_279:  2.415096163749695
========> pt_280:  2.7739789709448814
========> pt_281:  2.8447934985160828
========> pt_282:  2.808164395391941
========> pt_283:  6.602245047688484
========> pt_284:  2.8881288319826126
========> pt_285:  2.8240646049380302
========> pt_286:  3.1786003708839417
========> pt_287:  4.418485015630722
========> pt_288:  2.381944954395294
========> pt_289:  3.863039016723633
========> pt_290:  4.070787169039249
========> pt_291:  2.660521976649761
========> pt_292:  2.8997743129730225
========> pt_293:  3.0027999728918076
========> pt_294:  2.8677352890372276
========> pt_295:  3.1245875358581543
========> pt_296:  2.235381342470646
========> pt_297:  4.098883084952831
========> pt_298:  3.461029753088951
========> pt_299:  2.604820393025875
========> pt_300:  4.783367365598679
========> pt_301:  3.3447355777025223
========> pt_302:  4.848651736974716
========> pt_303:  3.0717067047953606
========> pt_304:  2.8078220039606094
========> pt_305:  3.6202647164463997
========> pt_306:  2.4726390466094017
========> pt_307:  2.9181385412812233
========> pt_308:  3.498825542628765
========> pt_309:  2.383481152355671
========> pt_310:  4.052690230309963
========> pt_311:  2.3755067959427834
========> pt_312:  2.4211757630109787
========> pt_313:  2.41661436855793
========> pt_314:  3.3407825604081154
========> pt_315:  3.0454739928245544
========> pt_316:  4.170978777110577
========> pt_317:  4.275343492627144
========> pt_318:  5.377607643604279
========> pt_319:  2.417718470096588
========> pt_320:  2.447074167430401
========> pt_321:  4.128441661596298
========> pt_322:  3.0521439760923386
========> pt_323:  7.411094605922699
========> pt_324:  2.8246990591287613
========> pt_325:  3.4948506206274033
========> pt_326:  2.7432456240057945
========> pt_327:  2.8285375982522964
========> pt_328:  2.5433896854519844
========> pt_329:  6.705801114439964
========> pt_330:  7.471852004528046
========> pt_331:  3.416987732052803
========> pt_332:  2.4079226329922676
========> pt_333:  2.28166077286005
========> pt_334:  1.986912339925766
========> pt_335:  4.810864552855492
========> pt_336:  3.8162589073181152
========> pt_337:  2.9407160356640816
========> pt_338:  2.5330858677625656
========> pt_339:  2.924168072640896
========> pt_340:  3.2836488261818886
===============================================> mean Dose score: 3.4039026137441395
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.043760284901,     best is           0.043760284901
            Average val evaluation index is   -3.403902613744,     best is           -3.339612062834
    Train use time   1524.44953
    Train loader use time     71.34263
    Val use time     44.98631
    Total use time   1572.64089
    End lr is 0.000161814941, 0.000161814941
    time: 01:23:05
Epoch: 76, iter: 37999
    Begin lr is 0.000161814941, 0.000161814941
========> pt_241:  3.02181925624609
========> pt_242:  2.5567924976348877
========> pt_243:  4.000901505351067
========> pt_244:  2.8337767347693443
========> pt_245:  3.214568682014942
========> pt_246:  3.6662278324365616
========> pt_247:  2.6843253523111343
========> pt_248:  2.322453334927559
========> pt_249:  3.6733025312423706
========> pt_250:  2.632425017654896
========> pt_251:  3.4229179099202156
========> pt_252:  3.3201532810926437
========> pt_253:  3.884180188179016
========> pt_254:  3.547964058816433
========> pt_255:  3.7813083827495575
========> pt_256:  2.4870432168245316
========> pt_257:  2.893049567937851
========> pt_258:  2.5850730389356613
========> pt_259:  2.947423830628395
========> pt_260:  3.953746147453785
========> pt_261:  3.7408793345093727
========> pt_262:  3.2339950278401375
========> pt_263:  2.9843447357416153
========> pt_264:  3.5469937324523926
========> pt_265:  2.5143755972385406
========> pt_266:  3.2113544270396233
========> pt_267:  3.3574627339839935
========> pt_268:  3.6899516731500626
========> pt_269:  2.4987656250596046
========> pt_270:  4.951272159814835
========> pt_271:  3.429032191634178
========> pt_272:  3.6177055165171623
========> pt_273:  2.7586524561047554
========> pt_274:  4.14853036403656
========> pt_275:  3.6240949109196663
========> pt_276:  2.4170224741101265
========> pt_277:  2.4467672407627106
========> pt_278:  4.338276498019695
========> pt_279:  3.0574939399957657
========> pt_280:  2.831287421286106
========> pt_281:  2.7357636019587517
========> pt_282:  2.606787644326687
========> pt_283:  6.0591089725494385
========> pt_284:  2.8829655796289444
========> pt_285:  2.645505517721176
========> pt_286:  3.0959583446383476
========> pt_287:  4.4821421802043915
========> pt_288:  2.2898906841874123
========> pt_289:  3.686043508350849
========> pt_290:  3.879353068768978
========> pt_291:  2.627457343041897
========> pt_292:  3.1108877062797546
========> pt_293:  2.9744983091950417
========> pt_294:  2.728036977350712
========> pt_295:  3.0588243901729584
========> pt_296:  2.0912166871130466
========> pt_297:  4.018976800143719
========> pt_298:  3.4050926938652992
========> pt_299:  2.5295286998152733
========> pt_300:  4.549353122711182
========> pt_301:  3.386450745165348
========> pt_302:  4.590446874499321
========> pt_303:  2.7458152547478676
========> pt_304:  2.7037566527724266
========> pt_305:  3.6013460904359818
========> pt_306:  2.514525018632412
========> pt_307:  2.791460230946541
========> pt_308:  3.358096666634083
========> pt_309:  2.264217585325241
========> pt_310:  3.985452689230442
========> pt_311:  2.5712256133556366
========> pt_312:  2.3790013790130615
========> pt_313:  2.5586922094225883
========> pt_314:  3.2543040812015533
========> pt_315:  2.9135049134492874
========> pt_316:  4.030730500817299
========> pt_317:  4.364556148648262
========> pt_318:  5.2972085028886795
========> pt_319:  2.3509597033262253
========> pt_320:  2.2279081866145134
========> pt_321:  4.143667258322239
========> pt_322:  3.0178245157003403
========> pt_323:  7.100660055875778
========> pt_324:  2.7544887363910675
========> pt_325:  3.4585015848279
========> pt_326:  2.8076978772878647
========> pt_327:  2.749975584447384
========> pt_328:  2.769159935414791
========> pt_329:  5.824123099446297
========> pt_330:  7.811282128095627
========> pt_331:  3.907512091100216
========> pt_332:  2.398875467479229
========> pt_333:  2.275042161345482
========> pt_334:  2.044959031045437
========> pt_335:  4.145160689949989
========> pt_336:  3.7883322313427925
========> pt_337:  2.904501035809517
========> pt_338:  2.5352729484438896
========> pt_339:  2.752217687666416
========> pt_340:  3.2463837042450905
===============================================> mean Dose score: 3.316103991307318
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.043320125259,     best is           0.043320125259
            Average val evaluation index is   -3.316103991307,     best is           -3.316103991307
    Train use time   1524.01990
    Train loader use time     71.72944
    Val use time     45.04423
    Total use time   1574.07713
    End lr is 0.000158877677, 0.000158877677
    time: 01:49:19
Epoch: 77, iter: 38499
    Begin lr is 0.000158877677, 0.000158877677
========> pt_241:  3.0089111253619194
========> pt_242:  2.6430412381887436
========> pt_243:  3.9220578595995903
========> pt_244:  2.866436652839184
========> pt_245:  3.2848940044641495
========> pt_246:  3.399004228413105
========> pt_247:  2.8563615307211876
========> pt_248:  2.3375120386481285
========> pt_249:  3.7725329399108887
========> pt_250:  2.920609340071678
========> pt_251:  3.311399221420288
========> pt_252:  3.3939408510923386
========> pt_253:  4.216730669140816
========> pt_254:  3.4179257228970528
========> pt_255:  3.466857448220253
========> pt_256:  2.6291416585445404
========> pt_257:  2.898377627134323
========> pt_258:  2.769975885748863
========> pt_259:  2.812497094273567
========> pt_260:  4.731011465191841
========> pt_261:  3.920104429125786
========> pt_262:  3.3641984313726425
========> pt_263:  3.00959512591362
========> pt_264:  3.3568543568253517
========> pt_265:  2.638910375535488
========> pt_266:  3.109007552266121
========> pt_267:  3.3792706951498985
========> pt_268:  3.6149202287197113
========> pt_269:  2.578641138970852
========> pt_270:  4.87452432513237
========> pt_271:  3.318529985845089
========> pt_272:  3.643030747771263
========> pt_273:  2.8690404444932938
========> pt_274:  3.8488799706101418
========> pt_275:  3.5765285789966583
========> pt_276:  2.3949766904115677
========> pt_277:  2.448851577937603
========> pt_278:  4.495551511645317
========> pt_279:  2.6594262197613716
========> pt_280:  2.882871702313423
========> pt_281:  2.7624302357435226
========> pt_282:  2.5363314151763916
========> pt_283:  6.773027181625366
========> pt_284:  2.709307409822941
========> pt_285:  2.5873229652643204
========> pt_286:  3.2033834606409073
========> pt_287:  4.216504320502281
========> pt_288:  2.4288635328412056
========> pt_289:  3.6596697196364403
========> pt_290:  3.7663665041327477
========> pt_291:  2.850062884390354
========> pt_292:  3.062770366668701
========> pt_293:  2.92966328561306
========> pt_294:  2.73132111877203
========> pt_295:  3.2560189068317413
========> pt_296:  2.287808172404766
========> pt_297:  4.300542511045933
========> pt_298:  3.276943117380142
========> pt_299:  2.6324714347720146
========> pt_300:  4.583264738321304
========> pt_301:  3.4509215131402016
========> pt_302:  4.47166234254837
========> pt_303:  2.6892664283514023
========> pt_304:  2.79828067868948
========> pt_305:  3.404584191739559
========> pt_306:  2.666567414999008
========> pt_307:  2.8380270302295685
========> pt_308:  3.2541486620903015
========> pt_309:  2.3076697438955307
========> pt_310:  3.822411522269249
========> pt_311:  2.683407701551914
========> pt_312:  2.401909790933132
========> pt_313:  2.464987523853779
========> pt_314:  3.4203464537858963
========> pt_315:  3.0293119698762894
========> pt_316:  3.9304160699248314
========> pt_317:  4.428827688097954
========> pt_318:  5.515313148498535
========> pt_319:  2.4694735556840897
========> pt_320:  2.1995650604367256
========> pt_321:  3.8767284154891968
========> pt_322:  2.9895517975091934
========> pt_323:  7.594952806830406
========> pt_324:  2.8663482517004013
========> pt_325:  3.40922798961401
========> pt_326:  2.7813277393579483
========> pt_327:  2.7910296991467476
========> pt_328:  2.809474766254425
========> pt_329:  5.059209167957306
========> pt_330:  7.354828715324402
========> pt_331:  3.898303508758545
========> pt_332:  2.4357035383582115
========> pt_333:  2.2592707723379135
========> pt_334:  2.155531905591488
========> pt_335:  4.2693353444337845
========> pt_336:  4.060935266315937
========> pt_337:  2.8329970315098763
========> pt_338:  2.643166668713093
========> pt_339:  2.9415392875671387
========> pt_340:  3.3631378784775734
===============================================> mean Dose score: 3.338367050141096
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.043115974203,     best is           0.043115974203
            Average val evaluation index is   -3.338367050141,     best is           -3.316103991307
    Train use time   1524.75510
    Train loader use time     72.98723
    Val use time     44.98093
    Total use time   1573.12006
    End lr is 0.000155937009, 0.000155937009
    time: 02:15:32
Epoch: 78, iter: 38999
    Begin lr is 0.000155937009, 0.000155937009
========> pt_241:  3.0771156027913094
========> pt_242:  2.483619824051857
========> pt_243:  4.114850051701069
========> pt_244:  2.7030137181282043
========> pt_245:  3.1940369307994843
========> pt_246:  3.400483578443527
========> pt_247:  2.562403231859207
========> pt_248:  2.359342686831951
========> pt_249:  3.7669355049729347
========> pt_250:  2.555704042315483
========> pt_251:  3.384803980588913
========> pt_252:  3.345053195953369
========> pt_253:  3.8319283351302147
========> pt_254:  3.4399988874793053
========> pt_255:  3.1868454068899155
========> pt_256:  2.1551981195807457
========> pt_257:  2.9136746749281883
========> pt_258:  2.571592517197132
========> pt_259:  2.6621244102716446
========> pt_260:  3.806139715015888
========> pt_261:  3.7470215186476707
========> pt_262:  3.316522315144539
========> pt_263:  2.734508253633976
========> pt_264:  3.3950212225317955
========> pt_265:  2.5120795145630836
========> pt_266:  3.501099720597267
========> pt_267:  3.215588293969631
========> pt_268:  3.725326471030712
========> pt_269:  2.610124461352825
========> pt_270:  4.993778243660927
========> pt_271:  3.3730878308415413
========> pt_272:  3.3650892227888107
========> pt_273:  2.5540554523468018
========> pt_274:  3.992609791457653
========> pt_275:  3.381149023771286
========> pt_276:  2.2062034904956818
========> pt_277:  2.3055434226989746
========> pt_278:  4.4125086814165115
========> pt_279:  2.335512451827526
========> pt_280:  2.72237591445446
========> pt_281:  2.818172499537468
========> pt_282:  2.7666278555989265
========> pt_283:  5.9563326090574265
========> pt_284:  2.648441530764103
========> pt_285:  2.601608745753765
========> pt_286:  3.086869977414608
========> pt_287:  4.254328273236752
========> pt_288:  2.1696983836591244
========> pt_289:  3.581710085272789
========> pt_290:  3.6969129368662834
========> pt_291:  2.4834438040852547
========> pt_292:  2.776575982570648
========> pt_293:  2.702186554670334
========> pt_294:  2.5742075219750404
========> pt_295:  3.175075799226761
========> pt_296:  2.0253716595470905
========> pt_297:  4.147537089884281
========> pt_298:  3.3097652345895767
========> pt_299:  2.440543957054615
========> pt_300:  4.601031541824341
========> pt_301:  3.4773777052760124
========> pt_302:  4.788205176591873
========> pt_303:  3.1662028282880783
========> pt_304:  2.6727179437875748
========> pt_305:  3.281874805688858
========> pt_306:  2.3988131433725357
========> pt_307:  2.598525397479534
========> pt_308:  3.2146208360791206
========> pt_309:  2.3121023178100586
========> pt_310:  3.5694463178515434
========> pt_311:  2.357555106282234
========> pt_312:  2.2795912995934486
========> pt_313:  2.4385008215904236
========> pt_314:  3.2447499781847
========> pt_315:  2.929282560944557
========> pt_316:  3.8807860016822815
========> pt_317:  4.290322922170162
========> pt_318:  5.060923993587494
========> pt_319:  2.309053912758827
========> pt_320:  2.2965778782963753
========> pt_321:  3.830421604216099
========> pt_322:  2.9341910406947136
========> pt_323:  7.535998895764351
========> pt_324:  2.764957621693611
========> pt_325:  3.299166224896908
========> pt_326:  2.7311448380351067
========> pt_327:  2.5750161707401276
========> pt_328:  2.4509911984205246
========> pt_329:  6.2285783886909485
========> pt_330:  6.881617158651352
========> pt_331:  3.396538645029068
========> pt_332:  2.414654418826103
========> pt_333:  2.1763580664992332
========> pt_334:  1.9724351540207863
========> pt_335:  5.0884488224983215
========> pt_336:  3.8587741181254387
========> pt_337:  2.562905214726925
========> pt_338:  2.330254279077053
========> pt_339:  2.6042506098747253
========> pt_340:  3.238370232284069
===============================================> mean Dose score: 3.231728154048324
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.043237685278,     best is           0.043115974203
            Average val evaluation index is   -3.231728154048,     best is           -3.231728154048
    Train use time   1526.83075
    Train loader use time     72.54464
    Val use time     45.12339
    Total use time   1575.27846
    End lr is 0.000152994072, 0.000152994072
    time: 02:41:48
Epoch: 79, iter: 39499
    Begin lr is 0.000152994072, 0.000152994072
========> pt_241:  2.8075946122407913
========> pt_242:  2.4409353733062744
========> pt_243:  4.220134764909744
========> pt_244:  2.70517211407423
========> pt_245:  3.3939405903220177
========> pt_246:  3.515510931611061
========> pt_247:  2.40474671125412
========> pt_248:  2.348773404955864
========> pt_249:  3.701343424618244
========> pt_250:  2.430824004113674
========> pt_251:  3.4860238060355186
========> pt_252:  3.732117973268032
========> pt_253:  3.6746924370527267
========> pt_254:  3.3469169214367867
========> pt_255:  3.553263172507286
========> pt_256:  2.3644837737083435
========> pt_257:  2.8535326942801476
========> pt_258:  2.4501272663474083
========> pt_259:  2.791841998696327
========> pt_260:  4.075588211417198
========> pt_261:  3.789653293788433
========> pt_262:  3.4105563536286354
========> pt_263:  2.9758691787719727
========> pt_264:  3.74697457998991
========> pt_265:  2.6326151192188263
========> pt_266:  3.8606618344783783
========> pt_267:  3.55346005409956
========> pt_268:  3.6991140991449356
========> pt_269:  2.6140693947672844
========> pt_270:  4.853474423289299
========> pt_271:  3.2673553749918938
========> pt_272:  3.326774761080742
========> pt_273:  2.644331268966198
========> pt_274:  4.065639041364193
========> pt_275:  3.3799633011221886
========> pt_276:  2.2271516919136047
========> pt_277:  2.48456459492445
========> pt_278:  4.023611210286617
========> pt_279:  2.4966293945908546
========> pt_280:  2.7494563907384872
========> pt_281:  2.972601465880871
========> pt_282:  2.748350203037262
========> pt_283:  5.772201642394066
========> pt_284:  2.548266351222992
========> pt_285:  2.87185937166214
========> pt_286:  3.1825312227010727
========> pt_287:  4.141273647546768
========> pt_288:  2.3462512344121933
========> pt_289:  3.636714890599251
========> pt_290:  3.834105506539345
========> pt_291:  2.417414151132107
========> pt_292:  2.977740988135338
========> pt_293:  2.9635806381702423
========> pt_294:  2.543211840093136
========> pt_295:  3.0376631394028664
========> pt_296:  2.155313901603222
========> pt_297:  4.2934732884168625
========> pt_298:  3.1908422335982323
========> pt_299:  2.6969844475388527
========> pt_300:  4.593048319220543
========> pt_301:  3.5112447291612625
========> pt_302:  4.692472219467163
========> pt_303:  3.1889576464891434
========> pt_304:  2.701151557266712
========> pt_305:  3.2093796133995056
========> pt_306:  2.4100609496235847
========> pt_307:  2.830745540559292
========> pt_308:  3.383944481611252
========> pt_309:  2.358999252319336
========> pt_310:  3.7342367321252823
========> pt_311:  2.1371435560286045
========> pt_312:  2.352941818535328
========> pt_313:  2.504725009202957
========> pt_314:  3.463122174143791
========> pt_315:  3.123619817197323
========> pt_316:  3.8060088083148003
========> pt_317:  4.326913170516491
========> pt_318:  5.508298948407173
========> pt_319:  2.362109199166298
========> pt_320:  2.365889847278595
========> pt_321:  4.012276828289032
========> pt_322:  2.987644262611866
========> pt_323:  7.724385634064674
========> pt_324:  2.8828860446810722
========> pt_325:  3.4816572070121765
========> pt_326:  2.7993274107575417
========> pt_327:  2.772517092525959
========> pt_328:  2.7477068826556206
========> pt_329:  7.59535439312458
========> pt_330:  6.797484308481216
========> pt_331:  3.4855734556913376
========> pt_332:  2.4399952962994576
========> pt_333:  2.3377352580428123
========> pt_334:  1.9921162724494934
========> pt_335:  4.893661737442017
========> pt_336:  3.8137737661600113
========> pt_337:  2.8083855286240578
========> pt_338:  2.361000142991543
========> pt_339:  2.6511887460947037
========> pt_340:  3.271891474723816
===============================================> mean Dose score: 3.308514448441565
        ==> Saving latest model successfully !
            Average train loss is             0.043257340498,     best is           0.043115974203
            Average val evaluation index is   -3.308514448442,     best is           -3.231728154048
    Train use time   1527.31285
    Train loader use time     74.29709
    Val use time     44.82853
    Total use time   1573.96626
    End lr is 0.000150050000, 0.000150050000
    time: 03:08:02
Epoch: 80, iter: 39999
    Begin lr is 0.000150050000, 0.000150050000
========> pt_241:  3.0605530366301537
========> pt_242:  2.4127278476953506
========> pt_243:  4.3608346953988075
========> pt_244:  2.709275595843792
========> pt_245:  3.3063822612166405
========> pt_246:  3.663552850484848
========> pt_247:  2.5082381069660187
========> pt_248:  2.276012748479843
========> pt_249:  3.8117363676428795
========> pt_250:  2.472885474562645
========> pt_251:  3.4469911828637123
========> pt_252:  3.3959800750017166
========> pt_253:  3.995479568839073
========> pt_254:  3.4349462017416954
========> pt_255:  3.2271860539913177
========> pt_256:  2.2198569029569626
========> pt_257:  2.995656691491604
========> pt_258:  2.498403415083885
========> pt_259:  2.658364102244377
========> pt_260:  3.973243162035942
========> pt_261:  3.861146867275238
========> pt_262:  3.3610840514302254
========> pt_263:  2.806927040219307
========> pt_264:  3.666967637836933
========> pt_265:  2.5432121008634567
========> pt_266:  3.4237807989120483
========> pt_267:  3.377230428159237
========> pt_268:  3.969803862273693
========> pt_269:  2.5562748685479164
========> pt_270:  5.264334753155708
========> pt_271:  3.3961008116602898
========> pt_272:  3.1838559359312057
========> pt_273:  2.555559054017067
========> pt_274:  4.052773155272007
========> pt_275:  3.6069821193814278
========> pt_276:  2.3364003747701645
========> pt_277:  2.338651604950428
========> pt_278:  4.339449964463711
========> pt_279:  2.6400160416960716
========> pt_280:  2.9234102740883827
========> pt_281:  2.722961865365505
========> pt_282:  2.7074361220002174
========> pt_283:  5.574731752276421
========> pt_284:  2.804436944425106
========> pt_285:  2.6655175536870956
========> pt_286:  3.1299155950546265
========> pt_287:  4.2990511655807495
========> pt_288:  2.2829708829522133
========> pt_289:  3.804658018052578
========> pt_290:  3.790784515440464
========> pt_291:  2.6103252544999123
========> pt_292:  2.993171811103821
========> pt_293:  2.8518225625157356
========> pt_294:  2.5421372056007385
========> pt_295:  3.321453742682934
========> pt_296:  1.9873257912695408
========> pt_297:  4.236690029501915
========> pt_298:  3.4873031452298164
========> pt_299:  2.551938258111477
========> pt_300:  4.793585911393166
========> pt_301:  3.2789145410060883
========> pt_302:  4.906136989593506
========> pt_303:  3.0647287517786026
========> pt_304:  2.729090228676796
========> pt_305:  3.330603390932083
========> pt_306:  2.4220363050699234
========> pt_307:  2.6407490670681
========> pt_308:  3.428639993071556
========> pt_309:  2.272786758840084
========> pt_310:  3.898942917585373
========> pt_311:  2.3297718539834023
========> pt_312:  2.2862500697374344
========> pt_313:  2.354222983121872
========> pt_314:  3.342531807720661
========> pt_315:  2.933186814188957
========> pt_316:  4.1892144456505775
========> pt_317:  4.537306576967239
========> pt_318:  5.331408008933067
========> pt_319:  2.1959153190255165
========> pt_320:  2.3420536145567894
========> pt_321:  3.999391905963421
========> pt_322:  3.015863262116909
========> pt_323:  6.981918811798096
========> pt_324:  2.845945581793785
========> pt_325:  3.3181552588939667
========> pt_326:  2.7559422701597214
========> pt_327:  2.591489814221859
========> pt_328:  2.605036050081253
========> pt_329:  6.414577513933182
========> pt_330:  7.161259949207306
========> pt_331:  3.454742319881916
========> pt_332:  2.272619865834713
========> pt_333:  2.311987057328224
========> pt_334:  1.955456268042326
========> pt_335:  5.058218240737915
========> pt_336:  3.9706461504101753
========> pt_337:  2.656227871775627
========> pt_338:  2.388196401298046
========> pt_339:  2.6258405670523643
========> pt_340:  3.2421858236193657
===============================================> mean Dose score: 3.292306776344776
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.042998532809,     best is           0.042998532809
            Average val evaluation index is   -3.292306776345,     best is           -3.231728154048
    Train use time   1523.91189
    Train loader use time     71.42121
    Val use time     45.46310
    Total use time   1572.63730
    End lr is 0.000147105928, 0.000147105928
    time: 03:34:14
Epoch: 81, iter: 40499
    Begin lr is 0.000147105928, 0.000147105928
========> pt_241:  3.0241453275084496
========> pt_242:  2.602105773985386
========> pt_243:  4.430304691195488
========> pt_244:  2.768206037580967
========> pt_245:  3.332553170621395
========> pt_246:  3.742755316197872
========> pt_247:  2.5419025123119354
========> pt_248:  2.5785965472459793
========> pt_249:  3.666594475507736
========> pt_250:  2.4391331896185875
========> pt_251:  3.7686628475785255
========> pt_252:  3.4297873824834824
========> pt_253:  3.7776166573166847
========> pt_254:  3.7075938284397125
========> pt_255:  3.518443815410137
========> pt_256:  2.4128668382763863
========> pt_257:  3.419366739690304
========> pt_258:  2.477221302688122
========> pt_259:  3.1259234622120857
========> pt_260:  4.332757033407688
========> pt_261:  3.847208432853222
========> pt_262:  3.413168489933014
========> pt_263:  3.0295153707265854
========> pt_264:  3.8063399866223335
========> pt_265:  2.579258903861046
========> pt_266:  3.2357048988342285
========> pt_267:  3.1938734278082848
========> pt_268:  3.898025266826153
========> pt_269:  2.801491804420948
========> pt_270:  4.850671663880348
========> pt_271:  3.9185872673988342
========> pt_272:  3.8187748193740845
========> pt_273:  2.660239301621914
========> pt_274:  4.260328598320484
========> pt_275:  3.4989603608846664
========> pt_276:  2.3871616646647453
========> pt_277:  2.3847054690122604
========> pt_278:  4.440619200468063
========> pt_279:  2.6835665106773376
========> pt_280:  2.8280363976955414
========> pt_281:  2.790173329412937
========> pt_282:  2.693316712975502
========> pt_283:  5.727130100131035
========> pt_284:  3.008977882564068
========> pt_285:  2.8902319446206093
========> pt_286:  3.0684898421168327
========> pt_287:  4.129749946296215
========> pt_288:  2.3343494161963463
========> pt_289:  3.7690886855125427
========> pt_290:  3.9547430723905563
========> pt_291:  2.5919727608561516
========> pt_292:  2.818724289536476
========> pt_293:  2.9841767996549606
========> pt_294:  2.761988751590252
========> pt_295:  3.217848651111126
========> pt_296:  2.102511301636696
========> pt_297:  4.318266548216343
========> pt_298:  3.2601213455200195
========> pt_299:  2.530270852148533
========> pt_300:  4.689158350229263
========> pt_301:  3.562210202217102
========> pt_302:  5.0680336356163025
========> pt_303:  3.060843273997307
========> pt_304:  2.776603102684021
========> pt_305:  3.343505524098873
========> pt_306:  2.511272169649601
========> pt_307:  2.7958010137081146
========> pt_308:  3.3950650319457054
========> pt_309:  2.391548864543438
========> pt_310:  3.7674500048160553
========> pt_311:  2.3294028639793396
========> pt_312:  2.3562009260058403
========> pt_313:  2.3187848180532455
========> pt_314:  3.2430946081876755
========> pt_315:  2.933998852968216
========> pt_316:  4.1628701239824295
========> pt_317:  4.350865967571735
========> pt_318:  4.917151927947998
========> pt_319:  2.3353518173098564
========> pt_320:  2.3097725957632065
========> pt_321:  3.31766813993454
========> pt_322:  2.9539579525589943
========> pt_323:  7.229360640048981
========> pt_324:  2.8118981048464775
========> pt_325:  3.481178432703018
========> pt_326:  2.7179892361164093
========> pt_327:  2.71848913282156
========> pt_328:  2.416704073548317
========> pt_329:  6.208580955862999
========> pt_330:  7.460366114974022
========> pt_331:  3.4779787808656693
========> pt_332:  2.3459216207265854
========> pt_333:  2.3412366211414337
========> pt_334:  1.9682804308831692
========> pt_335:  4.738970696926117
========> pt_336:  3.8400255143642426
========> pt_337:  2.8165390342473984
========> pt_338:  2.4231915175914764
========> pt_339:  2.8466319292783737
========> pt_340:  3.2804687321186066
===============================================> mean Dose score: 3.3359933035448193
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.042778926633,     best is           0.042778926633
            Average val evaluation index is   -3.335993303545,     best is           -3.231728154048
    Train use time   1527.56588
    Train loader use time     73.81538
    Val use time     44.98625
    Total use time   1575.95441
    End lr is 0.000144162991, 0.000144162991
    time: 04:00:30
Epoch: 82, iter: 40999
    Begin lr is 0.000144162991, 0.000144162991
========> pt_241:  2.869580239057541
========> pt_242:  2.387213818728924
========> pt_243:  4.750313684344292
========> pt_244:  2.903484031558037
========> pt_245:  3.925594948232174
========> pt_246:  3.8456474617123604
========> pt_247:  2.4361536279320717
========> pt_248:  2.623772658407688
========> pt_249:  4.2867618426680565
========> pt_250:  2.5183719024062157
========> pt_251:  4.064273908734322
========> pt_252:  3.9155422523617744
========> pt_253:  4.06967680901289
========> pt_254:  4.193993583321571
========> pt_255:  3.4012729302048683
========> pt_256:  2.2842927277088165
========> pt_257:  3.0462156236171722
========> pt_258:  2.378600053489208
========> pt_259:  2.7667853608727455
========> pt_260:  4.015410244464874
========> pt_261:  4.603001922369003
========> pt_262:  3.5510359331965446
========> pt_263:  2.904449664056301
========> pt_264:  4.355436488986015
========> pt_265:  2.655137851834297
========> pt_266:  3.710889182984829
========> pt_267:  3.59612625092268
========> pt_268:  4.338419139385223
========> pt_269:  2.77924757450819
========> pt_270:  5.725969150662422
========> pt_271:  3.359360098838806
========> pt_272:  3.1803378835320473
========> pt_273:  2.8386588767170906
========> pt_274:  4.319477826356888
========> pt_275:  3.531436435878277
========> pt_276:  2.3959574475884438
========> pt_277:  2.5155365467071533
========> pt_278:  4.840334728360176
========> pt_279:  3.2167701050639153
========> pt_280:  2.8826722130179405
========> pt_281:  2.935097999870777
========> pt_282:  2.9769599810242653
========> pt_283:  6.071880981326103
========> pt_284:  2.516176216304302
========> pt_285:  2.864936701953411
========> pt_286:  3.350107967853546
========> pt_287:  4.910732805728912
========> pt_288:  2.4455510079860687
========> pt_289:  4.1641053929924965
========> pt_290:  4.253207482397556
========> pt_291:  2.3678453639149666
========> pt_292:  3.408108502626419
========> pt_293:  3.4111086651682854
========> pt_294:  2.70704235881567
========> pt_295:  3.4127817675471306
========> pt_296:  2.199087329208851
========> pt_297:  4.869774132966995
========> pt_298:  3.363821879029274
========> pt_299:  2.7642590180039406
========> pt_300:  4.476684778928757
========> pt_301:  3.8134699687361717
========> pt_302:  5.412865877151489
========> pt_303:  3.356957621872425
========> pt_304:  2.932593822479248
========> pt_305:  3.5133175924420357
========> pt_306:  2.5229405984282494
========> pt_307:  2.8790610656142235
========> pt_308:  3.9986638352274895
========> pt_309:  2.5676434114575386
========> pt_310:  3.8832291588187218
========> pt_311:  2.2007906809449196
========> pt_312:  2.568754032254219
========> pt_313:  2.7964938804507256
========> pt_314:  3.6748773232102394
========> pt_315:  3.172919489443302
========> pt_316:  4.803897812962532
========> pt_317:  4.132936559617519
========> pt_318:  5.409143641591072
========> pt_319:  2.4452266097068787
========> pt_320:  2.469698078930378
========> pt_321:  3.9928478747606277
========> pt_322:  3.318798840045929
========> pt_323:  8.344321176409721
========> pt_324:  3.1086868047714233
========> pt_325:  3.668310083448887
========> pt_326:  2.9377401247620583
========> pt_327:  2.8162675723433495
========> pt_328:  3.0289914831519127
========> pt_329:  6.708089634776115
========> pt_330:  6.2004125863313675
========> pt_331:  3.869696743786335
========> pt_332:  2.517261542379856
========> pt_333:  2.567526325583458
========> pt_334:  2.0063024386763573
========> pt_335:  5.361686050891876
========> pt_336:  3.8928215950727463
========> pt_337:  3.1345630437135696
========> pt_338:  2.644202709197998
========> pt_339:  2.6934971660375595
========> pt_340:  3.3492831513285637
===============================================> mean Dose score: 3.5216927137225866
        ==> Saving latest model successfully !
            Average train loss is             0.042791458033,     best is           0.042778926633
            Average val evaluation index is   -3.521692713723,     best is           -3.231728154048
    Train use time   1520.24567
    Train loader use time     68.67746
    Val use time     45.26195
    Total use time   1567.13729
    End lr is 0.000141222323, 0.000141222323
    time: 04:26:37
Epoch: 83, iter: 41499
    Begin lr is 0.000141222323, 0.000141222323
========> pt_241:  2.8908171132206917
========> pt_242:  2.507031261920929
========> pt_243:  4.163519963622093
========> pt_244:  2.7557524293661118
========> pt_245:  3.2499515637755394
========> pt_246:  3.5290157049894333
========> pt_247:  2.4952100217342377
========> pt_248:  2.5086863711476326
========> pt_249:  3.8799236342310905
========> pt_250:  2.3254086449742317
========> pt_251:  3.600156456232071
========> pt_252:  3.7345827743411064
========> pt_253:  3.724370487034321
========> pt_254:  3.8010896369814873
========> pt_255:  4.132299497723579
========> pt_256:  2.190021649003029
========> pt_257:  3.017175979912281
========> pt_258:  2.28204358369112
========> pt_259:  3.4316375479102135
========> pt_260:  3.711114749312401
========> pt_261:  3.8047239929437637
========> pt_262:  3.427707478404045
========> pt_263:  2.818126082420349
========> pt_264:  3.8129103556275368
========> pt_265:  2.5721795111894608
========> pt_266:  3.22522584348917
========> pt_267:  3.2246072962880135
========> pt_268:  3.865153081715107
========> pt_269:  2.609003409743309
========> pt_270:  4.928301423788071
========> pt_271:  3.422211743891239
========> pt_272:  3.8326266780495644
========> pt_273:  2.560054212808609
========> pt_274:  4.575873464345932
========> pt_275:  3.4093596786260605
========> pt_276:  2.336256690323353
========> pt_277:  2.3256349936127663
========> pt_278:  4.406617879867554
========> pt_279:  2.4066881462931633
========> pt_280:  2.860354967415333
========> pt_281:  2.740059271454811
========> pt_282:  2.7268293499946594
========> pt_283:  6.294041126966476
========> pt_284:  3.2473409920930862
========> pt_285:  2.844804711639881
========> pt_286:  3.0575906857848167
========> pt_287:  4.158305861055851
========> pt_288:  2.2246574237942696
========> pt_289:  3.715706914663315
========> pt_290:  3.9972713217139244
========> pt_291:  2.459116540849209
========> pt_292:  2.7412642911076546
========> pt_293:  3.0028855055570602
========> pt_294:  2.660222612321377
========> pt_295:  3.1619906052947044
========> pt_296:  2.039906606078148
========> pt_297:  4.349967874586582
========> pt_298:  3.143746331334114
========> pt_299:  2.4454526975750923
========> pt_300:  4.324360229074955
========> pt_301:  3.5134182497859
========> pt_302:  5.144722014665604
========> pt_303:  3.0843986570835114
========> pt_304:  2.7376359328627586
========> pt_305:  3.387601263821125
========> pt_306:  2.384566217660904
========> pt_307:  2.7432891726493835
========> pt_308:  3.4909753128886223
========> pt_309:  2.3080874979496
========> pt_310:  3.5915672034025192
========> pt_311:  2.2517170384526253
========> pt_312:  2.3609584197402
========> pt_313:  2.44800616055727
========> pt_314:  3.1624946743249893
========> pt_315:  2.9138916358351707
========> pt_316:  4.126804023981094
========> pt_317:  4.111441783607006
========> pt_318:  4.767309129238129
========> pt_319:  2.296910099685192
========> pt_320:  2.322828061878681
========> pt_321:  3.502756394445896
========> pt_322:  2.8700024262070656
========> pt_323:  6.54765747487545
========> pt_324:  2.6840919628739357
========> pt_325:  3.8228264078497887
========> pt_326:  2.8445663675665855
========> pt_327:  2.604723647236824
========> pt_328:  2.403046227991581
========> pt_329:  6.236700862646103
========> pt_330:  7.9262880980968475
========> pt_331:  3.5687878727912903
========> pt_332:  2.379486672580242
========> pt_333:  2.286486066877842
========> pt_334:  2.011820077896118
========> pt_335:  4.783920720219612
========> pt_336:  3.59391700476408
========> pt_337:  2.6247315108776093
========> pt_338:  2.3465873673558235
========> pt_339:  2.6118046045303345
========> pt_340:  3.1701834872365
===============================================> mean Dose score: 3.296579547598958
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.042299670383,     best is           0.042299670383
            Average val evaluation index is   -3.296579547599,     best is           -3.231728154048
    Train use time   1520.97263
    Train loader use time     69.11207
    Val use time     45.34970
    Total use time   1569.73912
    End lr is 0.000138285059, 0.000138285059
    time: 04:52:47
Epoch: 84, iter: 41999
    Begin lr is 0.000138285059, 0.000138285059
========> pt_241:  3.0618448927998543
========> pt_242:  2.453826032578945
========> pt_243:  4.486098065972328
========> pt_244:  2.8826049342751503
========> pt_245:  3.438769094645977
========> pt_246:  3.8339482620358467
========> pt_247:  2.590273581445217
========> pt_248:  2.281690500676632
========> pt_249:  4.022551700472832
========> pt_250:  2.5406482070684433
========> pt_251:  3.8142189010977745
========> pt_252:  3.595524914562702
========> pt_253:  3.9629429951310158
========> pt_254:  3.806695155799389
========> pt_255:  3.25756449252367
========> pt_256:  2.4431579187512398
========> pt_257:  2.979084737598896
========> pt_258:  2.543288767337799
========> pt_259:  2.6014507189393044
========> pt_260:  4.561497196555138
========> pt_261:  4.424659535288811
========> pt_262:  3.4742161259055138
========> pt_263:  3.020312786102295
========> pt_264:  4.1342174634337425
========> pt_265:  2.6630592718720436
========> pt_266:  3.5917484387755394
========> pt_267:  3.4720874577760696
========> pt_268:  4.146296866238117
========> pt_269:  2.61393666267395
========> pt_270:  5.086602047085762
========> pt_271:  3.6307482048869133
========> pt_272:  3.4621351584792137
========> pt_273:  2.8004763647913933
========> pt_274:  3.803485333919525
========> pt_275:  3.3901721984148026
========> pt_276:  2.4534280970692635
========> pt_277:  2.4433117732405663
========> pt_278:  4.5852674543857574
========> pt_279:  2.5694862753152847
========> pt_280:  2.9024041816592216
========> pt_281:  2.840268090367317
========> pt_282:  2.7274812757968903
========> pt_283:  6.7108506709337234
========> pt_284:  2.5320279225707054
========> pt_285:  2.748182788491249
========> pt_286:  3.120184689760208
========> pt_287:  4.5424385368824005
========> pt_288:  2.3001426085829735
========> pt_289:  4.134255535900593
========> pt_290:  4.259353317320347
========> pt_291:  2.458019219338894
========> pt_292:  3.034251220524311
========> pt_293:  3.3624890819191933
========> pt_294:  2.8792566433548927
========> pt_295:  3.4129927307367325
========> pt_296:  2.1922199428081512
========> pt_297:  4.7380053251981735
========> pt_298:  3.4391597285866737
========> pt_299:  2.6491492614150047
========> pt_300:  4.621890038251877
========> pt_301:  3.730018250644207
========> pt_302:  5.303829982876778
========> pt_303:  3.070004917681217
========> pt_304:  2.755950093269348
========> pt_305:  3.476703092455864
========> pt_306:  2.466934695839882
========> pt_307:  2.8468937426805496
========> pt_308:  3.744960129261017
========> pt_309:  2.3896726220846176
========> pt_310:  4.0823085233569145
========> pt_311:  2.294066660106182
========> pt_312:  2.3720935732126236
========> pt_313:  2.5742200389504433
========> pt_314:  3.274092637002468
========> pt_315:  2.9798028990626335
========> pt_316:  4.551626518368721
========> pt_317:  4.245302230119705
========> pt_318:  5.545358583331108
========> pt_319:  2.3850267380476
========> pt_320:  2.2776996716856956
========> pt_321:  4.54731285572052
========> pt_322:  3.151899054646492
========> pt_323:  8.151255697011948
========> pt_324:  2.8509651497006416
========> pt_325:  3.388717621564865
========> pt_326:  2.817007899284363
========> pt_327:  2.8401916846632957
========> pt_328:  2.7342628687620163
========> pt_329:  6.505534201860428
========> pt_330:  6.471014991402626
========> pt_331:  3.813827745616436
========> pt_332:  2.3209601640701294
========> pt_333:  2.3633019626140594
========> pt_334:  1.9383891113102436
========> pt_335:  4.685264006257057
========> pt_336:  3.8232871890068054
========> pt_337:  3.065413013100624
========> pt_338:  2.6034706458449364
========> pt_339:  2.896175943315029
========> pt_340:  3.397229164838791
===============================================> mean Dose score: 3.432603979669511
        ==> Saving latest model successfully !
            Average train loss is             0.042718097337,     best is           0.042299670383
            Average val evaluation index is   -3.432603979670,     best is           -3.231728154048
    Train use time   1520.83882
    Train loader use time     69.40605
    Val use time     45.08492
    Total use time   1567.61143
    End lr is 0.000135352330, 0.000135352330
    time: 05:18:55
Epoch: 85, iter: 42499
    Begin lr is 0.000135352330, 0.000135352330
========> pt_241:  3.2657912746071815
========> pt_242:  2.8996144607663155
========> pt_243:  4.018747061491013
========> pt_244:  2.712583988904953
========> pt_245:  3.3373552560806274
========> pt_246:  3.673619367182255
========> pt_247:  2.7463508769869804
========> pt_248:  2.4710827693343163
========> pt_249:  3.527892306447029
========> pt_250:  2.5315627083182335
========> pt_251:  3.3352555334568024
========> pt_252:  3.2552579790353775
========> pt_253:  3.7706181034445763
========> pt_254:  3.3159999921917915
========> pt_255:  3.4410059824585915
========> pt_256:  2.372829206287861
========> pt_257:  3.207421489059925
========> pt_258:  2.4060769006609917
========> pt_259:  2.933165170252323
========> pt_260:  4.6741703152656555
========> pt_261:  3.5350940003991127
========> pt_262:  3.2677384465932846
========> pt_263:  2.87472914904356
========> pt_264:  3.2217708975076675
========> pt_265:  2.4573704227805138
========> pt_266:  3.1405847519636154
========> pt_267:  3.076314516365528
========> pt_268:  3.5834309086203575
========> pt_269:  2.529047839343548
========> pt_270:  4.672113358974457
========> pt_271:  3.870631344616413
========> pt_272:  3.707016482949257
========> pt_273:  2.586763873696327
========> pt_274:  4.1463495418429375
========> pt_275:  3.4443386271595955
========> pt_276:  2.35361747443676
========> pt_277:  2.254338823258877
========> pt_278:  3.8934018090367317
========> pt_279:  2.5790954008698463
========> pt_280:  3.1917236372828484
========> pt_281:  2.6656847074627876
========> pt_282:  2.562684081494808
========> pt_283:  5.968701466917992
========> pt_284:  2.9546985402703285
========> pt_285:  2.7959590405225754
========> pt_286:  2.9262717068195343
========> pt_287:  3.939862996339798
========> pt_288:  2.2613736242055893
========> pt_289:  3.439957946538925
========> pt_290:  3.6527859047055244
========> pt_291:  2.7971262484788895
========> pt_292:  2.84535676240921
========> pt_293:  2.7578769251704216
========> pt_294:  2.779315896332264
========> pt_295:  3.118859715759754
========> pt_296:  2.0701313205063343
========> pt_297:  3.8755687698721886
========> pt_298:  3.5476962476968765
========> pt_299:  2.4411246925592422
========> pt_300:  5.03376841545105
========> pt_301:  3.6921676993370056
========> pt_302:  4.617414698004723
========> pt_303:  2.80243918299675
========> pt_304:  2.601752169430256
========> pt_305:  3.471823036670685
========> pt_306:  2.3489629849791527
========> pt_307:  2.6137491688132286
========> pt_308:  3.187476210296154
========> pt_309:  2.2737031057476997
========> pt_310:  4.332728609442711
========> pt_311:  2.536243014037609
========> pt_312:  2.1946080774068832
========> pt_313:  2.201443389058113
========> pt_314:  2.916395291686058
========> pt_315:  2.75109950453043
========> pt_316:  3.512943647801876
========> pt_317:  5.11749655008316
========> pt_318:  4.549244120717049
========> pt_319:  2.273109592497349
========> pt_320:  2.2787070274353027
========> pt_321:  3.2482432574033737
========> pt_322:  2.7978913486003876
========> pt_323:  7.172261327505112
========> pt_324:  2.704727239906788
========> pt_325:  3.3556008338928223
========> pt_326:  2.616770975291729
========> pt_327:  2.6415347680449486
========> pt_328:  2.303708642721176
========> pt_329:  5.042365491390228
========> pt_330:  7.774469703435898
========> pt_331:  3.5148172825574875
========> pt_332:  2.3789069801568985
========> pt_333:  2.105665449053049
========> pt_334:  2.0988109707832336
========> pt_335:  4.481370821595192
========> pt_336:  4.1419800743460655
========> pt_337:  2.5520381331443787
========> pt_338:  2.2926178202033043
========> pt_339:  2.7368108555674553
========> pt_340:  3.3922552317380905
===============================================> mean Dose score: 3.2434303134679796
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.042229301844,     best is           0.042229301844
            Average val evaluation index is   -3.243430313468,     best is           -3.231728154048
    Train use time   1523.04327
    Train loader use time     71.42148
    Val use time     46.57419
    Total use time   1573.51213
    End lr is 0.000132425267, 0.000132425267
    time: 05:45:08
Epoch: 86, iter: 42999
    Begin lr is 0.000132425267, 0.000132425267
========> pt_241:  2.8341631963849068
========> pt_242:  2.4312910437583923
========> pt_243:  4.2646873742341995
========> pt_244:  2.7210011333227158
========> pt_245:  3.316158801317215
========> pt_246:  3.6159635707736015
========> pt_247:  2.4861401692032814
========> pt_248:  2.411959618330002
========> pt_249:  3.7029289081692696
========> pt_250:  2.413589172065258
========> pt_251:  3.5910383611917496
========> pt_252:  3.466918207705021
========> pt_253:  3.975207805633545
========> pt_254:  3.7362762168049812
========> pt_255:  3.320605456829071
========> pt_256:  2.1602633222937584
========> pt_257:  2.956431359052658
========> pt_258:  2.466968335211277
========> pt_259:  2.6644426584243774
========> pt_260:  4.49035070836544
========> pt_261:  3.7689413502812386
========> pt_262:  3.4756725281476974
========> pt_263:  2.743244580924511
========> pt_264:  3.60271617770195
========> pt_265:  2.514209747314453
========> pt_266:  3.4431330859661102
========> pt_267:  3.370627723634243
========> pt_268:  3.868848718702793
========> pt_269:  2.4318942055106163
========> pt_270:  4.884169697761536
========> pt_271:  3.264206573367119
========> pt_272:  3.3762582764029503
========> pt_273:  2.5914303585886955
========> pt_274:  3.9522594958543777
========> pt_275:  3.3600686118006706
========> pt_276:  2.2403203323483467
========> pt_277:  2.3430195078253746
========> pt_278:  4.338290318846703
========> pt_279:  2.723820321261883
========> pt_280:  2.5992396473884583
========> pt_281:  2.8089696541428566
========> pt_282:  2.734569273889065
========> pt_283:  7.052065506577492
========> pt_284:  2.5595850870013237
========> pt_285:  2.5885694473981857
========> pt_286:  3.066163770854473
========> pt_287:  4.3488481268286705
========> pt_288:  2.274612672626972
========> pt_289:  3.502380885183811
========> pt_290:  3.886113539338112
========> pt_291:  2.4744706973433495
========> pt_292:  2.8575245663523674
========> pt_293:  2.8517531976103783
========> pt_294:  2.4883775785565376
========> pt_295:  3.068866916000843
========> pt_296:  2.0435950718820095
========> pt_297:  4.285741187632084
========> pt_298:  3.212936259806156
========> pt_299:  2.607709728181362
========> pt_300:  4.558286592364311
========> pt_301:  3.260326310992241
========> pt_302:  4.645764082670212
========> pt_303:  3.096500486135483
========> pt_304:  2.669076807796955
========> pt_305:  3.298461101949215
========> pt_306:  2.4607455730438232
========> pt_307:  2.7304548397660255
========> pt_308:  3.4946271404623985
========> pt_309:  2.290835976600647
========> pt_310:  3.7102190032601357
========> pt_311:  2.3177769407629967
========> pt_312:  2.3175443336367607
========> pt_313:  2.430117577314377
========> pt_314:  3.389132246375084
========> pt_315:  3.013690263032913
========> pt_316:  3.8863085955381393
========> pt_317:  4.305472373962402
========> pt_318:  5.286701545119286
========> pt_319:  2.266305834054947
========> pt_320:  2.298000641167164
========> pt_321:  3.6626365035772324
========> pt_322:  3.0576342344284058
========> pt_323:  7.354274317622185
========> pt_324:  2.9112495109438896
========> pt_325:  3.3972570672631264
========> pt_326:  2.6393698528409004
========> pt_327:  2.673059031367302
========> pt_328:  2.7696478366851807
========> pt_329:  6.595652252435684
========> pt_330:  6.6793011128902435
========> pt_331:  3.2601651549339294
========> pt_332:  2.3126966133713722
========> pt_333:  2.215910665690899
========> pt_334:  1.9342103973031044
========> pt_335:  4.921518266201019
========> pt_336:  3.786311261355877
========> pt_337:  2.703166790306568
========> pt_338:  2.4809153750538826
========> pt_339:  2.600436322391033
========> pt_340:  3.1698955968022346
===============================================> mean Dose score: 3.2748326627537607
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.041936308451,     best is           0.041936308451
            Average val evaluation index is   -3.274832662754,     best is           -3.231728154048
    Train use time   1543.47886
    Train loader use time     92.03999
    Val use time     46.62998
    Total use time   1593.54568
    End lr is 0.000129504999, 0.000129504999
    time: 06:11:42
Epoch: 87, iter: 43499
    Begin lr is 0.000129504999, 0.000129504999
========> pt_241:  2.8274068981409073
========> pt_242:  2.6716667786240578
========> pt_243:  4.148318357765675
========> pt_244:  2.6031162589788437
========> pt_245:  3.195284456014633
========> pt_246:  3.484586961567402
========> pt_247:  2.4780578538775444
========> pt_248:  2.357504777610302
========> pt_249:  3.732833005487919
========> pt_250:  2.3364517465233803
========> pt_251:  3.1867285817861557
========> pt_252:  3.4275708347558975
========> pt_253:  3.788192979991436
========> pt_254:  3.5101617500185966
========> pt_255:  3.8563166186213493
========> pt_256:  2.204435206949711
========> pt_257:  2.781573385000229
========> pt_258:  2.3074251413345337
========> pt_259:  3.0794085562229156
========> pt_260:  4.064469747245312
========> pt_261:  3.5291533917188644
========> pt_262:  3.4665559977293015
========> pt_263:  2.7313870936632156
========> pt_264:  3.4686069563031197
========> pt_265:  2.5161506608128548
========> pt_266:  3.2528888806700706
========> pt_267:  3.180636465549469
========> pt_268:  3.7541791424155235
========> pt_269:  2.545667514204979
========> pt_270:  4.739621058106422
========> pt_271:  3.298751600086689
========> pt_272:  3.7341569364070892
========> pt_273:  2.4987945705652237
========> pt_274:  4.104401767253876
========> pt_275:  3.1999819725751877
========> pt_276:  2.1867754496634007
========> pt_277:  2.344936430454254
========> pt_278:  4.200103171169758
========> pt_279:  2.4992866441607475
========> pt_280:  2.7389856800436974
========> pt_281:  2.7954192459583282
========> pt_282:  2.6678793504834175
========> pt_283:  6.265366822481155
========> pt_284:  2.8920409083366394
========> pt_285:  2.641603872179985
========> pt_286:  2.9580217972397804
========> pt_287:  3.997298963367939
========> pt_288:  2.219134047627449
========> pt_289:  3.690078929066658
========> pt_290:  3.761005848646164
========> pt_291:  2.454691007733345
========> pt_292:  2.8188862279057503
========> pt_293:  2.7283871918916702
========> pt_294:  2.5626611337065697
========> pt_295:  3.02754994481802
========> pt_296:  2.036538105458021
========> pt_297:  3.87639619410038
========> pt_298:  3.381403535604477
========> pt_299:  2.438153997063637
========> pt_300:  4.354028590023518
========> pt_301:  3.3181140571832657
========> pt_302:  4.875834435224533
========> pt_303:  3.11278585344553
========> pt_304:  2.5900965183973312
========> pt_305:  3.090846724808216
========> pt_306:  2.3306816816329956
========> pt_307:  2.650841400027275
========> pt_308:  3.253105841577053
========> pt_309:  2.3391590639948845
========> pt_310:  3.6747578904032707
========> pt_311:  2.3086875304579735
========> pt_312:  2.3093976080417633
========> pt_313:  2.307374030351639
========> pt_314:  3.180231489241123
========> pt_315:  2.944115959107876
========> pt_316:  3.987344577908516
========> pt_317:  4.39687967300415
========> pt_318:  5.072433352470398
========> pt_319:  2.2250571846961975
========> pt_320:  2.2929101437330246
========> pt_321:  3.858495354652405
========> pt_322:  2.8939593955874443
========> pt_323:  7.076262384653091
========> pt_324:  2.7345922216773033
========> pt_325:  3.466489240527153
========> pt_326:  2.6609084382653236
========> pt_327:  2.5734883174300194
========> pt_328:  2.2986387461423874
========> pt_329:  6.497349664568901
========> pt_330:  7.924346923828125
========> pt_331:  3.2142312452197075
========> pt_332:  2.3725465312600136
========> pt_333:  2.2225256264209747
========> pt_334:  2.0313521660864353
========> pt_335:  4.716741591691971
========> pt_336:  3.6813749372959137
========> pt_337:  2.584913708269596
========> pt_338:  2.318320646882057
========> pt_339:  2.528184689581394
========> pt_340:  3.0296194180846214
===============================================> mean Dose score: 3.225440732575953
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.041904530074,     best is           0.041904530074
            Average val evaluation index is   -3.225440732576,     best is           -3.225440732576
    Train use time   1539.34789
    Train loader use time     87.88419
    Val use time     46.63333
    Total use time   1590.91903
    End lr is 0.000126592652, 0.000126592652
    time: 06:38:13
Epoch: 88, iter: 43999
    Begin lr is 0.000126592652, 0.000126592652
========> pt_241:  2.677721343934536
========> pt_242:  2.58663322776556
========> pt_243:  4.250587001442909
========> pt_244:  2.6719051226973534
========> pt_245:  3.214529827237129
========> pt_246:  3.6033905297517776
========> pt_247:  2.4070868641138077
========> pt_248:  2.332320101559162
========> pt_249:  3.5294757038354874
========> pt_250:  2.3757214099168777
========> pt_251:  3.350984938442707
========> pt_252:  3.37034922093153
========> pt_253:  3.6181827262043953
========> pt_254:  3.4981144219636917
========> pt_255:  3.2254409790039062
========> pt_256:  2.2793560847640038
========> pt_257:  2.772630527615547
========> pt_258:  2.3030387237668037
========> pt_259:  2.7354809269309044
========> pt_260:  3.8037974759936333
========> pt_261:  3.789542205631733
========> pt_262:  3.2598621398210526
========> pt_263:  2.8083479776978493
========> pt_264:  3.56783214956522
========> pt_265:  2.5325705856084824
========> pt_266:  3.3041923120617867
========> pt_267:  3.34908340126276
========> pt_268:  3.7039047107100487
========> pt_269:  2.609182558953762
========> pt_270:  4.743167012929916
========> pt_271:  3.2513532042503357
========> pt_272:  3.566208593547344
========> pt_273:  2.5226574018597603
========> pt_274:  4.011520333588123
========> pt_275:  3.2641705870628357
========> pt_276:  2.2324886173009872
========> pt_277:  2.3085036873817444
========> pt_278:  4.254830256104469
========> pt_279:  2.5980646163225174
========> pt_280:  2.6598358899354935
========> pt_281:  2.7379582449793816
========> pt_282:  2.5787146762013435
========> pt_283:  5.804473012685776
========> pt_284:  2.717648409307003
========> pt_285:  2.6447349414229393
========> pt_286:  2.9583365470170975
========> pt_287:  4.141480438411236
========> pt_288:  2.17588959261775
========> pt_289:  3.756176643073559
========> pt_290:  3.684084080159664
========> pt_291:  2.3564914241433144
========> pt_292:  2.906443513929844
========> pt_293:  2.830919213593006
========> pt_294:  2.5286605954170227
========> pt_295:  3.1754275783896446
========> pt_296:  2.0179763436317444
========> pt_297:  4.218113794922829
========> pt_298:  3.2580456137657166
========> pt_299:  2.473350167274475
========> pt_300:  4.412834122776985
========> pt_301:  3.4515351057052612
========> pt_302:  4.976500645279884
========> pt_303:  3.0074017867445946
========> pt_304:  2.636534236371517
========> pt_305:  3.4080320969223976
========> pt_306:  2.349587269127369
========> pt_307:  2.757832333445549
========> pt_308:  3.3838310465216637
========> pt_309:  2.3035821691155434
========> pt_310:  3.7989458441734314
========> pt_311:  2.1704229339957237
========> pt_312:  2.3192980140447617
========> pt_313:  2.3928318545222282
========> pt_314:  3.2163437455892563
========> pt_315:  2.9159848392009735
========> pt_316:  4.026411101222038
========> pt_317:  4.226171337068081
========> pt_318:  5.03548376262188
========> pt_319:  2.2352467849850655
========> pt_320:  2.21108615398407
========> pt_321:  4.038445129990578
========> pt_322:  2.8254662454128265
========> pt_323:  7.284773290157318
========> pt_324:  2.6898693293333054
========> pt_325:  3.4374582022428513
========> pt_326:  2.7761318907141685
========> pt_327:  2.644650973379612
========> pt_328:  2.5620006024837494
========> pt_329:  6.857905834913254
========> pt_330:  7.048771977424622
========> pt_331:  3.34176879376173
========> pt_332:  2.274854928255081
========> pt_333:  2.2354986891150475
========> pt_334:  1.9314088113605976
========> pt_335:  4.614254683256149
========> pt_336:  3.62714983522892
========> pt_337:  2.6883448660373688
========> pt_338:  2.34054297208786
========> pt_339:  2.553159184753895
========> pt_340:  3.1893785297870636
===============================================> mean Dose score: 3.2210672218352556
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.041811990745,     best is           0.041811990745
            Average val evaluation index is   -3.221067221835,     best is           -3.221067221835
    Train use time   1543.76082
    Train loader use time     92.15309
    Val use time     46.87680
    Total use time   1595.76561
    End lr is 0.000123689348, 0.000123689348
    time: 07:04:48
Epoch: 89, iter: 44499
    Begin lr is 0.000123689348, 0.000123689348
========> pt_241:  3.0069123208522797
========> pt_242:  2.6525142416357994
========> pt_243:  4.061519913375378
========> pt_244:  2.651863358914852
========> pt_245:  3.22877649217844
========> pt_246:  3.6399077624082565
========> pt_247:  2.515469789505005
========> pt_248:  2.354602664709091
========> pt_249:  3.793210983276367
========> pt_250:  2.398133836686611
========> pt_251:  3.2280977070331573
========> pt_252:  3.302139528095722
========> pt_253:  3.4138209372758865
========> pt_254:  3.5531802475452423
========> pt_255:  3.3850843086838722
========> pt_256:  2.244507782161236
========> pt_257:  2.7744384482502937
========> pt_258:  2.5019316375255585
========> pt_259:  2.9129140079021454
========> pt_260:  4.137829914689064
========> pt_261:  3.3723686262965202
========> pt_262:  3.3114271238446236
========> pt_263:  2.812330462038517
========> pt_264:  3.4637362882494926
========> pt_265:  2.4514394626021385
========> pt_266:  3.221856951713562
========> pt_267:  3.1389906629920006
========> pt_268:  3.7321583926677704
========> pt_269:  2.671358548104763
========> pt_270:  4.734800457954407
========> pt_271:  3.1275882199406624
========> pt_272:  3.6238855123519897
========> pt_273:  2.497875615954399
========> pt_274:  4.0060339868068695
========> pt_275:  3.2405711337924004
========> pt_276:  2.184562422335148
========> pt_277:  2.2840502113103867
========> pt_278:  4.2641207203269005
========> pt_279:  2.388315051794052
========> pt_280:  2.742678187787533
========> pt_281:  2.7131081372499466
========> pt_282:  2.569783553481102
========> pt_283:  6.6656628251075745
========> pt_284:  2.782071977853775
========> pt_285:  2.6965030655264854
========> pt_286:  2.8038035333156586
========> pt_287:  4.023073241114616
========> pt_288:  2.1386029571294785
========> pt_289:  3.642856813967228
========> pt_290:  3.7304190546274185
========> pt_291:  2.3395418748259544
========> pt_292:  2.887500375509262
========> pt_293:  2.73269459605217
========> pt_294:  2.6183178648352623
========> pt_295:  2.9695525392889977
========> pt_296:  2.0305203087627888
========> pt_297:  4.041265361011028
========> pt_298:  3.2426703348755836
========> pt_299:  2.5014275684952736
========> pt_300:  4.244940020143986
========> pt_301:  3.3109313994646072
========> pt_302:  5.109628066420555
========> pt_303:  2.95549388974905
========> pt_304:  2.5449157133698463
========> pt_305:  3.1695902347564697
========> pt_306:  2.29714322835207
========> pt_307:  2.70721185952425
========> pt_308:  3.285875804722309
========> pt_309:  2.4142171069979668
========> pt_310:  3.7682675197720528
========> pt_311:  2.2830332070589066
========> pt_312:  2.268500477075577
========> pt_313:  2.4827850982546806
========> pt_314:  3.0963489785790443
========> pt_315:  2.8418848663568497
========> pt_316:  3.81279144436121
========> pt_317:  4.38943937420845
========> pt_318:  4.796806424856186
========> pt_319:  2.3456530272960663
========> pt_320:  2.1048715338110924
========> pt_321:  3.6186466366052628
========> pt_322:  2.8498584404587746
========> pt_323:  7.33246922492981
========> pt_324:  2.6851702481508255
========> pt_325:  3.35676621645689
========> pt_326:  2.730766460299492
========> pt_327:  2.5942477211356163
========> pt_328:  2.49957375228405
========> pt_329:  6.437012106180191
========> pt_330:  7.488461509346962
========> pt_331:  3.428606614470482
========> pt_332:  2.3588821664452553
========> pt_333:  2.2648197039961815
========> pt_334:  1.9624175317585468
========> pt_335:  4.396597519516945
========> pt_336:  3.814593106508255
========> pt_337:  2.5373492017388344
========> pt_338:  2.2619666159152985
========> pt_339:  2.6303479820489883
========> pt_340:  3.066084496676922
===============================================> mean Dose score: 3.2070134643465282
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.041714453042,     best is           0.041714453042
            Average val evaluation index is   -3.207013464347,     best is           -3.207013464347
    Train use time   1542.32478
    Train loader use time     89.85817
    Val use time     46.71286
    Total use time   1594.20826
    End lr is 0.000120796206, 0.000120796206
    time: 07:31:23
Epoch: 90, iter: 44999
    Begin lr is 0.000120796206, 0.000120796206
========> pt_241:  3.0511819943785667
========> pt_242:  2.617216370999813
========> pt_243:  4.40745547413826
========> pt_244:  2.761022336781025
========> pt_245:  3.372972048819065
========> pt_246:  3.8402044028043747
========> pt_247:  2.528907023370266
========> pt_248:  2.402118146419525
========> pt_249:  3.6465944349765778
========> pt_250:  2.4831314012408257
========> pt_251:  3.6701881512999535
========> pt_252:  3.2662053778767586
========> pt_253:  3.668125458061695
========> pt_254:  3.5303878784179688
========> pt_255:  3.3556222170591354
========> pt_256:  2.2350000962615013
========> pt_257:  3.1609543040394783
========> pt_258:  2.288510426878929
========> pt_259:  2.994014620780945
========> pt_260:  4.396289810538292
========> pt_261:  3.9864712581038475
========> pt_262:  3.2104771956801414
========> pt_263:  2.8951365128159523
========> pt_264:  3.8944245502352715
========> pt_265:  2.517695724964142
========> pt_266:  3.235219083726406
========> pt_267:  3.306105062365532
========> pt_268:  4.0735382959246635
========> pt_269:  2.530534230172634
========> pt_270:  5.441523492336273
========> pt_271:  4.14168905466795
========> pt_272:  3.606184422969818
========> pt_273:  2.523524984717369
========> pt_274:  4.13992702960968
========> pt_275:  3.340044841170311
========> pt_276:  2.344868890941143
========> pt_277:  2.212195210158825
========> pt_278:  4.382922202348709
========> pt_279:  2.2632263973355293
========> pt_280:  2.742527984082699
========> pt_281:  2.623993270099163
========> pt_282:  2.7589160948991776
========> pt_283:  6.6356804966926575
========> pt_284:  2.867930084466934
========> pt_285:  2.6427890732884407
========> pt_286:  2.9153642058372498
========> pt_287:  4.128992408514023
========> pt_288:  2.1225943975150585
========> pt_289:  3.8095318153500557
========> pt_290:  3.941219784319401
========> pt_291:  2.5833968073129654
========> pt_292:  2.8849150985479355
========> pt_293:  3.050040863454342
========> pt_294:  2.838483117520809
========> pt_295:  3.2419925928115845
========> pt_296:  2.050304301083088
========> pt_297:  4.4400762766599655
========> pt_298:  3.3020122721791267
========> pt_299:  2.421536147594452
========> pt_300:  4.521269723773003
========> pt_301:  3.46673384308815
========> pt_302:  4.997962564229965
========> pt_303:  3.011418953537941
========> pt_304:  2.642928324639797
========> pt_305:  3.255067616701126
========> pt_306:  2.34456118196249
========> pt_307:  2.6857517659664154
========> pt_308:  3.4820368885993958
========> pt_309:  2.3430904373526573
========> pt_310:  3.9803097769618034
========> pt_311:  2.3101694881916046
========> pt_312:  2.3127005249261856
========> pt_313:  2.33219962567091
========> pt_314:  3.1235235929489136
========> pt_315:  2.7833831310272217
========> pt_316:  4.268522262573242
========> pt_317:  4.583552107214928
========> pt_318:  4.993342235684395
========> pt_319:  2.282705418765545
========> pt_320:  2.213943935930729
========> pt_321:  4.3389784917235374
========> pt_322:  3.1100279465317726
========> pt_323:  7.124124690890312
========> pt_324:  2.7494490891695023
========> pt_325:  3.387332409620285
========> pt_326:  2.7537330240011215
========> pt_327:  2.5657186657190323
========> pt_328:  2.3432672396302223
========> pt_329:  6.7904894053936005
========> pt_330:  7.496372759342194
========> pt_331:  3.506849966943264
========> pt_332:  2.3180770874023438
========> pt_333:  2.213175445795059
========> pt_334:  1.949784904718399
========> pt_335:  4.690033495426178
========> pt_336:  3.813447281718254
========> pt_337:  2.7089449390769005
========> pt_338:  2.383199520409107
========> pt_339:  2.8408602997660637
========> pt_340:  3.1495416909456253
===============================================> mean Dose score: 3.318926892615855
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.041239326380,     best is           0.041239326380
            Average val evaluation index is   -3.318926892616,     best is           -3.207013464347
    Train use time   1539.86178
    Train loader use time     87.29705
    Val use time     45.91852
    Total use time   1589.13188
    End lr is 0.000117914342, 0.000117914342
    time: 07:57:52
Epoch: 91, iter: 45499
    Begin lr is 0.000117914342, 0.000117914342
========> pt_241:  2.9339787736535072
========> pt_242:  2.4480682238936424
========> pt_243:  4.324958696961403
========> pt_244:  2.7450720593333244
========> pt_245:  3.2300420105457306
========> pt_246:  3.6222507432103157
========> pt_247:  2.4407408386468887
========> pt_248:  2.1887827292084694
========> pt_249:  3.7824437767267227
========> pt_250:  2.4074772372841835
========> pt_251:  3.448548763990402
========> pt_252:  3.3677086606621742
========> pt_253:  3.5660122334957123
========> pt_254:  3.543871007859707
========> pt_255:  3.398951292037964
========> pt_256:  2.2473840788006783
========> pt_257:  2.9022930935025215
========> pt_258:  2.4090687185525894
========> pt_259:  2.804117761552334
========> pt_260:  4.076821655035019
========> pt_261:  3.6432917788624763
========> pt_262:  3.255036845803261
========> pt_263:  2.923917733132839
========> pt_264:  3.3397749438881874
========> pt_265:  2.491816096007824
========> pt_266:  3.646780624985695
========> pt_267:  3.3726612105965614
========> pt_268:  3.6476046591997147
========> pt_269:  2.5904037058353424
========> pt_270:  4.974055141210556
========> pt_271:  3.205176778137684
========> pt_272:  3.52373119443655
========> pt_273:  2.453237734735012
========> pt_274:  4.129606783390045
========> pt_275:  3.3183883875608444
========> pt_276:  2.2809340059757233
========> pt_277:  2.3318705335259438
========> pt_278:  4.361895248293877
========> pt_279:  2.2560082748532295
========> pt_280:  2.8254972770810127
========> pt_281:  2.7426260337233543
========> pt_282:  2.672259770333767
========> pt_283:  6.371620818972588
========> pt_284:  2.7563339471817017
========> pt_285:  2.583511285483837
========> pt_286:  2.9884693399071693
========> pt_287:  4.178436808288097
========> pt_288:  2.206469736993313
========> pt_289:  3.762039802968502
========> pt_290:  3.6161505430936813
========> pt_291:  2.322619967162609
========> pt_292:  2.9238389804959297
========> pt_293:  2.675054706633091
========> pt_294:  2.53346998244524
========> pt_295:  3.106287457048893
========> pt_296:  1.9918244704604149
========> pt_297:  4.141148738563061
========> pt_298:  3.25734231621027
========> pt_299:  2.5198934972286224
========> pt_300:  4.394597411155701
========> pt_301:  3.306172862648964
========> pt_302:  4.949363321065903
========> pt_303:  3.1078604236245155
========> pt_304:  2.635849714279175
========> pt_305:  3.2595041021704674
========> pt_306:  2.4283985793590546
========> pt_307:  2.7829763293266296
========> pt_308:  3.368072435259819
========> pt_309:  2.188136801123619
========> pt_310:  3.6741383001208305
========> pt_311:  2.3203593492507935
========> pt_312:  2.348448485136032
========> pt_313:  2.279833033680916
========> pt_314:  3.1808432564139366
========> pt_315:  2.8404754027724266
========> pt_316:  3.836151510477066
========> pt_317:  4.439330473542213
========> pt_318:  5.128476545214653
========> pt_319:  2.1653190068900585
========> pt_320:  2.2947660461068153
========> pt_321:  3.9813244342803955
========> pt_322:  2.942563332617283
========> pt_323:  7.424116432666779
========> pt_324:  2.5390272587537766
========> pt_325:  3.3826779201626778
========> pt_326:  2.7487971633672714
========> pt_327:  2.6791104674339294
========> pt_328:  2.4052640795707703
========> pt_329:  7.124627456068993
========> pt_330:  7.013167962431908
========> pt_331:  3.2523522153496742
========> pt_332:  2.278079614043236
========> pt_333:  2.2322260215878487
========> pt_334:  1.911633424460888
========> pt_335:  4.752956330776215
========> pt_336:  3.6678315699100494
========> pt_337:  2.5918616726994514
========> pt_338:  2.2382406890392303
========> pt_339:  2.57616825401783
========> pt_340:  3.2520001754164696
===============================================> mean Dose score: 3.2373277937993405
        ==> Saving latest model successfully !
            Average train loss is             0.041692302875,     best is           0.041239326380
            Average val evaluation index is   -3.237327793799,     best is           -3.207013464347
    Train use time   1538.87744
    Train loader use time     86.94664
    Val use time     45.57077
    Total use time   1586.12913
    End lr is 0.000115044868, 0.000115044868
    time: 08:24:18
Epoch: 92, iter: 45999
    Begin lr is 0.000115044868, 0.000115044868
========> pt_241:  2.6354974135756493
========> pt_242:  2.4267330393195152
========> pt_243:  4.280172437429428
========> pt_244:  2.6172390580177307
========> pt_245:  3.274911716580391
========> pt_246:  3.513961434364319
========> pt_247:  2.31438510119915
========> pt_248:  2.2857986763119698
========> pt_249:  3.7178942561149597
========> pt_250:  2.4122433364391327
========> pt_251:  3.537227623164654
========> pt_252:  3.525511473417282
========> pt_253:  3.6887800320982933
========> pt_254:  3.5974105447530746
========> pt_255:  3.3597199618816376
========> pt_256:  2.1727733872830868
========> pt_257:  2.7332651615142822
========> pt_258:  2.300735078752041
========> pt_259:  2.6892265304923058
========> pt_260:  3.9047588780522346
========> pt_261:  3.8136929273605347
========> pt_262:  3.2290294393897057
========> pt_263:  2.7582453936338425
========> pt_264:  3.68495374917984
========> pt_265:  2.4428249150514603
========> pt_266:  3.5401717200875282
========> pt_267:  3.1702061742544174
========> pt_268:  3.804880976676941
========> pt_269:  2.595708817243576
========> pt_270:  4.875177294015884
========> pt_271:  3.020136244595051
========> pt_272:  3.371938094496727
========> pt_273:  2.5296781212091446
========> pt_274:  4.049138277769089
========> pt_275:  3.274466060101986
========> pt_276:  2.195167690515518
========> pt_277:  2.2471673786640167
========> pt_278:  4.294033162295818
========> pt_279:  2.424941807985306
========> pt_280:  2.5984304770827293
========> pt_281:  2.74604968726635
========> pt_282:  2.686281129717827
========> pt_283:  5.9923674166202545
========> pt_284:  2.4815865978598595
========> pt_285:  2.546020597219467
========> pt_286:  2.9161060974001884
========> pt_287:  4.13762129843235
========> pt_288:  2.1343793906271458
========> pt_289:  3.5084688290953636
========> pt_290:  3.7918995693325996
========> pt_291:  2.433134689927101
========> pt_292:  2.9479307681322098
========> pt_293:  2.9442917183041573
========> pt_294:  2.532617263495922
========> pt_295:  3.1386782601475716
========> pt_296:  1.9639765471220016
========> pt_297:  4.311863332986832
========> pt_298:  3.1397737562656403
========> pt_299:  2.4265366792678833
========> pt_300:  4.46576789021492
========> pt_301:  3.4838437661528587
========> pt_302:  4.928124099969864
========> pt_303:  3.0694711208343506
========> pt_304:  2.6475679501891136
========> pt_305:  3.180803097784519
========> pt_306:  2.349637597799301
========> pt_307:  2.60239627212286
========> pt_308:  3.3609241992235184
========> pt_309:  2.260863296687603
========> pt_310:  3.5938244313001633
========> pt_311:  2.2438790649175644
========> pt_312:  2.3315106704831123
========> pt_313:  2.3743773996829987
========> pt_314:  3.232778012752533
========> pt_315:  2.87461519241333
========> pt_316:  3.9926616847515106
========> pt_317:  4.259855039417744
========> pt_318:  5.083473846316338
========> pt_319:  2.2556737065315247
========> pt_320:  2.3063429445028305
========> pt_321:  3.5965192317962646
========> pt_322:  2.921605221927166
========> pt_323:  7.803396433591843
========> pt_324:  2.641141004860401
========> pt_325:  3.4371666610240936
========> pt_326:  2.7884704992175102
========> pt_327:  2.5774337723851204
========> pt_328:  2.4790067970752716
========> pt_329:  6.454761698842049
========> pt_330:  6.887224242091179
========> pt_331:  3.4920277819037437
========> pt_332:  2.292301245033741
========> pt_333:  2.226104959845543
========> pt_334:  1.9322530552744865
========> pt_335:  4.791701585054398
========> pt_336:  3.705674558877945
========> pt_337:  2.619100697338581
========> pt_338:  2.2795068100094795
========> pt_339:  2.575453482568264
========> pt_340:  3.267834670841694
===============================================> mean Dose score: 3.213548931851983
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.040788436409,     best is           0.040788436409
            Average val evaluation index is   -3.213548931852,     best is           -3.207013464347
    Train use time   1539.66841
    Train loader use time     85.78314
    Val use time     46.40006
    Total use time   1589.31050
    End lr is 0.000112188888, 0.000112188888
    time: 08:50:47
Epoch: 93, iter: 46499
    Begin lr is 0.000112188888, 0.000112188888
========> pt_241:  2.9640137776732445
========> pt_242:  2.5860683992505074
========> pt_243:  4.458857476711273
========> pt_244:  2.7161069959402084
========> pt_245:  3.4414399042725563
========> pt_246:  3.779672309756279
========> pt_247:  2.5082245469093323
========> pt_248:  2.368377074599266
========> pt_249:  3.716716878116131
========> pt_250:  2.4003855884075165
========> pt_251:  3.740832395851612
========> pt_252:  3.371199853718281
========> pt_253:  3.6687424406409264
========> pt_254:  3.582647554576397
========> pt_255:  3.687203675508499
========> pt_256:  2.286154106259346
========> pt_257:  3.1651868671178818
========> pt_258:  2.278359942138195
========> pt_259:  3.291337378323078
========> pt_260:  3.9244045317173004
========> pt_261:  3.811127208173275
========> pt_262:  3.490120507776737
========> pt_263:  2.9233132675290108
========> pt_264:  3.8499045372009277
========> pt_265:  2.568322978913784
========> pt_266:  3.2646209374070168
========> pt_267:  3.1433723866939545
========> pt_268:  3.957629017531872
========> pt_269:  2.6391124725341797
========> pt_270:  5.142683312296867
========> pt_271:  3.8405512273311615
========> pt_272:  3.98749191313982
========> pt_273:  2.6325155049562454
========> pt_274:  4.2296140268445015
========> pt_275:  3.2136857137084007
========> pt_276:  2.3610559478402138
========> pt_277:  2.4338528513908386
========> pt_278:  4.392998889088631
========> pt_279:  2.402915321290493
========> pt_280:  2.8351496905088425
========> pt_281:  2.8380082547664642
========> pt_282:  2.7088437601923943
========> pt_283:  6.433581933379173
========> pt_284:  2.8980769589543343
========> pt_285:  2.7543163672089577
========> pt_286:  2.8826330974698067
========> pt_287:  4.223373532295227
========> pt_288:  2.2078805044293404
========> pt_289:  3.875655345618725
========> pt_290:  3.9556067436933517
========> pt_291:  2.529282011091709
========> pt_292:  2.910293787717819
========> pt_293:  3.0289416760206223
========> pt_294:  2.6317689195275307
========> pt_295:  3.2273266091942787
========> pt_296:  2.093422804027796
========> pt_297:  4.518984332680702
========> pt_298:  3.3574851602315903
========> pt_299:  2.5098684430122375
========> pt_300:  4.664560407400131
========> pt_301:  3.3776046335697174
========> pt_302:  5.1886023581027985
========> pt_303:  3.018469661474228
========> pt_304:  2.6531168818473816
========> pt_305:  3.2457977533340454
========> pt_306:  2.3335042595863342
========> pt_307:  2.7192633599042892
========> pt_308:  3.4867529198527336
========> pt_309:  2.3046351596713066
========> pt_310:  3.8576987013220787
========> pt_311:  2.382897287607193
========> pt_312:  2.236676588654518
========> pt_313:  2.239984981715679
========> pt_314:  3.053157329559326
========> pt_315:  2.8055340051651
========> pt_316:  4.249390065670013
========> pt_317:  4.406701326370239
========> pt_318:  5.127557590603828
========> pt_319:  2.273554466664791
========> pt_320:  2.2644567117094994
========> pt_321:  3.6600079387426376
========> pt_322:  3.058604560792446
========> pt_323:  7.157890796661377
========> pt_324:  2.7502473071217537
========> pt_325:  3.36947251111269
========> pt_326:  2.6076896488666534
========> pt_327:  2.6089324802160263
========> pt_328:  2.346651256084442
========> pt_329:  6.965671256184578
========> pt_330:  7.572262659668922
========> pt_331:  3.264170065522194
========> pt_332:  2.264816053211689
========> pt_333:  2.184784859418869
========> pt_334:  2.0081299170851707
========> pt_335:  4.759216383099556
========> pt_336:  3.748084679245949
========> pt_337:  2.702844999730587
========> pt_338:  2.3913029581308365
========> pt_339:  2.8259677067399025
========> pt_340:  3.4877420216798782
===============================================> mean Dose score: 3.3226572616025805
        ==> Saving latest model successfully !
            Average train loss is             0.040862549767,     best is           0.040788436409
            Average val evaluation index is   -3.322657261603,     best is           -3.207013464347
    Train use time   1546.32975
    Train loader use time     92.81313
    Val use time     46.01781
    Total use time   1593.97834
    End lr is 0.000109347505, 0.000109347505
    time: 09:17:21
Epoch: 94, iter: 46999
    Begin lr is 0.000109347505, 0.000109347505
========> pt_241:  2.7724996209144592
========> pt_242:  2.4112623184919357
========> pt_243:  4.1614823043346405
========> pt_244:  2.617701403796673
========> pt_245:  3.2927773520350456
========> pt_246:  3.595547340810299
========> pt_247:  2.3859331756830215
========> pt_248:  2.3095298185944557
========> pt_249:  3.535120077431202
========> pt_250:  2.422841042280197
========> pt_251:  3.4630468115210533
========> pt_252:  3.393661305308342
========> pt_253:  3.5761648043990135
========> pt_254:  3.330218493938446
========> pt_255:  3.4323038160800934
========> pt_256:  2.16883497312665
========> pt_257:  2.576048821210861
========> pt_258:  2.3372288420796394
========> pt_259:  2.6951585337519646
========> pt_260:  4.003363437950611
========> pt_261:  3.5861515253782272
========> pt_262:  3.295084647834301
========> pt_263:  2.7512338012456894
========> pt_264:  3.4815967082977295
========> pt_265:  2.5205621123313904
========> pt_266:  3.3120980858802795
========> pt_267:  3.352769911289215
========> pt_268:  3.834233544766903
========> pt_269:  2.550687864422798
========> pt_270:  4.770082160830498
========> pt_271:  3.0153046920895576
========> pt_272:  3.3314060419797897
========> pt_273:  2.4643851444125175
========> pt_274:  3.9374078437685966
========> pt_275:  3.1991203874349594
========> pt_276:  2.159078251570463
========> pt_277:  2.407861351966858
========> pt_278:  4.002903699874878
========> pt_279:  2.560700662434101
========> pt_280:  2.6732269674539566
========> pt_281:  2.7594053000211716
========> pt_282:  2.664739154279232
========> pt_283:  6.449715793132782
========> pt_284:  2.617524340748787
========> pt_285:  2.531089149415493
========> pt_286:  3.037771098315716
========> pt_287:  4.056920185685158
========> pt_288:  2.1392044238746166
========> pt_289:  3.6421091854572296
========> pt_290:  3.7900178506970406
========> pt_291:  2.2777463495731354
========> pt_292:  3.029968850314617
========> pt_293:  2.714505083858967
========> pt_294:  2.4103358015418053
========> pt_295:  3.117717020213604
========> pt_296:  1.9553647376596928
========> pt_297:  4.12213284522295
========> pt_298:  3.224150165915489
========> pt_299:  2.4688557907938957
========> pt_300:  4.427405446767807
========> pt_301:  3.4037794545292854
========> pt_302:  4.543780982494354
========> pt_303:  3.0086537450551987
========> pt_304:  2.6843250915408134
========> pt_305:  3.149954490363598
========> pt_306:  2.3131099343299866
========> pt_307:  2.5945191830396652
========> pt_308:  3.2714953646063805
========> pt_309:  2.3774323239922523
========> pt_310:  4.026564955711365
========> pt_311:  2.2775372117757797
========> pt_312:  2.264816574752331
========> pt_313:  2.3342109471559525
========> pt_314:  3.2151447236537933
========> pt_315:  2.8852149844169617
========> pt_316:  3.7709252908825874
========> pt_317:  4.523589536547661
========> pt_318:  4.967546314001083
========> pt_319:  2.259897403419018
========> pt_320:  2.2576915472745895
========> pt_321:  3.569941781461239
========> pt_322:  3.0954764410853386
========> pt_323:  7.491241320967674
========> pt_324:  2.6371486112475395
========> pt_325:  3.3239563554525375
========> pt_326:  2.786121480166912
========> pt_327:  2.6160356029868126
========> pt_328:  2.619493417441845
========> pt_329:  5.876736640930176
========> pt_330:  7.197641059756279
========> pt_331:  3.3527474850416183
========> pt_332:  2.3823757469654083
========> pt_333:  2.210203967988491
========> pt_334:  1.9413244724273682
========> pt_335:  4.720741286873817
========> pt_336:  3.8348330557346344
========> pt_337:  2.5791851058602333
========> pt_338:  2.3158936575055122
========> pt_339:  2.507307678461075
========> pt_340:  3.1621721014380455
===============================================> mean Dose score: 3.194740656018257
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.041024017978,     best is           0.040788436409
            Average val evaluation index is   -3.194740656018,     best is           -3.194740656018
    Train use time   1546.80512
    Train loader use time     92.44251
    Val use time     46.12717
    Total use time   1596.31042
    End lr is 0.000106521813, 0.000106521813
    time: 09:43:58
Epoch: 95, iter: 47499
    Begin lr is 0.000106521813, 0.000106521813
========> pt_241:  3.3796631544828415
========> pt_242:  2.7937015518546104
========> pt_243:  4.33078795671463
========> pt_244:  2.968815863132477
========> pt_245:  3.536868803203106
========> pt_246:  3.8361825421452522
========> pt_247:  3.0212927609682083
========> pt_248:  2.4030926451087
========> pt_249:  3.6610012128949165
========> pt_250:  2.862912081182003
========> pt_251:  3.697432391345501
========> pt_252:  3.3673808723688126
========> pt_253:  4.266156814992428
========> pt_254:  3.5452324897050858
========> pt_255:  3.8707155734300613
========> pt_256:  2.8551995381712914
========> pt_257:  3.51275771856308
========> pt_258:  2.3941440507769585
========> pt_259:  3.1038810685276985
========> pt_260:  5.48159085214138
========> pt_261:  3.945748582482338
========> pt_262:  3.156096674501896
========> pt_263:  3.082122914493084
========> pt_264:  3.6841007694602013
========> pt_265:  2.5329314917325974
========> pt_266:  3.333827294409275
========> pt_267:  3.264220394194126
========> pt_268:  4.002665616571903
========> pt_269:  2.6898719370365143
========> pt_270:  5.3511227667331696
========> pt_271:  4.468899220228195
========> pt_272:  3.73008344322443
========> pt_273:  2.9241738095879555
========> pt_274:  4.341720752418041
========> pt_275:  3.7873144447803497
========> pt_276:  2.502126432955265
========> pt_277:  2.3583337664604187
========> pt_278:  4.405226930975914
========> pt_279:  2.874329388141632
========> pt_280:  3.2499703392386436
========> pt_281:  2.7999084070324898
========> pt_282:  2.613631561398506
========> pt_283:  6.839583069086075
========> pt_284:  3.2383647561073303
========> pt_285:  2.6505840197205544
========> pt_286:  3.04491750895977
========> pt_287:  4.512514621019363
========> pt_288:  2.240365445613861
========> pt_289:  3.780847080051899
========> pt_290:  3.875700458884239
========> pt_291:  2.8730542212724686
========> pt_292:  2.979687377810478
========> pt_293:  3.0630261823534966
========> pt_294:  2.854786477982998
========> pt_295:  3.4330013766884804
========> pt_296:  2.1356858499348164
========> pt_297:  4.417997375130653
========> pt_298:  3.306015096604824
========> pt_299:  2.5545164942741394
========> pt_300:  4.768022075295448
========> pt_301:  3.701098822057247
========> pt_302:  4.985126927495003
========> pt_303:  2.9796573892235756
========> pt_304:  2.8410986438393593
========> pt_305:  3.571382276713848
========> pt_306:  2.556788846850395
========> pt_307:  2.8332963958382607
========> pt_308:  3.6662698164582253
========> pt_309:  2.292383909225464
========> pt_310:  4.236440472304821
========> pt_311:  2.5457460060715675
========> pt_312:  2.3309745267033577
========> pt_313:  2.384268157184124
========> pt_314:  3.135182373225689
========> pt_315:  2.8586437925696373
========> pt_316:  4.104938171803951
========> pt_317:  4.5901720225811005
========> pt_318:  5.064805820584297
========> pt_319:  2.3552006110548973
========> pt_320:  2.266578860580921
========> pt_321:  4.108116701245308
========> pt_322:  2.9971537739038467
========> pt_323:  7.262477949261665
========> pt_324:  2.899787612259388
========> pt_325:  3.487943857908249
========> pt_326:  2.851800136268139
========> pt_327:  3.1649120151996613
========> pt_328:  2.6055964455008507
========> pt_329:  5.711410865187645
========> pt_330:  7.62031689286232
========> pt_331:  4.064767807722092
========> pt_332:  2.378462366759777
========> pt_333:  2.244052477180958
========> pt_334:  2.089946735650301
========> pt_335:  4.465616121888161
========> pt_336:  4.045397266745567
========> pt_337:  2.9114805534482002
========> pt_338:  2.654831185936928
========> pt_339:  2.9115454852581024
========> pt_340:  3.451375514268875
===============================================> mean Dose score: 3.4578295197337865
        ==> Saving latest model successfully !
            Average train loss is             0.040997372039,     best is           0.040788436409
            Average val evaluation index is   -3.457829519734,     best is           -3.194740656018
    Train use time   1545.62853
    Train loader use time     90.91975
    Val use time     46.13882
    Total use time   1593.42472
    End lr is 0.000103712902, 0.000103712902
    time: 10:10:31
Epoch: 96, iter: 47999
    Begin lr is 0.000103712902, 0.000103712902
========> pt_241:  2.6375379413366318
========> pt_242:  2.2582008317112923
========> pt_243:  4.240017719566822
========> pt_244:  2.723230719566345
========> pt_245:  3.3888542652130127
========> pt_246:  3.6381496489048004
========> pt_247:  2.3853104561567307
========> pt_248:  2.2202981263399124
========> pt_249:  3.9378727972507477
========> pt_250:  2.532615177333355
========> pt_251:  3.359328806400299
========> pt_252:  3.471158593893051
========> pt_253:  3.8245104625821114
========> pt_254:  3.741692416369915
========> pt_255:  3.260730504989624
========> pt_256:  2.2074656188488007
========> pt_257:  2.7529900893568993
========> pt_258:  2.6850296929478645
========> pt_259:  2.5986583903431892
========> pt_260:  3.801710791885853
========> pt_261:  4.025222510099411
========> pt_262:  3.4437018260359764
========> pt_263:  2.725191190838814
========> pt_264:  3.988773338496685
========> pt_265:  2.6882515102624893
========> pt_266:  3.307664208114147
========> pt_267:  3.535638488829136
========> pt_268:  3.909808173775673
========> pt_269:  2.6330774649977684
========> pt_270:  4.801998883485794
========> pt_271:  3.3221489563584328
========> pt_272:  3.164059817790985
========> pt_273:  2.608846165239811
========> pt_274:  4.2591869458556175
========> pt_275:  3.5722845420241356
========> pt_276:  2.2085916250944138
========> pt_277:  2.4539626762270927
========> pt_278:  4.605868309736252
========> pt_279:  2.560504302382469
========> pt_280:  2.832213416695595
========> pt_281:  2.9556120187044144
========> pt_282:  2.7615556120872498
========> pt_283:  6.144050732254982
========> pt_284:  2.6559073850512505
========> pt_285:  2.541242502629757
========> pt_286:  3.287971615791321
========> pt_287:  4.558662623167038
========> pt_288:  2.244071513414383
========> pt_289:  3.9029141888022423
========> pt_290:  3.8209543377161026
========> pt_291:  2.2700799629092216
========> pt_292:  2.9109999537467957
========> pt_293:  3.2401419058442116
========> pt_294:  2.6006053015589714
========> pt_295:  3.2512585446238518
========> pt_296:  2.04156706109643
========> pt_297:  4.618455693125725
========> pt_298:  3.104861304163933
========> pt_299:  2.624182850122452
========> pt_300:  4.028445892035961
========> pt_301:  3.516980893909931
========> pt_302:  4.97556708753109
========> pt_303:  3.128829225897789
========> pt_304:  2.8307517990469933
========> pt_305:  3.3778875693678856
========> pt_306:  2.4088913947343826
========> pt_307:  2.7940181270241737
========> pt_308:  3.7116188183426857
========> pt_309:  2.361745163798332
========> pt_310:  3.8358310237526894
========> pt_311:  2.2217777371406555
========> pt_312:  2.483217976987362
========> pt_313:  2.397325448691845
========> pt_314:  3.368533216416836
========> pt_315:  3.0368005111813545
========> pt_316:  4.316738694906235
========> pt_317:  3.9899588003754616
========> pt_318:  5.392296835780144
========> pt_319:  2.360970415174961
========> pt_320:  2.366340719163418
========> pt_321:  4.127755053341389
========> pt_322:  3.3264125511050224
========> pt_323:  7.16635175049305
========> pt_324:  2.743084467947483
========> pt_325:  3.545314371585846
========> pt_326:  2.8491540998220444
========> pt_327:  2.6892737299203873
========> pt_328:  2.736881785094738
========> pt_329:  6.788071021437645
========> pt_330:  6.81371882557869
========> pt_331:  3.7838245555758476
========> pt_332:  2.435344196856022
========> pt_333:  2.258671000599861
========> pt_334:  1.8924450315535069
========> pt_335:  4.67658556997776
========> pt_336:  3.9443091303110123
========> pt_337:  2.8137000277638435
========> pt_338:  2.461584210395813
========> pt_339:  2.6240435987710953
========> pt_340:  3.3549968898296356
===============================================> mean Dose score: 3.3178350172936915
        ==> Saving latest model successfully !
            Average train loss is             0.041058259800,     best is           0.040788436409
            Average val evaluation index is   -3.317835017294,     best is           -3.194740656018
    Train use time   1546.03072
    Train loader use time     91.01506
    Val use time     45.83235
    Total use time   1593.57899
    End lr is 0.000100921855, 0.000100921855
    time: 10:37:05
Epoch: 97, iter: 48499
    Begin lr is 0.000100921855, 0.000100921855
========> pt_241:  3.104397915303707
========> pt_242:  2.3918724805116653
========> pt_243:  4.415707811713219
========> pt_244:  2.661459445953369
========> pt_245:  3.3170832321047783
========> pt_246:  3.7161069363355637
========> pt_247:  2.3999203741550446
========> pt_248:  2.215462401509285
========> pt_249:  3.9701540768146515
========> pt_250:  2.358032315969467
========> pt_251:  3.540889620780945
========> pt_252:  3.2674844563007355
========> pt_253:  3.5741811245679855
========> pt_254:  3.730500154197216
========> pt_255:  3.378750719130039
========> pt_256:  2.2711796313524246
========> pt_257:  2.8462911024689674
========> pt_258:  2.4039482325315475
========> pt_259:  2.696577124297619
========> pt_260:  3.7878284230828285
========> pt_261:  3.852948509156704
========> pt_262:  3.3299480751156807
========> pt_263:  2.86607027053833
========> pt_264:  3.6606742069125175
========> pt_265:  2.4788685888051987
========> pt_266:  3.4038084000349045
========> pt_267:  3.4620212018489838
========> pt_268:  3.8778481632471085
========> pt_269:  2.5523893907666206
========> pt_270:  4.991220086812973
========> pt_271:  3.442515842616558
========> pt_272:  3.5513723269104958
========> pt_273:  2.5559361279010773
========> pt_274:  4.139017201960087
========> pt_275:  3.368782512843609
========> pt_276:  2.3830680921673775
========> pt_277:  2.308824434876442
========> pt_278:  4.377428814768791
========> pt_279:  2.4785947799682617
========> pt_280:  2.786416672170162
========> pt_281:  2.768646478652954
========> pt_282:  2.708674520254135
========> pt_283:  6.438004598021507
========> pt_284:  2.8279412165284157
========> pt_285:  2.735411301255226
========> pt_286:  2.8763675689697266
========> pt_287:  4.090735577046871
========> pt_288:  2.2917943075299263
========> pt_289:  3.8639238104224205
========> pt_290:  3.9469848945736885
========> pt_291:  2.3217886313796043
========> pt_292:  2.870045192539692
========> pt_293:  2.8996939957141876
========> pt_294:  2.6248139142990112
========> pt_295:  3.1538616120815277
========> pt_296:  2.107902467250824
========> pt_297:  4.386830627918243
========> pt_298:  3.1121086329221725
========> pt_299:  2.5663763284683228
========> pt_300:  4.480542615056038
========> pt_301:  3.309497945010662
========> pt_302:  4.988455921411514
========> pt_303:  3.143203668296337
========> pt_304:  2.6186396554112434
========> pt_305:  3.2075852528214455
========> pt_306:  2.331874966621399
========> pt_307:  2.7987777069211006
========> pt_308:  3.2074209675192833
========> pt_309:  2.255997322499752
========> pt_310:  3.893951252102852
========> pt_311:  2.2315654903650284
========> pt_312:  2.321687191724777
========> pt_313:  2.3836131021380424
========> pt_314:  3.2463644072413445
========> pt_315:  2.9030608013272285
========> pt_316:  4.079159200191498
========> pt_317:  4.338308311998844
========> pt_318:  5.426888018846512
========> pt_319:  2.2635935619473457
========> pt_320:  2.3405184596776962
========> pt_321:  4.200392626225948
========> pt_322:  2.8373675420880318
========> pt_323:  7.30508416891098
========> pt_324:  2.6140832155942917
========> pt_325:  3.259805291891098
========> pt_326:  2.8159014508128166
========> pt_327:  2.663675472140312
========> pt_328:  2.591516673564911
========> pt_329:  7.66830749809742
========> pt_330:  6.82877779006958
========> pt_331:  3.5565971210598946
========> pt_332:  2.227233313024044
========> pt_333:  2.343681864440441
========> pt_334:  1.9012944027781487
========> pt_335:  4.75090928375721
========> pt_336:  3.6475060880184174
========> pt_337:  2.6757342740893364
========> pt_338:  2.3511625826358795
========> pt_339:  2.6949509605765343
========> pt_340:  3.1985706835985184
===============================================> mean Dose score: 3.2841074507683516
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.040346607700,     best is           0.040346607700
            Average val evaluation index is   -3.284107450768,     best is           -3.194740656018
    Train use time   1544.14183
    Train loader use time     90.35030
    Val use time     46.04770
    Total use time   1593.54304
    End lr is 0.000098149747, 0.000098149747
    time: 11:03:38
Epoch: 98, iter: 48999
    Begin lr is 0.000098149747, 0.000098149747
========> pt_241:  2.9128167405724525
========> pt_242:  2.4439477920532227
========> pt_243:  4.061901941895485
========> pt_244:  2.576151303946972
========> pt_245:  3.1856077909469604
========> pt_246:  3.5111427679657936
========> pt_247:  2.466842383146286
========> pt_248:  2.1840153262019157
========> pt_249:  3.6358556523919106
========> pt_250:  2.422664761543274
========> pt_251:  3.413916639983654
========> pt_252:  3.4059149026870728
========> pt_253:  3.616963103413582
========> pt_254:  3.616759181022644
========> pt_255:  3.6814070120453835
========> pt_256:  2.2707590088248253
========> pt_257:  2.8866450488567352
========> pt_258:  2.4061553925275803
========> pt_259:  3.0006733909249306
========> pt_260:  4.076540805399418
========> pt_261:  3.4608884155750275
========> pt_262:  3.3144139871001244
========> pt_263:  2.7978749200701714
========> pt_264:  3.6448123306035995
========> pt_265:  2.471645511686802
========> pt_266:  3.290818966925144
========> pt_267:  3.2780127972364426
========> pt_268:  3.766983486711979
========> pt_269:  2.6001153141260147
========> pt_270:  4.919745549559593
========> pt_271:  3.4869492799043655
========> pt_272:  3.3374353125691414
========> pt_273:  2.550233341753483
========> pt_274:  4.37855951488018
========> pt_275:  3.199058584868908
========> pt_276:  2.228296212852001
========> pt_277:  2.265738397836685
========> pt_278:  4.131316132843494
========> pt_279:  2.381814308464527
========> pt_280:  2.8367672488093376
========> pt_281:  2.815834954380989
========> pt_282:  2.592511773109436
========> pt_283:  6.212234869599342
========> pt_284:  2.894826717674732
========> pt_285:  2.5818832963705063
========> pt_286:  2.8737403079867363
========> pt_287:  4.330839328467846
========> pt_288:  2.1338673681020737
========> pt_289:  3.8053373247385025
========> pt_290:  3.795177713036537
========> pt_291:  2.3368311673402786
========> pt_292:  2.6728175580501556
========> pt_293:  2.832871600985527
========> pt_294:  2.575082406401634
========> pt_295:  3.090076930820942
========> pt_296:  2.015782482922077
========> pt_297:  4.220556430518627
========> pt_298:  3.1586550921201706
========> pt_299:  2.39471722394228
========> pt_300:  4.28101159632206
========> pt_301:  3.330845385789871
========> pt_302:  5.082868859171867
========> pt_303:  2.989549972116947
========> pt_304:  2.5812144204974174
========> pt_305:  3.276018165051937
========> pt_306:  2.3014608025550842
========> pt_307:  2.6980729028582573
========> pt_308:  3.406570740044117
========> pt_309:  2.26540956646204
========> pt_310:  3.80270354449749
========> pt_311:  2.339829243719578
========> pt_312:  2.2925088182091713
========> pt_313:  2.2374004870653152
========> pt_314:  3.055790849030018
========> pt_315:  2.854919731616974
========> pt_316:  3.8537978380918503
========> pt_317:  3.9993001148104668
========> pt_318:  4.861010164022446
========> pt_319:  2.247248999774456
========> pt_320:  2.1753765270113945
========> pt_321:  4.096779190003872
========> pt_322:  2.8584542125463486
========> pt_323:  7.2268760204315186
========> pt_324:  2.6339174062013626
========> pt_325:  3.6546556279063225
========> pt_326:  2.642541341483593
========> pt_327:  2.6893331855535507
========> pt_328:  2.259284071624279
========> pt_329:  6.463224217295647
========> pt_330:  7.152157500386238
========> pt_331:  3.4205952286720276
========> pt_332:  2.29787390679121
========> pt_333:  2.1242686733603477
========> pt_334:  1.8826165981590748
========> pt_335:  4.496755227446556
========> pt_336:  3.599495403468609
========> pt_337:  2.637311853468418
========> pt_338:  2.248319201171398
========> pt_339:  2.612947039306164
========> pt_340:  3.3016683161258698
===============================================> mean Dose score: 3.216837880574167
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.040295095965,     best is           0.040295095965
            Average val evaluation index is   -3.216837880574,     best is           -3.194740656018
    Train use time   1544.38984
    Train loader use time     90.74453
    Val use time     45.69010
    Total use time   1593.44457
    End lr is 0.000095397649, 0.000095397649
    time: 11:30:12
Epoch: 99, iter: 49499
    Begin lr is 0.000095397649, 0.000095397649
========> pt_241:  2.917911149561405
========> pt_242:  2.5919435545802116
========> pt_243:  4.168576039373875
========> pt_244:  2.683258540928364
========> pt_245:  3.1770696491003036
========> pt_246:  3.8523370027542114
========> pt_247:  2.4530455470085144
========> pt_248:  2.289675548672676
========> pt_249:  3.336089216172695
========> pt_250:  2.4297358095645905
========> pt_251:  3.432728871703148
========> pt_252:  3.1342467293143272
========> pt_253:  3.708214983344078
========> pt_254:  3.325299583375454
========> pt_255:  3.757930062711239
========> pt_256:  2.1528674848377705
========> pt_257:  3.022008053958416
========> pt_258:  2.3442310467362404
========> pt_259:  3.1849102303385735
========> pt_260:  3.895966485142708
========> pt_261:  3.6379793658852577
========> pt_262:  3.2542790472507477
========> pt_263:  2.717444747686386
========> pt_264:  3.4635284543037415
========> pt_265:  2.4393154680728912
========> pt_266:  3.4344103187322617
========> pt_267:  3.1989863514900208
========> pt_268:  3.7647416442632675
========> pt_269:  2.544277347624302
========> pt_270:  4.7686416655778885
========> pt_271:  3.4230561181902885
========> pt_272:  4.0111250057816505
========> pt_273:  2.5335758551955223
========> pt_274:  4.330477640032768
========> pt_275:  3.4814994409680367
========> pt_276:  2.3093783110380173
========> pt_277:  2.2510020062327385
========> pt_278:  3.8303566724061966
========> pt_279:  2.5384822487831116
========> pt_280:  2.9528507217764854
========> pt_281:  2.6447829231619835
========> pt_282:  2.4438541755080223
========> pt_283:  6.3049668818712234
========> pt_284:  3.393137939274311
========> pt_285:  2.670805975794792
========> pt_286:  2.8573182970285416
========> pt_287:  4.07343503087759
========> pt_288:  2.079120073467493
========> pt_289:  3.4240707755088806
========> pt_290:  3.7948108091950417
========> pt_291:  2.346252016723156
========> pt_292:  2.7418572828173637
========> pt_293:  2.5647778064012527
========> pt_294:  2.54679586738348
========> pt_295:  3.1560372188687325
========> pt_296:  1.9304807297885418
========> pt_297:  3.9755629748106003
========> pt_298:  3.3389175310730934
========> pt_299:  2.3996666446328163
========> pt_300:  4.68304380774498
========> pt_301:  3.3047477528452873
========> pt_302:  4.758763164281845
========> pt_303:  2.886226773262024
========> pt_304:  2.5875287130475044
========> pt_305:  3.5926105454564095
========> pt_306:  2.350132018327713
========> pt_307:  2.7058396860957146
========> pt_308:  3.0934468656778336
========> pt_309:  2.2148853167891502
========> pt_310:  4.184424355626106
========> pt_311:  2.4291031807661057
========> pt_312:  2.2724229842424393
========> pt_313:  2.2927461192011833
========> pt_314:  2.9872189462184906
========> pt_315:  2.758820652961731
========> pt_316:  3.6300399526953697
========> pt_317:  4.631831645965576
========> pt_318:  4.96466688811779
========> pt_319:  2.1588959731161594
========> pt_320:  2.1653291769325733
========> pt_321:  3.5817544162273407
========> pt_322:  2.7731799706816673
========> pt_323:  6.815789341926575
========> pt_324:  2.70360566675663
========> pt_325:  3.4114953875541687
========> pt_326:  2.7044646441936493
========> pt_327:  2.6291896402835846
========> pt_328:  2.3157090321183205
========> pt_329:  5.939413830637932
========> pt_330:  7.960333749651909
========> pt_331:  3.5765108466148376
========> pt_332:  2.31980312615633
========> pt_333:  2.2046198323369026
========> pt_334:  1.9046369567513466
========> pt_335:  4.1150083392858505
========> pt_336:  3.9761877804994583
========> pt_337:  2.5105800852179527
========> pt_338:  2.3399056494235992
========> pt_339:  2.5898902490735054
========> pt_340:  3.15126433968544
===============================================> mean Dose score: 3.216061723791063
        ==> Saving latest model successfully !
            Average train loss is             0.040499771729,     best is           0.040295095965
            Average val evaluation index is   -3.216061723791,     best is           -3.194740656018
    Train use time   1549.26914
    Train loader use time     94.90066
    Val use time     45.83994
    Total use time   1596.62423
    End lr is 0.000092666619, 0.000092666619
    time: 11:56:48
Epoch: 100, iter: 49999
    Begin lr is 0.000092666619, 0.000092666619
========> pt_241:  2.8090817853808403
========> pt_242:  2.547112964093685
========> pt_243:  4.2294445261359215
========> pt_244:  2.680240646004677
========> pt_245:  3.0918066203594208
========> pt_246:  3.6055077239871025
========> pt_247:  2.4794284626841545
========> pt_248:  2.0583222061395645
========> pt_249:  3.4591183066368103
========> pt_250:  2.455359101295471
========> pt_251:  3.256415016949177
========> pt_252:  3.164527639746666
========> pt_253:  3.7362313643097878
========> pt_254:  3.394327834248543
========> pt_255:  3.790295571088791
========> pt_256:  2.098039221018553
========> pt_257:  2.8440818563103676
========> pt_258:  2.353731170296669
========> pt_259:  3.1129592657089233
========> pt_260:  4.140528626739979
========> pt_261:  3.5925012826919556
========> pt_262:  3.191472515463829
========> pt_263:  2.6553235203027725
========> pt_264:  3.4386759996414185
========> pt_265:  2.3871608823537827
========> pt_266:  3.2884084060788155
========> pt_267:  3.050999976694584
========> pt_268:  3.6773063987493515
========> pt_269:  2.401028387248516
========> pt_270:  5.009826570749283
========> pt_271:  3.2962828874588013
========> pt_272:  3.7027719244360924
========> pt_273:  2.470494732260704
========> pt_274:  4.099887050688267
========> pt_275:  3.1837378069758415
========> pt_276:  2.1948305144906044
========> pt_277:  2.200254015624523
========> pt_278:  3.949657529592514
========> pt_279:  2.4308787658810616
========> pt_280:  2.68226969987154
========> pt_281:  2.685590870678425
========> pt_282:  2.538638710975647
========> pt_283:  7.061207592487335
========> pt_284:  2.9545071348547935
========> pt_285:  2.369983419775963
========> pt_286:  2.907605767250061
========> pt_287:  4.059528931975365
========> pt_288:  2.1322260797023773
========> pt_289:  3.427668623626232
========> pt_290:  3.8941721245646477
========> pt_291:  2.4895862489938736
========> pt_292:  2.729695737361908
========> pt_293:  2.702481225132942
========> pt_294:  2.574576251208782
========> pt_295:  3.0432571843266487
========> pt_296:  1.956411860883236
========> pt_297:  4.003172032535076
========> pt_298:  3.2777337729930878
========> pt_299:  2.4008476734161377
========> pt_300:  4.634255766868591
========> pt_301:  3.344222642481327
========> pt_302:  4.815544858574867
========> pt_303:  2.869534343481064
========> pt_304:  2.5600727275013924
========> pt_305:  3.216848336160183
========> pt_306:  2.351371720433235
========> pt_307:  2.584093324840069
========> pt_308:  3.287259452044964
========> pt_309:  2.178121656179428
========> pt_310:  3.9150919020175934
========> pt_311:  2.4901414290070534
========> pt_312:  2.2176163643598557
========> pt_313:  2.2444937005639076
========> pt_314:  3.0052225291728973
========> pt_315:  2.7400120720267296
========> pt_316:  3.863915205001831
========> pt_317:  4.601310044527054
========> pt_318:  5.198910087347031
========> pt_319:  2.151766512542963
========> pt_320:  2.102244794368744
========> pt_321:  3.5938760638237
========> pt_322:  2.8271476924419403
========> pt_323:  6.977009549736977
========> pt_324:  2.6682157441973686
========> pt_325:  3.341734893620014
========> pt_326:  2.627921514213085
========> pt_327:  2.5569314882159233
========> pt_328:  2.3345551639795303
========> pt_329:  6.547249108552933
========> pt_330:  7.770216539502144
========> pt_331:  3.28073862940073
========> pt_332:  2.286699377000332
========> pt_333:  2.1335419267416
========> pt_334:  1.8867679312825203
========> pt_335:  4.672521725296974
========> pt_336:  3.7954405695199966
========> pt_337:  2.5664613395929337
========> pt_338:  2.3594318702816963
========> pt_339:  2.5872838497161865
========> pt_340:  3.246770426630974
===============================================> mean Dose score: 3.198517848923802
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.040036107443,     best is           0.040036107443
            Average val evaluation index is   -3.198517848924,     best is           -3.194740656018
    Train use time   1545.92944
    Train loader use time     91.11787
    Val use time     45.89707
    Total use time   1595.04906
    End lr is 0.000089957712, 0.000089957712
    time: 12:23:23
Epoch: 101, iter: 50499
    Begin lr is 0.000089957712, 0.000089957712
========> pt_241:  2.7932506799697876
========> pt_242:  2.291124388575554
========> pt_243:  4.462376311421394
========> pt_244:  2.7473272010684013
========> pt_245:  3.302662894129753
========> pt_246:  3.6045366153120995
========> pt_247:  2.393243871629238
========> pt_248:  2.122441064566374
========> pt_249:  3.8600779697299004
========> pt_250:  2.390204593539238
========> pt_251:  3.386048637330532
========> pt_252:  3.4529808163642883
========> pt_253:  3.7736323475837708
========> pt_254:  3.864375986158848
========> pt_255:  3.420030139386654
========> pt_256:  2.2117406874895096
========> pt_257:  2.763046696782112
========> pt_258:  2.633548676967621
========> pt_259:  2.7312645316123962
========> pt_260:  3.697156235575676
========> pt_261:  4.083950072526932
========> pt_262:  3.339318595826626
========> pt_263:  2.8181951865553856
========> pt_264:  3.712017275393009
========> pt_265:  2.6014262065291405
========> pt_266:  3.5993976145982742
========> pt_267:  3.4042318910360336
========> pt_268:  3.9948638901114464
========> pt_269:  2.5708667933940887
========> pt_270:  5.313297510147095
========> pt_271:  3.385809510946274
========> pt_272:  3.167431838810444
========> pt_273:  2.5920410826802254
========> pt_274:  4.123639836907387
========> pt_275:  3.4407660737633705
========> pt_276:  2.2706690430641174
========> pt_277:  2.330966703593731
========> pt_278:  4.6925238519907
========> pt_279:  2.471568062901497
========> pt_280:  2.897205203771591
========> pt_281:  2.756093256175518
========> pt_282:  2.666010670363903
========> pt_283:  6.055071726441383
========> pt_284:  2.6275042816996574
========> pt_285:  2.542855888605118
========> pt_286:  3.07724442332983
========> pt_287:  4.586714208126068
========> pt_288:  2.24346574395895
========> pt_289:  3.9820173010230064
========> pt_290:  4.018553048372269
========> pt_291:  2.305237017571926
========> pt_292:  2.9629459232091904
========> pt_293:  2.9441866278648376
========> pt_294:  2.514631152153015
========> pt_295:  3.151826038956642
========> pt_296:  2.0558768324553967
========> pt_297:  4.48454387485981
========> pt_298:  3.1199833750724792
========> pt_299:  2.595754712820053
========> pt_300:  4.407272934913635
========> pt_301:  3.491200879216194
========> pt_302:  5.032928213477135
========> pt_303:  3.1719502061605453
========> pt_304:  2.706148438155651
========> pt_305:  3.404500223696232
========> pt_306:  2.3730117455124855
========> pt_307:  2.7270888164639473
========> pt_308:  3.54215357452631
========> pt_309:  2.1885790675878525
========> pt_310:  3.751268684864044
========> pt_311:  2.3043741285800934
========> pt_312:  2.3762014880776405
========> pt_313:  2.3372528329491615
========> pt_314:  3.2378455623984337
========> pt_315:  2.9194264858961105
========> pt_316:  4.395379200577736
========> pt_317:  4.092463441193104
========> pt_318:  5.6702665239572525
========> pt_319:  2.2436626255512238
========> pt_320:  2.279279939830303
========> pt_321:  3.9939243346452713
========> pt_322:  3.276982493698597
========> pt_323:  7.694910243153572
========> pt_324:  2.642510049045086
========> pt_325:  3.307512439787388
========> pt_326:  2.895975410938263
========> pt_327:  2.704901434481144
========> pt_328:  2.4621571227908134
========> pt_329:  6.825732514262199
========> pt_330:  6.856812685728073
========> pt_331:  3.5883667692542076
========> pt_332:  2.2131728380918503
========> pt_333:  2.2275300696492195
========> pt_334:  1.8755428120493889
========> pt_335:  4.968115836381912
========> pt_336:  3.7073956429958344
========> pt_337:  2.839472219347954
========> pt_338:  2.433180846273899
========> pt_339:  2.6293249800801277
========> pt_340:  3.2483285292983055
===============================================> mean Dose score: 3.304458769783378
        ==> Saving latest model successfully !
            Average train loss is             0.040452697992,     best is           0.040036107443
            Average val evaluation index is   -3.304458769783,     best is           -3.194740656018
    Train use time   1549.56412
    Train loader use time     94.65835
    Val use time     45.92629
    Total use time   1597.24733
    End lr is 0.000087271972, 0.000087271972
    time: 12:50:00
Epoch: 102, iter: 50999
    Begin lr is 0.000087271972, 0.000087271972
========> pt_241:  2.9783063381910324
========> pt_242:  2.4565036222338676
========> pt_243:  4.459829106926918
========> pt_244:  2.5174644216895103
========> pt_245:  3.2661978155374527
========> pt_246:  3.643059954047203
========> pt_247:  2.3714880645275116
========> pt_248:  2.1542000211775303
========> pt_249:  3.8270631432533264
========> pt_250:  2.4377448484301567
========> pt_251:  3.512853682041168
========> pt_252:  3.3992167562246323
========> pt_253:  3.617265075445175
========> pt_254:  3.539975881576538
========> pt_255:  3.426547832787037
========> pt_256:  2.1737954765558243
========> pt_257:  2.7030259743332863
========> pt_258:  2.2882533073425293
========> pt_259:  2.70323745906353
========> pt_260:  3.9637206122279167
========> pt_261:  3.847954235970974
========> pt_262:  3.3842941746115685
========> pt_263:  2.750529982149601
========> pt_264:  3.704267181456089
========> pt_265:  2.5343910232186317
========> pt_266:  3.3303486183285713
========> pt_267:  3.3784789964556694
========> pt_268:  4.024638645350933
========> pt_269:  2.493910863995552
========> pt_270:  4.915311932563782
========> pt_271:  3.3492515981197357
========> pt_272:  3.3466950058937073
========> pt_273:  2.4919089302420616
========> pt_274:  4.177969768643379
========> pt_275:  3.3695124089717865
========> pt_276:  2.192910984158516
========> pt_277:  2.363651394844055
========> pt_278:  4.369480796158314
========> pt_279:  2.3828354850411415
========> pt_280:  2.7834339812397957
========> pt_281:  2.9091935977339745
========> pt_282:  2.593003585934639
========> pt_283:  6.230077818036079
========> pt_284:  2.7933987975120544
========> pt_285:  2.5384720787405968
========> pt_286:  3.0224138125777245
========> pt_287:  4.164021164178848
========> pt_288:  2.1739291213452816
========> pt_289:  3.7626974657177925
========> pt_290:  3.842991776764393
========> pt_291:  2.2425199300050735
========> pt_292:  2.8401225805282593
========> pt_293:  3.023218810558319
========> pt_294:  2.526799477636814
========> pt_295:  3.1938134506344795
========> pt_296:  2.003545183688402
========> pt_297:  4.534320756793022
========> pt_298:  3.1639382988214493
========> pt_299:  2.545454725623131
========> pt_300:  4.376335144042969
========> pt_301:  3.4146439284086227
========> pt_302:  4.913317561149597
========> pt_303:  3.0649785697460175
========> pt_304:  2.696807384490967
========> pt_305:  3.2097402587532997
========> pt_306:  2.379400357604027
========> pt_307:  2.683980353176594
========> pt_308:  3.410400152206421
========> pt_309:  2.2740165516734123
========> pt_310:  3.921065367758274
========> pt_311:  2.302623838186264
========> pt_312:  2.361537851393223
========> pt_313:  2.3540211468935013
========> pt_314:  3.150782696902752
========> pt_315:  2.8829269856214523
========> pt_316:  4.1081250458955765
========> pt_317:  4.181565530598164
========> pt_318:  5.053712651133537
========> pt_319:  2.2745826840400696
========> pt_320:  2.2808443009853363
========> pt_321:  4.090231508016586
========> pt_322:  2.985450401902199
========> pt_323:  7.294018641114235
========> pt_324:  2.583733983337879
========> pt_325:  3.2697581127285957
========> pt_326:  2.7918625995516777
========> pt_327:  2.635205090045929
========> pt_328:  2.5405631959438324
========> pt_329:  6.959934309124947
========> pt_330:  6.936260536313057
========> pt_331:  3.4779083728790283
========> pt_332:  2.276797667145729
========> pt_333:  2.183057777583599
========> pt_334:  1.9153058528900146
========> pt_335:  4.83636736869812
========> pt_336:  3.6780649796128273
========> pt_337:  2.648528888821602
========> pt_338:  2.3120522499084473
========> pt_339:  2.629406340420246
========> pt_340:  3.1403080746531487
===============================================> mean Dose score: 3.2521568214520813
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.040015944451,     best is           0.040015944451
            Average val evaluation index is   -3.252156821452,     best is           -3.194740656018
    Train use time   1541.34110
    Train loader use time     86.05735
    Val use time     45.98283
    Total use time   1590.56392
    End lr is 0.000084610434, 0.000084610434
    time: 13:16:31
Epoch: 103, iter: 51499
    Begin lr is 0.000084610434, 0.000084610434
========> pt_241:  3.0789853259921074
========> pt_242:  2.37775307148695
========> pt_243:  4.275608956813812
========> pt_244:  2.655821330845356
========> pt_245:  3.313601166009903
========> pt_246:  3.6787111684679985
========> pt_247:  2.365964688360691
========> pt_248:  2.3687924817204475
========> pt_249:  3.551389016211033
========> pt_250:  2.385060116648674
========> pt_251:  3.3600743487477303
========> pt_252:  3.4483281522989273
========> pt_253:  3.4858767315745354
========> pt_254:  3.4594372287392616
========> pt_255:  3.7265609577298164
========> pt_256:  2.0794267393648624
========> pt_257:  2.6231736689805984
========> pt_258:  2.3358438909053802
========> pt_259:  2.9228397086262703
========> pt_260:  3.733338378369808
========> pt_261:  3.5140469670295715
========> pt_262:  3.353898525238037
========> pt_263:  2.7115317806601524
========> pt_264:  3.3483511582016945
========> pt_265:  2.409309409558773
========> pt_266:  3.4340593218803406
========> pt_267:  3.242424428462982
========> pt_268:  3.6911407858133316
========> pt_269:  2.471298426389694
========> pt_270:  4.6953558176755905
========> pt_271:  3.1788501888513565
========> pt_272:  3.346407897770405
========> pt_273:  2.4994248524308205
========> pt_274:  4.173001311719418
========> pt_275:  3.212941735982895
========> pt_276:  2.3090771213173866
========> pt_277:  2.4100100994110107
========> pt_278:  4.282257817685604
========> pt_279:  2.7020976319909096
========> pt_280:  2.7938705310225487
========> pt_281:  2.6433971896767616
========> pt_282:  2.4845990166068077
========> pt_283:  6.428851559758186
========> pt_284:  2.7226395532488823
========> pt_285:  2.648942209780216
========> pt_286:  2.9163162782788277
========> pt_287:  4.050971493124962
========> pt_288:  2.091345377266407
========> pt_289:  3.6645255237817764
========> pt_290:  3.639890030026436
========> pt_291:  2.3253244161605835
========> pt_292:  2.8895067423582077
========> pt_293:  2.657766677439213
========> pt_294:  2.3735786601901054
========> pt_295:  2.9690328240394592
========> pt_296:  1.9127866812050343
========> pt_297:  4.11760039627552
========> pt_298:  3.337842635810375
========> pt_299:  2.402045913040638
========> pt_300:  4.282821342349052
========> pt_301:  3.186289966106415
========> pt_302:  5.021787062287331
========> pt_303:  2.924633026123047
========> pt_304:  2.5374021381139755
========> pt_305:  3.411104492843151
========> pt_306:  2.3087282106280327
========> pt_307:  2.6024695485830307
========> pt_308:  3.1582600250840187
========> pt_309:  2.3094049096107483
========> pt_310:  3.850875645875931
========> pt_311:  2.522849850356579
========> pt_312:  2.3181894794106483
========> pt_313:  2.3846937343478203
========> pt_314:  3.1004636734724045
========> pt_315:  2.780415564775467
========> pt_316:  3.745650127530098
========> pt_317:  4.271525293588638
========> pt_318:  5.161442086100578
========> pt_319:  2.2766485065221786
========> pt_320:  2.178665231913328
========> pt_321:  3.7244338542222977
========> pt_322:  2.834705337882042
========> pt_323:  7.073541507124901
========> pt_324:  2.5708626210689545
========> pt_325:  3.404288999736309
========> pt_326:  2.6997606083750725
========> pt_327:  2.609711140394211
========> pt_328:  2.415648475289345
========> pt_329:  6.701515093445778
========> pt_330:  7.540852352976799
========> pt_331:  3.2309210672974586
========> pt_332:  2.282773479819298
========> pt_333:  2.098595704883337
========> pt_334:  1.9237273000180721
========> pt_335:  4.6233899891376495
========> pt_336:  3.623969480395317
========> pt_337:  2.63681560754776
========> pt_338:  2.282988876104355
========> pt_339:  2.4475135654211044
========> pt_340:  3.1736475601792336
===============================================> mean Dose score: 3.1951688865199683
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.039735876296,     best is           0.039735876296
            Average val evaluation index is   -3.195168886520,     best is           -3.194740656018
    Train use time   1541.80164
    Train loader use time     86.80448
    Val use time     46.00083
    Total use time   1590.98111
    End lr is 0.000081974125, 0.000081974125
    time: 13:43:02
Epoch: 104, iter: 51999
    Begin lr is 0.000081974125, 0.000081974125
========> pt_241:  2.657434195280075
========> pt_242:  2.3615647107362747
========> pt_243:  4.417831003665924
========> pt_244:  2.6485682651400566
========> pt_245:  3.5079000890254974
========> pt_246:  3.6468935385346413
========> pt_247:  2.345287688076496
========> pt_248:  2.27484293282032
========> pt_249:  3.893730901181698
========> pt_250:  2.3559657111763954
========> pt_251:  3.676982782781124
========> pt_252:  3.6130836233496666
========> pt_253:  3.755379728972912
========> pt_254:  3.5629427060484886
========> pt_255:  3.3305178582668304
========> pt_256:  2.175483964383602
========> pt_257:  2.871979847550392
========> pt_258:  2.4188097938895226
========> pt_259:  2.5539926066994667
========> pt_260:  4.1588858142495155
========> pt_261:  3.9633823931217194
========> pt_262:  3.3051303029060364
========> pt_263:  2.780405655503273
========> pt_264:  3.9305727928876877
========> pt_265:  2.6965711265802383
========> pt_266:  3.6170129105448723
========> pt_267:  3.551134765148163
========> pt_268:  4.055830426514149
========> pt_269:  2.6152371242642403
========> pt_270:  5.277184471487999
========> pt_271:  3.2920677959918976
========> pt_272:  2.8493131697177887
========> pt_273:  2.515374608337879
========> pt_274:  4.322092570364475
========> pt_275:  3.359409384429455
========> pt_276:  2.21286304295063
========> pt_277:  2.4210086092352867
========> pt_278:  4.610345736145973
========> pt_279:  2.5702938809990883
========> pt_280:  2.879490815103054
========> pt_281:  2.8681157529354095
========> pt_282:  2.702193595468998
========> pt_283:  6.960965916514397
========> pt_284:  2.415733225643635
========> pt_285:  2.639896869659424
========> pt_286:  3.080953098833561
========> pt_287:  4.346081092953682
========> pt_288:  2.2401417046785355
========> pt_289:  3.914537765085697
========> pt_290:  3.977152891457081
========> pt_291:  2.1826979145407677
========> pt_292:  2.9548583924770355
========> pt_293:  3.128705620765686
========> pt_294:  2.6117049902677536
========> pt_295:  3.2300083711743355
========> pt_296:  2.054075300693512
========> pt_297:  4.420574307441711
========> pt_298:  3.0761398002505302
========> pt_299:  2.618812546133995
========> pt_300:  4.156206138432026
========> pt_301:  3.520417846739292
========> pt_302:  5.178152769804001
========> pt_303:  3.238556943833828
========> pt_304:  2.738537937402725
========> pt_305:  3.308497630059719
========> pt_306:  2.320210188627243
========> pt_307:  2.7727150171995163
========> pt_308:  3.929176889359951
========> pt_309:  2.464153580367565
========> pt_310:  3.7081317976117134
========> pt_311:  2.1615512669086456
========> pt_312:  2.3980602994561195
========> pt_313:  2.382610961794853
========> pt_314:  3.2752903550863266
========> pt_315:  2.9503869637846947
========> pt_316:  4.372495301067829
========> pt_317:  3.9986176788806915
========> pt_318:  5.6791431456804276
========> pt_319:  2.2326012700796127
========> pt_320:  2.273634262382984
========> pt_321:  4.672352746129036
========> pt_322:  3.2829616963863373
========> pt_323:  8.004597947001457
========> pt_324:  2.758089192211628
========> pt_325:  3.4593717753887177
========> pt_326:  2.7600574865937233
========> pt_327:  2.6331450045108795
========> pt_328:  2.4644359946250916
========> pt_329:  6.967540457844734
========> pt_330:  6.528383418917656
========> pt_331:  3.599911853671074
========> pt_332:  2.2946395725011826
========> pt_333:  2.3052125051617622
========> pt_334:  1.8849339336156845
========> pt_335:  5.1698122918605804
========> pt_336:  3.6964289471507072
========> pt_337:  2.8247446939349174
========> pt_338:  2.4181821197271347
========> pt_339:  2.5975751504302025
========> pt_340:  3.2705125212669373
===============================================> mean Dose score: 3.3312818605452774
        ==> Saving latest model successfully !
            Average train loss is             0.039770978298,     best is           0.039735876296
            Average val evaluation index is   -3.331281860545,     best is           -3.194740656018
    Train use time   1541.96745
    Train loader use time     87.45936
    Val use time     45.63157
    Total use time   1589.26819
    End lr is 0.000079364059, 0.000079364059
    time: 14:09:31
Epoch: 105, iter: 52499
    Begin lr is 0.000079364059, 0.000079364059
========> pt_241:  2.730509862303734
========> pt_242:  2.361637204885483
========> pt_243:  4.023392163217068
========> pt_244:  2.4381378293037415
========> pt_245:  3.2252344489097595
========> pt_246:  3.4716201573610306
========> pt_247:  2.3524852097034454
========> pt_248:  2.207243964076042
========> pt_249:  3.4302200004458427
========> pt_250:  2.281871475279331
========> pt_251:  3.4145091101527214
========> pt_252:  3.333251513540745
========> pt_253:  3.4060439839959145
========> pt_254:  3.235172927379608
========> pt_255:  3.3484387770295143
========> pt_256:  2.1091571636497974
========> pt_257:  2.5318560749292374
========> pt_258:  2.1982596442103386
========> pt_259:  2.5949914380908012
========> pt_260:  4.244419261813164
========> pt_261:  3.4510526806116104
========> pt_262:  3.2332768663764
========> pt_263:  2.7808773890137672
========> pt_264:  3.583257496356964
========> pt_265:  2.4245261400938034
========> pt_266:  3.335847482085228
========> pt_267:  3.2414358481764793
========> pt_268:  3.6737390607595444
========> pt_269:  2.458793967962265
========> pt_270:  4.604629650712013
========> pt_271:  3.1033120676875114
========> pt_272:  3.2804742082953453
========> pt_273:  2.4597512558102608
========> pt_274:  4.215248450636864
========> pt_275:  3.341601900756359
========> pt_276:  2.069966122508049
========> pt_277:  2.3133545368909836
========> pt_278:  3.777708187699318
========> pt_279:  2.362552508711815
========> pt_280:  2.739557810127735
========> pt_281:  2.729056589305401
========> pt_282:  2.56425391882658
========> pt_283:  7.288524731993675
========> pt_284:  2.7705996483564377
========> pt_285:  2.521973140537739
========> pt_286:  2.9589535295963287
========> pt_287:  3.982677310705185
========> pt_288:  2.114872597157955
========> pt_289:  3.3298244699835777
========> pt_290:  3.585326448082924
========> pt_291:  2.2010957822203636
========> pt_292:  2.8893930464982986
========> pt_293:  2.786455787718296
========> pt_294:  2.4725326523184776
========> pt_295:  3.0121373757719994
========> pt_296:  1.9346946477890015
========> pt_297:  4.033766649663448
========> pt_298:  3.1857264414429665
========> pt_299:  2.430487610399723
========> pt_300:  4.39391054213047
========> pt_301:  3.313528671860695
========> pt_302:  4.579597786068916
========> pt_303:  2.939152717590332
========> pt_304:  2.5769179686903954
========> pt_305:  3.43535665422678
========> pt_306:  2.240995466709137
========> pt_307:  2.601805627346039
========> pt_308:  3.179028294980526
========> pt_309:  2.355833761394024
========> pt_310:  3.8465794548392296
========> pt_311:  2.2445672377943993
========> pt_312:  2.2043421119451523
========> pt_313:  2.2971077635884285
========> pt_314:  3.062252476811409
========> pt_315:  2.8382153064012527
========> pt_316:  3.6305836588144302
========> pt_317:  4.305026195943356
========> pt_318:  4.663088619709015
========> pt_319:  2.1658842265605927
========> pt_320:  2.1246272325515747
========> pt_321:  3.315545991063118
========> pt_322:  2.967158928513527
========> pt_323:  7.355776354670525
========> pt_324:  2.6159099116921425
========> pt_325:  3.192981854081154
========> pt_326:  2.626718580722809
========> pt_327:  2.5567810237407684
========> pt_328:  2.2603602707386017
========> pt_329:  6.517888456583023
========> pt_330:  7.094290480017662
========> pt_331:  3.3957920596003532
========> pt_332:  2.3635995015501976
========> pt_333:  2.165234386920929
========> pt_334:  1.8005516566336155
========> pt_335:  4.350344426929951
========> pt_336:  3.5393959283828735
========> pt_337:  2.5288739055395126
========> pt_338:  2.2666800394654274
========> pt_339:  2.4538150802254677
========> pt_340:  3.043193817138672
===============================================> mean Dose score: 3.135910626500845
        ==> Saving best_train_loss model successfully !
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.039471152719,     best is           0.039471152719
            Average val evaluation index is   -3.135910626501,     best is           -3.135910626501
    Train use time   1544.53492
    Train loader use time     88.55952
    Val use time     45.92302
    Total use time   1595.59697
    End lr is 0.000076781245, 0.000076781245
    time: 14:36:07
Epoch: 106, iter: 52999
    Begin lr is 0.000076781245, 0.000076781245
========> pt_241:  2.8199397400021553
========> pt_242:  2.336534671485424
========> pt_243:  4.52281191945076
========> pt_244:  2.7048683166503906
========> pt_245:  3.3826961740851402
========> pt_246:  3.765229545533657
========> pt_247:  2.403799332678318
========> pt_248:  2.20976822078228
========> pt_249:  3.7462950125336647
========> pt_250:  2.4172110110521317
========> pt_251:  3.5999582707881927
========> pt_252:  3.408784680068493
========> pt_253:  3.7251587957143784
========> pt_254:  3.4507809579372406
========> pt_255:  3.5030195116996765
========> pt_256:  2.186942081898451
========> pt_257:  2.671196609735489
========> pt_258:  2.285301387310028
========> pt_259:  2.6387659087777138
========> pt_260:  4.0650103241205215
========> pt_261:  4.000830054283142
========> pt_262:  3.224034644663334
========> pt_263:  2.7815908566117287
========> pt_264:  3.80239088088274
========> pt_265:  2.4895312264561653
========> pt_266:  3.80882877856493
========> pt_267:  3.285173811018467
========> pt_268:  4.005439169704914
========> pt_269:  2.596251219511032
========> pt_270:  5.1552461832761765
========> pt_271:  3.294448107481003
========> pt_272:  3.2102784886956215
========> pt_273:  2.5784510374069214
========> pt_274:  4.139774218201637
========> pt_275:  3.308989964425564
========> pt_276:  2.2748909145593643
========> pt_277:  2.3392680659890175
========> pt_278:  4.410399571061134
========> pt_279:  2.532658465206623
========> pt_280:  2.7272583171725273
========> pt_281:  2.6691657304763794
========> pt_282:  2.655208520591259
========> pt_283:  6.64546512067318
========> pt_284:  2.826685607433319
========> pt_285:  2.596248611807823
========> pt_286:  2.915119081735611
========> pt_287:  4.3334950134158134
========> pt_288:  2.183762900531292
========> pt_289:  3.602263741195202
========> pt_290:  3.867047056555748
========> pt_291:  2.2680582106113434
========> pt_292:  2.985113486647606
========> pt_293:  2.8965451940894127
========> pt_294:  2.60564886033535
========> pt_295:  3.185162916779518
========> pt_296:  1.9392111897468567
========> pt_297:  4.52508270740509
========> pt_298:  3.2598577067255974
========> pt_299:  2.54738911986351
========> pt_300:  4.504063576459885
========> pt_301:  3.333781659603119
========> pt_302:  5.054395347833633
========> pt_303:  2.9754263907670975
========> pt_304:  2.6496384665369987
========> pt_305:  3.369445390999317
========> pt_306:  2.297377921640873
========> pt_307:  2.642737962305546
========> pt_308:  3.4621382877230644
========> pt_309:  2.3549768701195717
========> pt_310:  4.0048980712890625
========> pt_311:  2.318545952439308
========> pt_312:  2.26662527769804
========> pt_313:  2.3083078488707542
========> pt_314:  3.148243837058544
========> pt_315:  2.8565826639533043
========> pt_316:  4.090099036693573
========> pt_317:  4.41594198346138
========> pt_318:  5.650185123085976
========> pt_319:  2.2096949443221092
========> pt_320:  2.1462328359484673
========> pt_321:  3.8410088792443275
========> pt_322:  2.9944349825382233
========> pt_323:  7.474786713719368
========> pt_324:  2.6957987248897552
========> pt_325:  3.4607794135808945
========> pt_326:  2.754664234817028
========> pt_327:  2.654471844434738
========> pt_328:  2.5339391082525253
========> pt_329:  6.583376228809357
========> pt_330:  6.717940494418144
========> pt_331:  3.4137416630983353
========> pt_332:  2.2427773103117943
========> pt_333:  2.2330404072999954
========> pt_334:  1.8452997133135796
========> pt_335:  4.583287164568901
========> pt_336:  3.8122404366731644
========> pt_337:  2.7294107154011726
========> pt_338:  2.3563313111662865
========> pt_339:  2.6560453325510025
========> pt_340:  3.159897141158581
===============================================> mean Dose score: 3.2711894849315284
        ==> Saving latest model successfully !
            Average train loss is             0.040068204049,     best is           0.039471152719
            Average val evaluation index is   -3.271189484932,     best is           -3.135910626501
    Train use time   1545.23434
    Train loader use time     90.87345
    Val use time     46.03504
    Total use time   1593.05810
    End lr is 0.000074226677, 0.000074226677
    time: 15:02:40
Epoch: 107, iter: 53499
    Begin lr is 0.000074226677, 0.000074226677
========> pt_241:  2.7538493275642395
========> pt_242:  2.4038170650601387
========> pt_243:  4.138041138648987
========> pt_244:  2.5277025252580643
========> pt_245:  3.1676459312438965
========> pt_246:  3.4339404106140137
========> pt_247:  2.2600390017032623
========> pt_248:  2.2073297575116158
========> pt_249:  3.525436110794544
========> pt_250:  2.2618242353200912
========> pt_251:  3.5666628554463387
========> pt_252:  3.4039898961782455
========> pt_253:  3.472853861749172
========> pt_254:  3.5546700283885
========> pt_255:  3.5487205535173416
========> pt_256:  2.1741853281855583
========> pt_257:  2.6062632352113724
========> pt_258:  2.2968748956918716
========> pt_259:  2.7418293803930283
========> pt_260:  3.747665099799633
========> pt_261:  3.423159122467041
========> pt_262:  3.2322442159056664
========> pt_263:  2.8104685619473457
========> pt_264:  3.70538666844368
========> pt_265:  2.437800131738186
========> pt_266:  3.6563600227236748
========> pt_267:  3.30112487077713
========> pt_268:  3.5348139330744743
========> pt_269:  2.529277056455612
========> pt_270:  4.72302719950676
========> pt_271:  3.0271251499652863
========> pt_272:  3.3763638883829117
========> pt_273:  2.4610621482133865
========> pt_274:  4.217718206346035
========> pt_275:  3.296506628394127
========> pt_276:  2.141299843788147
========> pt_277:  2.3235537856817245
========> pt_278:  4.013543389737606
========> pt_279:  2.40518219769001
========> pt_280:  2.6628660410642624
========> pt_281:  2.868213541805744
========> pt_282:  2.5641031935811043
========> pt_283:  6.043085157871246
========> pt_284:  2.8223834186792374
========> pt_285:  2.546577602624893
========> pt_286:  2.937883287668228
========> pt_287:  4.094123765826225
========> pt_288:  2.1684534661471844
========> pt_289:  3.4216583892703056
========> pt_290:  3.579072132706642
========> pt_291:  2.167763989418745
========> pt_292:  2.836991250514984
========> pt_293:  2.7334044128656387
========> pt_294:  2.4760254099965096
========> pt_295:  2.8159254416823387
========> pt_296:  1.9354439713060856
========> pt_297:  4.301987700164318
========> pt_298:  3.0902213975787163
========> pt_299:  2.4011486023664474
========> pt_300:  4.193831384181976
========> pt_301:  3.358273468911648
========> pt_302:  4.944629296660423
========> pt_303:  3.0712683498859406
========> pt_304:  2.597433552145958
========> pt_305:  3.3180512115359306
========> pt_306:  2.2435781359672546
========> pt_307:  2.6922480762004852
========> pt_308:  3.258673809468746
========> pt_309:  2.1556457318365574
========> pt_310:  3.5825140401721
========> pt_311:  2.2711392119526863
========> pt_312:  2.26804256439209
========> pt_313:  2.297190949320793
========> pt_314:  3.1218815222382545
========> pt_315:  2.863336093723774
========> pt_316:  3.7353773415088654
========> pt_317:  3.740254007279873
========> pt_318:  5.0412822514772415
========> pt_319:  2.207762636244297
========> pt_320:  2.2481901198625565
========> pt_321:  3.55010237544775
========> pt_322:  2.8354161977767944
========> pt_323:  7.427092343568802
========> pt_324:  2.5082503631711006
========> pt_325:  3.197263441979885
========> pt_326:  2.6846007257699966
========> pt_327:  2.6777393370866776
========> pt_328:  2.3182737082242966
========> pt_329:  6.952124238014221
========> pt_330:  6.91543385386467
========> pt_331:  3.548998013138771
========> pt_332:  2.320653758943081
========> pt_333:  2.188101075589657
========> pt_334:  1.8351849541068077
========> pt_335:  4.515140056610107
========> pt_336:  3.3006948605179787
========> pt_337:  2.5074341520667076
========> pt_338:  2.1570145152509212
========> pt_339:  2.461822032928467
========> pt_340:  3.339713141322136
===============================================> mean Dose score: 3.1532834673300387
        ==> Saving latest model successfully !
            Average train loss is             0.039491341062,     best is           0.039471152719
            Average val evaluation index is   -3.153283467330,     best is           -3.135910626501
    Train use time   1541.41486
    Train loader use time     86.94580
    Val use time     46.13073
    Total use time   1589.04359
    End lr is 0.000071701340, 0.000071701340
    time: 15:29:09
Epoch: 108, iter: 53999
    Begin lr is 0.000071701340, 0.000071701340
========> pt_241:  2.6663175970315933
========> pt_242:  2.2695310413837433
========> pt_243:  4.3838150799274445
========> pt_244:  2.535650283098221
========> pt_245:  3.3605072274804115
========> pt_246:  3.5097622498869896
========> pt_247:  2.3742350190877914
========> pt_248:  2.4625495821237564
========> pt_249:  3.5733873397111893
========> pt_250:  2.395992912352085
========> pt_251:  3.791760839521885
========> pt_252:  3.439963683485985
========> pt_253:  3.696347065269947
========> pt_254:  3.4930022805929184
========> pt_255:  3.298887722194195
========> pt_256:  2.1767725609242916
========> pt_257:  2.751460410654545
========> pt_258:  2.1776068955659866
========> pt_259:  2.5955479219555855
========> pt_260:  4.068982899188995
========> pt_261:  4.04181245714426
========> pt_262:  3.3845113962888718
========> pt_263:  2.7641208097338676
========> pt_264:  3.7285947054624557
========> pt_265:  2.5519750267267227
========> pt_266:  3.603411391377449
========> pt_267:  3.3330924436450005
========> pt_268:  3.8800879195332527
========> pt_269:  2.475312203168869
========> pt_270:  4.761914312839508
========> pt_271:  3.2725058495998383
========> pt_272:  3.143036514520645
========> pt_273:  2.665083631873131
========> pt_274:  4.094753786921501
========> pt_275:  3.3916139975190163
========> pt_276:  2.069827653467655
========> pt_277:  2.4706514552235603
========> pt_278:  4.353961832821369
========> pt_279:  2.724638096988201
========> pt_280:  2.7873538807034492
========> pt_281:  2.952023558318615
========> pt_282:  2.4500805884599686
========> pt_283:  6.3748204708099365
========> pt_284:  2.4922502785921097
========> pt_285:  2.636145167052746
========> pt_286:  2.9781172797083855
========> pt_287:  4.526655673980713
========> pt_288:  2.153074536472559
========> pt_289:  3.5918277129530907
========> pt_290:  3.8693321868777275
========> pt_291:  2.214769273996353
========> pt_292:  2.9461830854415894
========> pt_293:  2.9133468866348267
========> pt_294:  2.354806065559387
========> pt_295:  3.172401078045368
========> pt_296:  1.9826744310557842
========> pt_297:  4.677702188491821
========> pt_298:  3.1345030665397644
========> pt_299:  2.4277667328715324
========> pt_300:  4.303345270454884
========> pt_301:  3.5009609907865524
========> pt_302:  4.8404411226511
========> pt_303:  3.021382726728916
========> pt_304:  2.666446939110756
========> pt_305:  3.3771660178899765
========> pt_306:  2.3249105736613274
========> pt_307:  2.608811743557453
========> pt_308:  3.602529466152191
========> pt_309:  2.3021262884140015
========> pt_310:  3.8182339817285538
========> pt_311:  2.283039204776287
========> pt_312:  2.2597135603427887
========> pt_313:  2.404547482728958
========> pt_314:  3.201986253261566
========> pt_315:  2.9309136793017387
========> pt_316:  4.071087576448917
========> pt_317:  3.972194865345955
========> pt_318:  5.306090340018272
========> pt_319:  2.21006341278553
========> pt_320:  2.2120822966098785
========> pt_321:  3.531777262687683
========> pt_322:  2.9998571798205376
========> pt_323:  7.878602594137192
========> pt_324:  2.7129673212766647
========> pt_325:  3.5853619128465652
========> pt_326:  2.7554473280906677
========> pt_327:  2.671174705028534
========> pt_328:  2.640121653676033
========> pt_329:  6.56157948076725
========> pt_330:  6.54228039085865
========> pt_331:  3.7319985404610634
========> pt_332:  2.2741367667913437
========> pt_333:  2.208680287003517
========> pt_334:  1.8919416144490242
========> pt_335:  4.79753240942955
========> pt_336:  3.541015312075615
========> pt_337:  2.9076044633984566
========> pt_338:  2.438778541982174
========> pt_339:  2.556621693074703
========> pt_340:  3.3000674471259117
===============================================> mean Dose score: 3.251144409365952
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.039144406568,     best is           0.039144406568
            Average val evaluation index is   -3.251144409366,     best is           -3.135910626501
    Train use time   1544.63720
    Train loader use time     88.80484
    Val use time     46.28551
    Total use time   1594.20095
    End lr is 0.000069206208, 0.000069206208
    time: 15:55:43
Epoch: 109, iter: 54499
    Begin lr is 0.000069206208, 0.000069206208
========> pt_241:  2.735086902976036
========> pt_242:  2.3800937458872795
========> pt_243:  4.352497085928917
========> pt_244:  2.5669461116194725
========> pt_245:  3.2582104206085205
========> pt_246:  3.5182109475135803
========> pt_247:  2.4178731068968773
========> pt_248:  2.2311169654130936
========> pt_249:  3.5007930546998978
========> pt_250:  2.3692796006798744
========> pt_251:  3.486490324139595
========> pt_252:  3.4429946169257164
========> pt_253:  3.691735342144966
========> pt_254:  3.4457793831825256
========> pt_255:  3.5223660618066788
========> pt_256:  2.1643215604126453
========> pt_257:  2.5639934092760086
========> pt_258:  2.2031626477837563
========> pt_259:  2.8662588074803352
========> pt_260:  3.9532454684376717
========> pt_261:  3.549085631966591
========> pt_262:  3.292299099266529
========> pt_263:  2.7759261429309845
========> pt_264:  3.5185421258211136
========> pt_265:  2.4830349162220955
========> pt_266:  3.642694354057312
========> pt_267:  3.225127011537552
========> pt_268:  3.8002337887883186
========> pt_269:  2.4804874509572983
========> pt_270:  4.649738743901253
========> pt_271:  3.1286412104964256
========> pt_272:  3.397553041577339
========> pt_273:  2.472166270017624
========> pt_274:  4.09883301705122
========> pt_275:  3.2326343283057213
========> pt_276:  2.1470935083925724
========> pt_277:  2.247106358408928
========> pt_278:  3.9050086960196495
========> pt_279:  2.5069720670580864
========> pt_280:  2.7073372900485992
========> pt_281:  2.809968665242195
========> pt_282:  2.605721093714237
========> pt_283:  6.294238269329071
========> pt_284:  2.7171555534005165
========> pt_285:  2.4695058912038803
========> pt_286:  2.8115979582071304
========> pt_287:  4.101222455501556
========> pt_288:  2.0817014388740063
========> pt_289:  3.2746587693691254
========> pt_290:  3.767003044486046
========> pt_291:  2.257952056825161
========> pt_292:  2.998086027801037
========> pt_293:  2.7328328043222427
========> pt_294:  2.499551586806774
========> pt_295:  3.0689461901783943
========> pt_296:  1.9424624741077423
========> pt_297:  4.30801410228014
========> pt_298:  3.2379571720957756
========> pt_299:  2.4313491955399513
========> pt_300:  4.425058513879776
========> pt_301:  3.2617754116654396
========> pt_302:  4.7608498483896255
========> pt_303:  2.9927144199609756
========> pt_304:  2.5663846731185913
========> pt_305:  3.2769887521862984
========> pt_306:  2.347382716834545
========> pt_307:  2.6330146193504333
========> pt_308:  3.276137597858906
========> pt_309:  2.173525970429182
========> pt_310:  3.8720762729644775
========> pt_311:  2.2965799644589424
========> pt_312:  2.3019155859947205
========> pt_313:  2.323419488966465
========> pt_314:  3.0711984634399414
========> pt_315:  2.829239070415497
========> pt_316:  3.5595905035734177
========> pt_317:  4.263430200517178
========> pt_318:  5.2416446059942245
========> pt_319:  2.1560543589293957
========> pt_320:  2.182531412690878
========> pt_321:  3.592316135764122
========> pt_322:  3.0053873360157013
========> pt_323:  7.535525858402252
========> pt_324:  2.5941473245620728
========> pt_325:  3.329423926770687
========> pt_326:  2.802075147628784
========> pt_327:  2.618470937013626
========> pt_328:  2.5506195425987244
========> pt_329:  7.059696689248085
========> pt_330:  6.799494847655296
========> pt_331:  3.4236204251646996
========> pt_332:  2.2454755008220673
========> pt_333:  2.1478649973869324
========> pt_334:  1.882737074047327
========> pt_335:  4.625127241015434
========> pt_336:  3.68605837225914
========> pt_337:  2.571074366569519
========> pt_338:  2.2562745213508606
========> pt_339:  2.5642458349466324
========> pt_340:  3.203877881169319
===============================================> mean Dose score: 3.1864592177793383
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.038874518983,     best is           0.038874518983
            Average val evaluation index is   -3.186459217779,     best is           -3.135910626501
    Train use time   1543.76722
    Train loader use time     88.65423
    Val use time     46.07888
    Total use time   1592.87856
    End lr is 0.000066742244, 0.000066742244
    time: 16:22:16
Epoch: 110, iter: 54999
    Begin lr is 0.000066742244, 0.000066742244
========> pt_241:  2.7893847599625587
========> pt_242:  2.327352948486805
========> pt_243:  4.34046383947134
========> pt_244:  2.597552463412285
========> pt_245:  3.310191072523594
========> pt_246:  3.656994476914406
========> pt_247:  2.3571477830410004
========> pt_248:  2.2157690674066544
========> pt_249:  3.6088959127664566
========> pt_250:  2.3182857036590576
========> pt_251:  3.655397780239582
========> pt_252:  3.451569527387619
========> pt_253:  3.7556245923042297
========> pt_254:  3.4967362508177757
========> pt_255:  3.4424321353435516
========> pt_256:  2.125477734953165
========> pt_257:  2.5847110897302628
========> pt_258:  2.2639547288417816
========> pt_259:  2.580830827355385
========> pt_260:  3.8910603523254395
========> pt_261:  3.579431213438511
========> pt_262:  3.251715935766697
========> pt_263:  2.702033221721649
========> pt_264:  3.791136294603348
========> pt_265:  2.4746720120310783
========> pt_266:  3.7356845289468765
========> pt_267:  3.2491301372647285
========> pt_268:  3.962426669895649
========> pt_269:  2.4314986169338226
========> pt_270:  4.98393677175045
========> pt_271:  3.1335843726992607
========> pt_272:  3.101965971291065
========> pt_273:  2.471281737089157
========> pt_274:  4.11913450807333
========> pt_275:  3.3258046954870224
========> pt_276:  2.2178294137120247
========> pt_277:  2.3113878071308136
========> pt_278:  4.238232225179672
========> pt_279:  2.410007230937481
========> pt_280:  2.8400732949376106
========> pt_281:  2.887382246553898
========> pt_282:  2.712121643126011
========> pt_283:  5.985984280705452
========> pt_284:  2.5879832357168198
========> pt_285:  2.506706342101097
========> pt_286:  3.0054055899381638
========> pt_287:  4.404308497905731
========> pt_288:  2.1406869031488895
========> pt_289:  3.51596936583519
========> pt_290:  3.922034129500389
========> pt_291:  2.2226643562316895
========> pt_292:  2.9024331271648407
========> pt_293:  3.0254843831062317
========> pt_294:  2.491803839802742
========> pt_295:  3.0924833193421364
========> pt_296:  1.9420732744038105
========> pt_297:  4.445784017443657
========> pt_298:  3.1767288222908974
========> pt_299:  2.4862416088581085
========> pt_300:  4.280591756105423
========> pt_301:  3.2731324806809425
========> pt_302:  4.6070292592048645
========> pt_303:  3.0677972361445427
========> pt_304:  2.6092446222901344
========> pt_305:  3.3517030999064445
========> pt_306:  2.3373154178261757
========> pt_307:  2.6826678961515427
========> pt_308:  3.40542308986187
========> pt_309:  2.2127245739102364
========> pt_310:  3.8431910052895546
========> pt_311:  2.275305539369583
========> pt_312:  2.3705868422985077
========> pt_313:  2.460526004433632
========> pt_314:  3.182944543659687
========> pt_315:  2.9224688932299614
========> pt_316:  3.9093145355582237
========> pt_317:  4.1963645070791245
========> pt_318:  5.353956297039986
========> pt_319:  2.223314456641674
========> pt_320:  2.244953438639641
========> pt_321:  3.5942161083221436
========> pt_322:  3.126714639365673
========> pt_323:  7.9799676686525345
========> pt_324:  2.620263732969761
========> pt_325:  3.2967063784599304
========> pt_326:  2.7403737604618073
========> pt_327:  2.6568732783198357
========> pt_328:  2.4745405837893486
========> pt_329:  7.021439075469971
========> pt_330:  6.813227012753487
========> pt_331:  3.408222198486328
========> pt_332:  2.247774191200733
========> pt_333:  2.1332013607025146
========> pt_334:  1.8713476695120335
========> pt_335:  4.646696597337723
========> pt_336:  3.604445606470108
========> pt_337:  2.724253460764885
========> pt_338:  2.2644105553627014
========> pt_339:  2.561199776828289
========> pt_340:  3.2147303596138954
===============================================> mean Dose score: 3.223662381991744
        ==> Saving latest model successfully !
            Average train loss is             0.038905278541,     best is           0.038874518983
            Average val evaluation index is   -3.223662381992,     best is           -3.135910626501
    Train use time   1554.78213
    Train loader use time     98.66537
    Val use time     51.40838
    Total use time   1608.07904
    End lr is 0.000064310395, 0.000064310395
    time: 16:49:04
Epoch: 111, iter: 55499
    Begin lr is 0.000064310395, 0.000064310395
========> pt_241:  2.708895392715931
========> pt_242:  2.389843948185444
========> pt_243:  4.386601150035858
========> pt_244:  2.609555199742317
========> pt_245:  3.117111511528492
========> pt_246:  3.4483563154935837
========> pt_247:  2.341485135257244
========> pt_248:  2.2769510000944138
========> pt_249:  3.8680270314216614
========> pt_250:  2.4009180814027786
========> pt_251:  3.6381759867072105
========> pt_252:  3.443603254854679
========> pt_253:  3.6430737748742104
========> pt_254:  3.5870957747101784
========> pt_255:  3.7174900621175766
========> pt_256:  2.1194561570882797
========> pt_257:  2.68251221626997
========> pt_258:  2.3611344397068024
========> pt_259:  3.1105872988700867
========> pt_260:  3.6831335723400116
========> pt_261:  3.8350659236311913
========> pt_262:  3.2197358459234238
========> pt_263:  2.7859175577759743
========> pt_264:  3.78090001642704
========> pt_265:  2.496582977473736
========> pt_266:  3.3734046667814255
========> pt_267:  3.1526073068380356
========> pt_268:  3.911520652472973
========> pt_269:  2.5120680406689644
========> pt_270:  4.972093626856804
========> pt_271:  3.2714583352208138
========> pt_272:  3.5715363919734955
========> pt_273:  2.4203233048319817
========> pt_274:  4.304819665849209
========> pt_275:  3.4107939153909683
========> pt_276:  2.1789283491671085
========> pt_277:  2.212747521698475
========> pt_278:  4.283346012234688
========> pt_279:  2.3714080080389977
========> pt_280:  2.6768850535154343
========> pt_281:  2.7574779465794563
========> pt_282:  2.6704056933522224
========> pt_283:  5.75889453291893
========> pt_284:  2.7103614434599876
========> pt_285:  2.429628111422062
========> pt_286:  2.890833541750908
========> pt_287:  4.24284603446722
========> pt_288:  2.053934223949909
========> pt_289:  3.8403016701340675
========> pt_290:  3.8577208667993546
========> pt_291:  2.314342074096203
========> pt_292:  2.884429544210434
========> pt_293:  2.96551451086998
========> pt_294:  2.4471763893961906
========> pt_295:  3.1207093596458435
========> pt_296:  1.9508043862879276
========> pt_297:  4.463192000985146
========> pt_298:  3.166036978363991
========> pt_299:  2.370002456009388
========> pt_300:  4.26094688475132
========> pt_301:  3.290829136967659
========> pt_302:  5.21320603787899
========> pt_303:  3.0381223559379578
========> pt_304:  2.5434494018554688
========> pt_305:  3.3563116937875748
========> pt_306:  2.2548220306634903
========> pt_307:  2.6030145585536957
========> pt_308:  3.5634595528244972
========> pt_309:  2.2155771404504776
========> pt_310:  3.7954682111740112
========> pt_311:  2.3548994213342667
========> pt_312:  2.2703253477811813
========> pt_313:  2.3899203538894653
========> pt_314:  3.0561674013733864
========> pt_315:  2.7521950006484985
========> pt_316:  4.157917313277721
========> pt_317:  3.893824778497219
========> pt_318:  5.258449167013168
========> pt_319:  2.1821847185492516
========> pt_320:  2.2024916857481003
========> pt_321:  3.9347388595342636
========> pt_322:  2.9252544417977333
========> pt_323:  7.478175163269043
========> pt_324:  2.670128755271435
========> pt_325:  3.4612394124269485
========> pt_326:  2.7606426551938057
========> pt_327:  2.5952068343758583
========> pt_328:  2.501797340810299
========> pt_329:  6.164609342813492
========> pt_330:  7.154310941696167
========> pt_331:  3.519301228225231
========> pt_332:  2.185155674815178
========> pt_333:  2.197200395166874
========> pt_334:  1.8704304099082947
========> pt_335:  4.779162704944611
========> pt_336:  3.646012395620346
========> pt_337:  2.7487025037407875
========> pt_338:  2.327583469450474
========> pt_339:  2.6166095584630966
========> pt_340:  3.2871175929903984
===============================================> mean Dose score: 3.2264772411435843
        ==> Saving latest model successfully !
            Average train loss is             0.038911536612,     best is           0.038874518983
            Average val evaluation index is   -3.226477241144,     best is           -3.135910626501
    Train use time   1568.25630
    Train loader use time    108.90536
    Val use time     52.23199
    Total use time   1622.70474
    End lr is 0.000061911601, 0.000061911601
    time: 17:16:07
Epoch: 112, iter: 55999
    Begin lr is 0.000061911601, 0.000061911601
========> pt_241:  2.690356969833374
========> pt_242:  2.3442748561501503
========> pt_243:  4.212312959134579
========> pt_244:  2.4589134007692337
========> pt_245:  3.268177844583988
========> pt_246:  3.5318367183208466
========> pt_247:  2.320253476500511
========> pt_248:  2.1849149838089943
========> pt_249:  3.7106868252158165
========> pt_250:  2.303282283246517
========> pt_251:  3.398681655526161
========> pt_252:  3.3039936050772667
========> pt_253:  3.601764887571335
========> pt_254:  3.4005268663167953
========> pt_255:  3.2603510841727257
========> pt_256:  2.046206034719944
========> pt_257:  2.6843634247779846
========> pt_258:  2.309698797762394
========> pt_259:  2.5931501388549805
========> pt_260:  3.791228085756302
========> pt_261:  3.5824300721287727
========> pt_262:  3.13813716173172
========> pt_263:  2.6029308512806892
========> pt_264:  3.5552700608968735
========> pt_265:  2.4425602331757545
========> pt_266:  3.7032467871904373
========> pt_267:  3.231777437031269
========> pt_268:  3.820847161114216
========> pt_269:  2.487725652754307
========> pt_270:  4.74954180419445
========> pt_271:  3.1670857965946198
========> pt_272:  3.0188339576125145
========> pt_273:  2.426000013947487
========> pt_274:  4.155538827180862
========> pt_275:  3.255757614970207
========> pt_276:  2.1398499608039856
========> pt_277:  2.2197385132312775
========> pt_278:  4.250889755785465
========> pt_279:  2.3615068197250366
========> pt_280:  2.662746086716652
========> pt_281:  2.7682146430015564
========> pt_282:  2.645062729716301
========> pt_283:  6.2794411182403564
========> pt_284:  2.5416289642453194
========> pt_285:  2.4359823018312454
========> pt_286:  2.917557805776596
========> pt_287:  4.228350855410099
========> pt_288:  2.052977066487074
========> pt_289:  3.462374024093151
========> pt_290:  3.722892962396145
========> pt_291:  2.2572041675448418
========> pt_292:  2.7381806820631027
========> pt_293:  2.9175541549921036
========> pt_294:  2.4145811423659325
========> pt_295:  3.140251748263836
========> pt_296:  1.9073854759335518
========> pt_297:  4.3216560408473015
========> pt_298:  3.132898025214672
========> pt_299:  2.361701615154743
========> pt_300:  4.232351332902908
========> pt_301:  3.192281164228916
========> pt_302:  4.932099282741547
========> pt_303:  3.0881568789482117
========> pt_304:  2.572912275791168
========> pt_305:  3.3438273146748543
========> pt_306:  2.2759032249450684
========> pt_307:  2.603471428155899
========> pt_308:  3.2644224911928177
========> pt_309:  2.271997407078743
========> pt_310:  3.68914432823658
========> pt_311:  2.307700514793396
========> pt_312:  2.2214924544095993
========> pt_313:  2.2621694952249527
========> pt_314:  3.114757537841797
========> pt_315:  2.741195186972618
========> pt_316:  3.799806647002697
========> pt_317:  4.078892953693867
========> pt_318:  5.658687800168991
========> pt_319:  2.145787961781025
========> pt_320:  2.1423803456127644
========> pt_321:  3.743746243417263
========> pt_322:  3.2111551985144615
========> pt_323:  7.709045559167862
========> pt_324:  2.64664925634861
========> pt_325:  3.3152322843670845
========> pt_326:  2.7121826633810997
========> pt_327:  2.5862767547369003
========> pt_328:  2.413407154381275
========> pt_329:  7.104893922805786
========> pt_330:  6.871591582894325
========> pt_331:  3.4877725318074226
========> pt_332:  2.2601941600441933
========> pt_333:  2.0656426809728146
========> pt_334:  1.8253516964614391
========> pt_335:  4.629485234618187
========> pt_336:  3.636429086327553
========> pt_337:  2.662041485309601
========> pt_338:  2.234113998711109
========> pt_339:  2.486407458782196
========> pt_340:  3.203677609562874
===============================================> mean Dose score: 3.173780195787549
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.038682473972,     best is           0.038682473972
            Average val evaluation index is   -3.173780195788,     best is           -3.135910626501
    Train use time   1566.32164
    Train loader use time    105.68434
    Val use time     52.11289
    Total use time   1622.72604
    End lr is 0.000059546787, 0.000059546787
    time: 17:43:10
Epoch: 113, iter: 56499
    Begin lr is 0.000059546787, 0.000059546787
========> pt_241:  2.524848133325577
========> pt_242:  2.367888130247593
========> pt_243:  4.373452849686146
========> pt_244:  2.5662407279014587
========> pt_245:  3.297811783850193
========> pt_246:  3.571093864738941
========> pt_247:  2.287365384399891
========> pt_248:  2.2564494982361794
========> pt_249:  3.809988424181938
========> pt_250:  2.325131706893444
========> pt_251:  3.70659951120615
========> pt_252:  3.559093475341797
========> pt_253:  3.6773838475346565
========> pt_254:  3.617127649486065
========> pt_255:  3.367513082921505
========> pt_256:  2.10526829585433
========> pt_257:  2.5897353515028954
========> pt_258:  2.2049491852521896
========> pt_259:  2.473536618053913
========> pt_260:  3.655383698642254
========> pt_261:  3.872675523161888
========> pt_262:  3.185976520180702
========> pt_263:  2.7073805779218674
========> pt_264:  3.911207988858223
========> pt_265:  2.4693168327212334
========> pt_266:  3.9103881269693375
========> pt_267:  3.3759813383221626
========> pt_268:  3.864583298563957
========> pt_269:  2.5199618190526962
========> pt_270:  5.054123103618622
========> pt_271:  3.114667572081089
========> pt_272:  3.1348996981978416
========> pt_273:  2.4515771493315697
========> pt_274:  4.104477651417255
========> pt_275:  3.1731046363711357
========> pt_276:  2.1247507072985172
========> pt_277:  2.3739726841449738
========> pt_278:  4.294167719781399
========> pt_279:  2.398451454937458
========> pt_280:  2.579997405409813
========> pt_281:  2.804740220308304
========> pt_282:  2.642596624791622
========> pt_283:  6.205539330840111
========> pt_284:  2.4987606704235077
========> pt_285:  2.5299793109297752
========> pt_286:  2.940702736377716
========> pt_287:  4.417019486427307
========> pt_288:  2.091394532471895
========> pt_289:  3.535296879708767
========> pt_290:  3.8986049592494965
========> pt_291:  2.0921003073453903
========> pt_292:  2.8530943393707275
========> pt_293:  2.930609881877899
========> pt_294:  2.5231393054127693
========> pt_295:  3.0224544927477837
========> pt_296:  1.9222446903586388
========> pt_297:  4.4333963841199875
========> pt_298:  3.2168718054890633
========> pt_299:  2.4700167402625084
========> pt_300:  4.1396863386034966
========> pt_301:  3.3248667046427727
========> pt_302:  4.895342662930489
========> pt_303:  3.003082387149334
========> pt_304:  2.634444162249565
========> pt_305:  3.3607275784015656
========> pt_306:  2.30555959045887
========> pt_307:  2.7120191603899
========> pt_308:  3.60514760017395
========> pt_309:  2.2979962080717087
========> pt_310:  3.7546782568097115
========> pt_311:  2.1864565275609493
========> pt_312:  2.3635118827223778
========> pt_313:  2.4529965221881866
========> pt_314:  3.243298791348934
========> pt_315:  2.9401154816150665
========> pt_316:  4.009223990142345
========> pt_317:  3.8847171142697334
========> pt_318:  5.7673429697752
========> pt_319:  2.197352685034275
========> pt_320:  2.17344356700778
========> pt_321:  3.734183795750141
========> pt_322:  3.0695104971528053
========> pt_323:  7.9098715633153915
========> pt_324:  2.6314346119761467
========> pt_325:  3.1201640889048576
========> pt_326:  2.6862091571092606
========> pt_327:  2.6590092480182648
========> pt_328:  2.517506927251816
========> pt_329:  7.0317478477954865
========> pt_330:  6.546995118260384
========> pt_331:  3.430006429553032
========> pt_332:  2.276427373290062
========> pt_333:  2.2183259204030037
========> pt_334:  1.8138173036277294
========> pt_335:  4.712477475404739
========> pt_336:  3.3874990418553352
========> pt_337:  2.778979502618313
========> pt_338:  2.2417569160461426
========> pt_339:  2.4884967505931854
========> pt_340:  3.234357237815857
===============================================> mean Dose score: 3.217278727144003
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.038554399393,     best is           0.038554399393
            Average val evaluation index is   -3.217278727144,     best is           -3.135910626501
    Train use time   1549.85063
    Train loader use time     88.45806
    Val use time     50.54542
    Total use time   1603.99634
    End lr is 0.000057216862, 0.000057216862
    time: 18:09:54
Epoch: 114, iter: 56999
    Begin lr is 0.000057216862, 0.000057216862
========> pt_241:  2.784295566380024
========> pt_242:  2.308313064277172
========> pt_243:  4.414097815752029
========> pt_244:  2.4640417098999023
========> pt_245:  3.3237560838460922
========> pt_246:  3.5473765432834625
========> pt_247:  2.294706329703331
========> pt_248:  2.204396091401577
========> pt_249:  3.53524811565876
========> pt_250:  2.249094471335411
========> pt_251:  3.7244875729084015
========> pt_252:  3.549123965203762
========> pt_253:  3.4966986998915672
========> pt_254:  3.5003823414444923
========> pt_255:  3.3186548948287964
========> pt_256:  2.1904733031988144
========> pt_257:  2.8458311036229134
========> pt_258:  2.29450736194849
========> pt_259:  2.7476920187473297
========> pt_260:  3.7649940699338913
========> pt_261:  3.659367226064205
========> pt_262:  3.3640271052718163
========> pt_263:  2.834968715906143
========> pt_264:  3.747250996530056
========> pt_265:  2.579297497868538
========> pt_266:  3.709172010421753
========> pt_267:  3.4984057024121284
========> pt_268:  3.756330758333206
========> pt_269:  2.5352956354618073
========> pt_270:  4.780140593647957
========> pt_271:  3.4710120409727097
========> pt_272:  3.239920511841774
========> pt_273:  2.3781590908765793
========> pt_274:  4.182001277804375
========> pt_275:  3.2892457395792007
========> pt_276:  2.1475748904049397
========> pt_277:  2.3808014765381813
========> pt_278:  4.101657159626484
========> pt_279:  2.475763075053692
========> pt_280:  2.6652132347226143
========> pt_281:  2.9749424010515213
========> pt_282:  2.578902691602707
========> pt_283:  6.639340668916702
========> pt_284:  2.4423018097877502
========> pt_285:  2.7288372814655304
========> pt_286:  2.9806144163012505
========> pt_287:  4.11076482385397
========> pt_288:  2.1675569377839565
========> pt_289:  3.556646928191185
========> pt_290:  3.7789544090628624
========> pt_291:  2.1637103147804737
========> pt_292:  3.008309006690979
========> pt_293:  2.9277659207582474
========> pt_294:  2.540081813931465
========> pt_295:  3.03100124001503
========> pt_296:  1.9904204830527306
========> pt_297:  4.437320455908775
========> pt_298:  3.027234934270382
========> pt_299:  2.4931900948286057
========> pt_300:  4.3175481259822845
========> pt_301:  3.2257885858416557
========> pt_302:  5.139607787132263
========> pt_303:  3.1664802879095078
========> pt_304:  2.5873740762472153
========> pt_305:  3.2716353982686996
========> pt_306:  2.1860524639487267
========> pt_307:  2.6795336976647377
========> pt_308:  3.360573723912239
========> pt_309:  2.2277136519551277
========> pt_310:  3.7229401618242264
========> pt_311:  2.1298393793404102
========> pt_312:  2.2602424025535583
========> pt_313:  2.2578133270144463
========> pt_314:  3.255208432674408
========> pt_315:  2.9708782956004143
========> pt_316:  3.858988732099533
========> pt_317:  4.011033736169338
========> pt_318:  5.455591008067131
========> pt_319:  2.2523105517029762
========> pt_320:  2.300674580037594
========> pt_321:  4.292682893574238
========> pt_322:  3.0844059586524963
========> pt_323:  7.866503372788429
========> pt_324:  2.6548147574067116
========> pt_325:  3.249173164367676
========> pt_326:  2.7347520738840103
========> pt_327:  2.771020010113716
========> pt_328:  2.4770914390683174
========> pt_329:  7.482946738600731
========> pt_330:  6.484131217002869
========> pt_331:  3.546682894229889
========> pt_332:  2.2041960805654526
========> pt_333:  2.2800669446587563
========> pt_334:  1.8459713272750378
========> pt_335:  4.731243550777435
========> pt_336:  3.455607034265995
========> pt_337:  2.653811313211918
========> pt_338:  2.250218130648136
========> pt_339:  2.5529025867581367
========> pt_340:  3.197602443397045
===============================================> mean Dose score: 3.23385324832052
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.038520874053,     best is           0.038520874053
            Average val evaluation index is   -3.233853248321,     best is           -3.135910626501
    Train use time   1548.67623
    Train loader use time     90.52727
    Val use time     46.37189
    Total use time   1598.28207
    End lr is 0.000054922727, 0.000054922727
    time: 18:36:32
Epoch: 115, iter: 57499
    Begin lr is 0.000054922727, 0.000054922727
========> pt_241:  2.570219561457634
========> pt_242:  2.369108535349369
========> pt_243:  4.395177885890007
========> pt_244:  2.514939121901989
========> pt_245:  3.528466783463955
========> pt_246:  3.571857661008835
========> pt_247:  2.262026071548462
========> pt_248:  2.6145875453948975
========> pt_249:  3.7003324180841446
========> pt_250:  2.280244790017605
========> pt_251:  3.822377622127533
========> pt_252:  3.6847344413399696
========> pt_253:  3.4300288558006287
========> pt_254:  3.5009437799453735
========> pt_255:  3.51735457777977
========> pt_256:  2.176986653357744
========> pt_257:  2.6245148107409477
========> pt_258:  2.1490608900785446
========> pt_259:  2.7063633129000664
========> pt_260:  3.8606008142232895
========> pt_261:  3.563932329416275
========> pt_262:  3.363592140376568
========> pt_263:  2.8066610544919968
========> pt_264:  3.7076131254434586
========> pt_265:  2.582358419895172
========> pt_266:  3.625590167939663
========> pt_267:  3.4472456946969032
========> pt_268:  3.8179820775985718
========> pt_269:  2.647365592420101
========> pt_270:  4.873564690351486
========> pt_271:  3.0757243931293488
========> pt_272:  3.140466623008251
========> pt_273:  2.476288266479969
========> pt_274:  4.09928310662508
========> pt_275:  3.31234659999609
========> pt_276:  2.0590809173882008
========> pt_277:  2.45147205889225
========> pt_278:  3.9918136596679688
========> pt_279:  2.4460868909955025
========> pt_280:  2.751663029193878
========> pt_281:  2.929163910448551
========> pt_282:  2.6659032329916954
========> pt_283:  6.16301603615284
========> pt_284:  2.4116936326026917
========> pt_285:  2.633740082383156
========> pt_286:  3.03593423217535
========> pt_287:  4.435946196317673
========> pt_288:  2.1251284331083298
========> pt_289:  3.503233604133129
========> pt_290:  3.781980909407139
========> pt_291:  2.026696763932705
========> pt_292:  3.0060961097478867
========> pt_293:  2.959880828857422
========> pt_294:  2.4175750464200974
========> pt_295:  3.034856729209423
========> pt_296:  1.955737378448248
========> pt_297:  4.4755300879478455
========> pt_298:  3.1224072352051735
========> pt_299:  2.5392575189471245
========> pt_300:  3.9955317229032516
========> pt_301:  3.223266936838627
========> pt_302:  5.1356784999370575
========> pt_303:  3.1293917074799538
========> pt_304:  2.6703929156064987
========> pt_305:  3.2944371551275253
========> pt_306:  2.252701446413994
========> pt_307:  2.658919021487236
========> pt_308:  3.688059002161026
========> pt_309:  2.4254876002669334
========> pt_310:  3.6133279651403427
========> pt_311:  2.0829224959015846
========> pt_312:  2.331015467643738
========> pt_313:  2.403593584895134
========> pt_314:  3.270196206867695
========> pt_315:  2.9856762290000916
========> pt_316:  3.737449422478676
========> pt_317:  3.78784641623497
========> pt_318:  5.222577080130577
========> pt_319:  2.239709608256817
========> pt_320:  2.2538428381085396
========> pt_321:  3.7053191289305687
========> pt_322:  3.293919786810875
========> pt_323:  8.061711341142654
========> pt_324:  2.6565780863165855
========> pt_325:  3.4157704561948776
========> pt_326:  2.8136105835437775
========> pt_327:  2.7264251559972763
========> pt_328:  2.635381370782852
========> pt_329:  7.672715038061142
========> pt_330:  6.5645767748355865
========> pt_331:  3.4289510920643806
========> pt_332:  2.331079877912998
========> pt_333:  2.2848768532276154
========> pt_334:  1.8531585484743118
========> pt_335:  4.916449934244156
========> pt_336:  3.3615023270249367
========> pt_337:  2.737235650420189
========> pt_338:  2.2261904925107956
========> pt_339:  2.403002679347992
========> pt_340:  3.2698898017406464
===============================================> mean Dose score: 3.2350420324131846
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.038267598689,     best is           0.038267598689
            Average val evaluation index is   -3.235042032413,     best is           -3.135910626501
    Train use time   1547.10445
    Train loader use time     90.17051
    Val use time     46.11336
    Total use time   1596.86673
    End lr is 0.000052665265, 0.000052665265
    time: 19:03:09
Epoch: 116, iter: 57999
    Begin lr is 0.000052665265, 0.000052665265
========> pt_241:  2.587066628038883
========> pt_242:  2.2786660864949226
========> pt_243:  4.283670410513878
========> pt_244:  2.472052313387394
========> pt_245:  3.132433071732521
========> pt_246:  3.411995805799961
========> pt_247:  2.32444666326046
========> pt_248:  2.1866987831890583
========> pt_249:  3.7786659970879555
========> pt_250:  2.2971896454691887
========> pt_251:  3.629755452275276
========> pt_252:  3.4697913751006126
========> pt_253:  3.6791589111089706
========> pt_254:  3.678811825811863
========> pt_255:  3.3508623763918877
========> pt_256:  2.1204826794564724
========> pt_257:  2.6982392743229866
========> pt_258:  2.231770195066929
========> pt_259:  2.592903971672058
========> pt_260:  3.6070308834314346
========> pt_261:  3.617767058312893
========> pt_262:  3.3342627808451653
========> pt_263:  2.7285611256957054
========> pt_264:  3.716697320342064
========> pt_265:  2.5497543066740036
========> pt_266:  3.617493510246277
========> pt_267:  3.336883522570133
========> pt_268:  3.732633516192436
========> pt_269:  2.389300763607025
========> pt_270:  4.9181558936834335
========> pt_271:  3.6133068427443504
========> pt_272:  3.1043340265750885
========> pt_273:  2.4897119402885437
========> pt_274:  4.148274026811123
========> pt_275:  3.2546569034457207
========> pt_276:  2.0686770044267178
========> pt_277:  2.327936291694641
========> pt_278:  4.131459817290306
========> pt_279:  2.360605075955391
========> pt_280:  2.660260945558548
========> pt_281:  2.919187620282173
========> pt_282:  2.544436678290367
========> pt_283:  6.254455670714378
========> pt_284:  2.5828664004802704
========> pt_285:  2.443869821727276
========> pt_286:  2.9515359178185463
========> pt_287:  4.151611626148224
========> pt_288:  2.1118786931037903
========> pt_289:  3.4951580688357353
========> pt_290:  3.8286350667476654
========> pt_291:  2.2363709658384323
========> pt_292:  2.8037258237600327
========> pt_293:  2.8964627906680107
========> pt_294:  2.47234333306551
========> pt_295:  3.0444812402129173
========> pt_296:  1.9429129548370838
========> pt_297:  4.404396116733551
========> pt_298:  3.150564432144165
========> pt_299:  2.418515905737877
========> pt_300:  4.149774499237537
========> pt_301:  3.3243991434574127
========> pt_302:  5.026074647903442
========> pt_303:  3.054458051919937
========> pt_304:  2.593737654387951
========> pt_305:  3.211915083229542
========> pt_306:  2.301785722374916
========> pt_307:  2.606278620660305
========> pt_308:  3.497992120683193
========> pt_309:  2.1773058362305164
========> pt_310:  3.71322438120842
========> pt_311:  2.2737229242920876
========> pt_312:  2.234026901423931
========> pt_313:  2.276707962155342
========> pt_314:  3.1582336872816086
========> pt_315:  2.9003871232271194
========> pt_316:  3.7602535262703896
========> pt_317:  3.8269251957535744
========> pt_318:  5.535327792167664
========> pt_319:  2.1467547677457333
========> pt_320:  2.1926890686154366
========> pt_321:  4.185116961598396
========> pt_322:  3.166026286780834
========> pt_323:  7.602564170956612
========> pt_324:  2.6263605430722237
========> pt_325:  3.167314752936363
========> pt_326:  2.641768679022789
========> pt_327:  2.6331982016563416
========> pt_328:  2.3321998864412308
========> pt_329:  7.60112002491951
========> pt_330:  6.428372785449028
========> pt_331:  3.5523640364408493
========> pt_332:  2.234466038644314
========> pt_333:  2.06632224842906
========> pt_334:  1.8233850970864296
========> pt_335:  4.847559109330177
========> pt_336:  3.3358821645379066
========> pt_337:  2.6567543670535088
========> pt_338:  2.287197969853878
========> pt_339:  2.529423087835312
========> pt_340:  3.2078037783503532
===============================================> mean Dose score: 3.1945300905033944
        ==> Saving latest model successfully !
            Average train loss is             0.038427574374,     best is           0.038267598689
            Average val evaluation index is   -3.194530090503,     best is           -3.135910626501
    Train use time   1549.42320
    Train loader use time     92.44415
    Val use time     46.49286
    Total use time   1597.65915
    End lr is 0.000050445347, 0.000050445347
    time: 19:29:46
Epoch: 117, iter: 58499
    Begin lr is 0.000050445347, 0.000050445347
========> pt_241:  2.7722537145018578
========> pt_242:  2.4052374809980392
========> pt_243:  4.459203779697418
========> pt_244:  2.5821764022111893
========> pt_245:  3.233284167945385
========> pt_246:  3.5225287824869156
========> pt_247:  2.382027357816696
========> pt_248:  2.2197340801358223
========> pt_249:  3.7740039452910423
========> pt_250:  2.2975677624344826
========> pt_251:  3.63146610558033
========> pt_252:  3.4017806500196457
========> pt_253:  3.554261401295662
========> pt_254:  3.5365767404437065
========> pt_255:  3.4036654978990555
========> pt_256:  2.142616994678974
========> pt_257:  2.752508446574211
========> pt_258:  2.2916899994015694
========> pt_259:  2.820882424712181
========> pt_260:  3.602924272418022
========> pt_261:  3.62489677965641
========> pt_262:  3.2501307129859924
========> pt_263:  2.681865245103836
========> pt_264:  3.7160276621580124
========> pt_265:  2.495913580060005
========> pt_266:  3.725197911262512
========> pt_267:  3.2298607751727104
========> pt_268:  3.8079100847244263
========> pt_269:  2.487921230494976
========> pt_270:  4.996275380253792
========> pt_271:  3.511083833873272
========> pt_272:  3.4071071445941925
========> pt_273:  2.4708689376711845
========> pt_274:  4.070615842938423
========> pt_275:  3.2234862446784973
========> pt_276:  2.2309837117791176
========> pt_277:  2.241327427327633
========> pt_278:  4.129127226769924
========> pt_279:  2.33281921595335
========> pt_280:  2.6593323424458504
========> pt_281:  2.894425392150879
========> pt_282:  2.558957412838936
========> pt_283:  5.823823735117912
========> pt_284:  2.7066856250166893
========> pt_285:  2.4678121879696846
========> pt_286:  2.8546949476003647
========> pt_287:  4.23820536583662
========> pt_288:  2.1384106390178204
========> pt_289:  3.6023177206516266
========> pt_290:  3.8484731689095497
========> pt_291:  2.259964682161808
========> pt_292:  2.7665647491812706
========> pt_293:  2.9090235754847527
========> pt_294:  2.556551806628704
========> pt_295:  3.117426782846451
========> pt_296:  1.901874616742134
========> pt_297:  4.605638831853867
========> pt_298:  3.1960928440093994
========> pt_299:  2.424021027982235
========> pt_300:  4.0913356095552444
========> pt_301:  3.295496143400669
========> pt_302:  4.924977123737335
========> pt_303:  3.1028546765446663
========> pt_304:  2.597779594361782
========> pt_305:  3.2300524413585663
========> pt_306:  2.3129672929644585
========> pt_307:  2.6810959726572037
========> pt_308:  3.386223614215851
========> pt_309:  2.1575945988297462
========> pt_310:  3.840990625321865
========> pt_311:  2.3054464161396027
========> pt_312:  2.252991162240505
========> pt_313:  2.303641363978386
========> pt_314:  3.0022646114230156
========> pt_315:  2.7907822281122208
========> pt_316:  3.94301388412714
========> pt_317:  3.971383608877659
========> pt_318:  5.447315201163292
========> pt_319:  2.1459573321044445
========> pt_320:  2.1452361717820168
========> pt_321:  4.194552414119244
========> pt_322:  3.0219947546720505
========> pt_323:  7.576537728309631
========> pt_324:  2.498718686401844
========> pt_325:  3.2068563997745514
========> pt_326:  2.688697427511215
========> pt_327:  2.624964378774166
========> pt_328:  2.5322894752025604
========> pt_329:  7.120734676718712
========> pt_330:  6.72567754983902
========> pt_331:  3.4228962659835815
========> pt_332:  2.131807804107666
========> pt_333:  2.114609871059656
========> pt_334:  1.8360071629285812
========> pt_335:  4.884048178792
========> pt_336:  3.5035814717411995
========> pt_337:  2.6744666695594788
========> pt_338:  2.263461612164974
========> pt_339:  2.6365459710359573
========> pt_340:  3.3248502761125565
===============================================> mean Dose score: 3.2086280485615135
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.037857495755,     best is           0.037857495755
            Average val evaluation index is   -3.208628048562,     best is           -3.135910626501
    Train use time   1548.65468
    Train loader use time     92.00987
    Val use time     45.80351
    Total use time   1597.64102
    End lr is 0.000048263828, 0.000048263828
    time: 19:56:24
Epoch: 118, iter: 58999
    Begin lr is 0.000048263828, 0.000048263828
========> pt_241:  2.91135773062706
========> pt_242:  2.40133348852396
========> pt_243:  4.384385645389557
========> pt_244:  2.5688160955905914
========> pt_245:  3.24523214250803
========> pt_246:  3.4691039845347404
========> pt_247:  2.461269460618496
========> pt_248:  2.176841925829649
========> pt_249:  3.833611086010933
========> pt_250:  2.3307471349835396
========> pt_251:  3.5803084447979927
========> pt_252:  3.38462196290493
========> pt_253:  3.571198955178261
========> pt_254:  3.5534871742129326
========> pt_255:  3.5091955959796906
========> pt_256:  2.1577240712940693
========> pt_257:  2.7517566457390785
========> pt_258:  2.2073474898934364
========> pt_259:  2.905915714800358
========> pt_260:  3.6302973330020905
========> pt_261:  3.571205735206604
========> pt_262:  3.277837820351124
========> pt_263:  2.8119153156876564
========> pt_264:  3.6302514374256134
========> pt_265:  2.5896362587809563
========> pt_266:  3.5823745280504227
========> pt_267:  3.2687515392899513
========> pt_268:  3.886164128780365
========> pt_269:  2.5697819888591766
========> pt_270:  4.709040001034737
========> pt_271:  3.5084857791662216
========> pt_272:  3.403908535838127
========> pt_273:  2.4263859540224075
========> pt_274:  4.14113100618124
========> pt_275:  3.31146702170372
========> pt_276:  2.1085657365620136
========> pt_277:  2.351175881922245
========> pt_278:  4.175809025764465
========> pt_279:  2.6150504127144814
========> pt_280:  2.847416326403618
========> pt_281:  2.8943974897265434
========> pt_282:  2.524987906217575
========> pt_283:  5.8875904232263565
========> pt_284:  2.7938752248883247
========> pt_285:  2.651059664785862
========> pt_286:  2.9508229717612267
========> pt_287:  4.013136327266693
========> pt_288:  2.0886765234172344
========> pt_289:  3.6831166222691536
========> pt_290:  3.8844722509384155
========> pt_291:  2.2160976380109787
========> pt_292:  2.9420222342014313
========> pt_293:  2.963494583964348
========> pt_294:  2.4555619806051254
========> pt_295:  3.1731335818767548
========> pt_296:  2.0122482627630234
========> pt_297:  4.427101910114288
========> pt_298:  3.1570127606391907
========> pt_299:  2.4164610356092453
========> pt_300:  4.114817455410957
========> pt_301:  3.2870320603251457
========> pt_302:  5.238413140177727
========> pt_303:  3.1016243621706963
========> pt_304:  2.605006843805313
========> pt_305:  3.1689320504665375
========> pt_306:  2.2705214470624924
========> pt_307:  2.6832588016986847
========> pt_308:  3.488655239343643
========> pt_309:  2.269476540386677
========> pt_310:  3.7831290811300278
========> pt_311:  2.231982722878456
========> pt_312:  2.2989754006266594
========> pt_313:  2.215392515063286
========> pt_314:  3.0180297419428825
========> pt_315:  2.79095146805048
========> pt_316:  3.888838067650795
========> pt_317:  4.00106605142355
========> pt_318:  5.0508081912994385
========> pt_319:  2.2484521940350533
========> pt_320:  2.145758494734764
========> pt_321:  4.078156277537346
========> pt_322:  3.1627677008509636
========> pt_323:  7.537657395005226
========> pt_324:  2.5631242617964745
========> pt_325:  3.2698047906160355
========> pt_326:  2.77344960719347
========> pt_327:  2.681448794901371
========> pt_328:  2.3466121405363083
========> pt_329:  7.865511402487755
========> pt_330:  7.057957351207733
========> pt_331:  3.632788732647896
========> pt_332:  2.212490662932396
========> pt_333:  2.1895726025104523
========> pt_334:  1.8707915768027306
========> pt_335:  4.7309113293886185
========> pt_336:  3.4914102777838707
========> pt_337:  2.6605133712291718
========> pt_338:  2.3266441747546196
========> pt_339:  2.615281455218792
========> pt_340:  3.3294909447431564
===============================================> mean Dose score: 3.2325368262827396
        ==> Saving latest model successfully !
            Average train loss is             0.037924501840,     best is           0.037857495755
            Average val evaluation index is   -3.232536826283,     best is           -3.135910626501
    Train use time   1548.04095
    Train loader use time     90.37558
    Val use time     46.42798
    Total use time   1596.26732
    End lr is 0.000046121550, 0.000046121550
    time: 20:23:00
Epoch: 119, iter: 59499
    Begin lr is 0.000046121550, 0.000046121550
========> pt_241:  2.621164955198765
========> pt_242:  2.3332489654421806
========> pt_243:  4.383016601204872
========> pt_244:  2.4718108400702477
========> pt_245:  3.21944247931242
========> pt_246:  3.4765461087226868
========> pt_247:  2.276582531630993
========> pt_248:  2.2635800018906593
========> pt_249:  3.711373433470726
========> pt_250:  2.308647371828556
========> pt_251:  3.6543111503124237
========> pt_252:  3.4223924577236176
========> pt_253:  3.5913095623254776
========> pt_254:  3.318546675145626
========> pt_255:  3.3616436645388603
========> pt_256:  2.1229863353073597
========> pt_257:  2.744273580610752
========> pt_258:  2.1846651658415794
========> pt_259:  2.6310695335268974
========> pt_260:  3.9991243556141853
========> pt_261:  3.677269369363785
========> pt_262:  3.173489533364773
========> pt_263:  2.706926055252552
========> pt_264:  3.511214219033718
========> pt_265:  2.489965669810772
========> pt_266:  3.573027215898037
========> pt_267:  3.159955032169819
========> pt_268:  3.770701289176941
========> pt_269:  2.4835705384612083
========> pt_270:  4.877390190958977
========> pt_271:  3.3288418874144554
========> pt_272:  3.186137154698372
========> pt_273:  2.4669240042567253
========> pt_274:  3.9918798953294754
========> pt_275:  3.2464520260691643
========> pt_276:  2.0502045564353466
========> pt_277:  2.2718894481658936
========> pt_278:  4.17764849960804
========> pt_279:  2.4493806809186935
========> pt_280:  2.66660887748003
========> pt_281:  2.813560776412487
========> pt_282:  2.516319379210472
========> pt_283:  6.689515486359596
========> pt_284:  2.6561055704951286
========> pt_285:  2.5262771546840668
========> pt_286:  2.8118879348039627
========> pt_287:  4.0208543464541435
========> pt_288:  2.1047651395201683
========> pt_289:  3.5864227265119553
========> pt_290:  3.7579936906695366
========> pt_291:  2.27211631834507
========> pt_292:  2.8927987068891525
========> pt_293:  2.8106458857655525
========> pt_294:  2.3759396746754646
========> pt_295:  3.180232271552086
========> pt_296:  1.9260670617222786
========> pt_297:  4.3840450793504715
========> pt_298:  3.118411973118782
========> pt_299:  2.377556189894676
========> pt_300:  4.296620264649391
========> pt_301:  3.2710684835910797
========> pt_302:  4.858010783791542
========> pt_303:  3.0174949020147324
========> pt_304:  2.5741877034306526
========> pt_305:  3.2456431165337563
========> pt_306:  2.255396507680416
========> pt_307:  2.5100186467170715
========> pt_308:  3.499321788549423
========> pt_309:  2.245004028081894
========> pt_310:  3.79381962120533
========> pt_311:  2.293606661260128
========> pt_312:  2.300625555217266
========> pt_313:  2.3089537769556046
========> pt_314:  3.0790643393993378
========> pt_315:  2.8431177884340286
========> pt_316:  3.8576428964734077
========> pt_317:  4.132368862628937
========> pt_318:  5.351402834057808
========> pt_319:  2.116532139480114
========> pt_320:  2.2024304047226906
========> pt_321:  3.6739307269454002
========> pt_322:  3.128723092377186
========> pt_323:  7.842644974589348
========> pt_324:  2.577498182654381
========> pt_325:  3.325886055827141
========> pt_326:  2.6919054239988327
========> pt_327:  2.5714822113513947
========> pt_328:  2.4090149998664856
========> pt_329:  7.270421534776688
========> pt_330:  6.738880351185799
========> pt_331:  3.3442607149481773
========> pt_332:  2.2229446843266487
========> pt_333:  2.1857905201613903
========> pt_334:  1.8980002216994762
========> pt_335:  4.812702462077141
========> pt_336:  3.561711087822914
========> pt_337:  2.7266037836670876
========> pt_338:  2.2762490063905716
========> pt_339:  2.5034983456134796
========> pt_340:  3.1263795495033264
===============================================> mean Dose score: 3.191175863146782
        ==> Saving latest model successfully !
            Average train loss is             0.037931685306,     best is           0.037857495755
            Average val evaluation index is   -3.191175863147,     best is           -3.135910626501
    Train use time   1550.18906
    Train loader use time     91.87246
    Val use time     46.54197
    Total use time   1598.45122
    End lr is 0.000044019338, 0.000044019338
    time: 20:49:39
Epoch: 120, iter: 59999
    Begin lr is 0.000044019338, 0.000044019338
========> pt_241:  2.8229328617453575
========> pt_242:  2.508251406252384
========> pt_243:  4.321840927004814
========> pt_244:  2.4750245735049248
========> pt_245:  3.1962211430072784
========> pt_246:  3.5207998752593994
========> pt_247:  2.365678884088993
========> pt_248:  2.2679322585463524
========> pt_249:  3.709673210978508
========> pt_250:  2.2017664834856987
========> pt_251:  3.62580556422472
========> pt_252:  3.383381739258766
========> pt_253:  3.643912672996521
========> pt_254:  3.445717841386795
========> pt_255:  3.6381496489048004
========> pt_256:  2.2026437148451805
========> pt_257:  2.957593612372875
========> pt_258:  2.1415583975613117
========> pt_259:  3.1349578499794006
========> pt_260:  3.786511793732643
========> pt_261:  3.7480416521430016
========> pt_262:  3.2435963302850723
========> pt_263:  2.7359675243496895
========> pt_264:  3.6332328245043755
========> pt_265:  2.475239187479019
========> pt_266:  3.4428290277719498
========> pt_267:  3.03612120449543
========> pt_268:  3.77997063100338
========> pt_269:  2.521653175354004
========> pt_270:  4.772879704833031
========> pt_271:  3.8999291509389877
========> pt_272:  3.7417124956846237
========> pt_273:  2.421945035457611
========> pt_274:  4.305617101490498
========> pt_275:  3.250005804002285
========> pt_276:  2.2171881794929504
========> pt_277:  2.283060848712921
========> pt_278:  4.245261810719967
========> pt_279:  2.4493128806352615
========> pt_280:  2.7227960154414177
========> pt_281:  2.8460227698087692
========> pt_282:  2.560107409954071
========> pt_283:  6.231485977768898
========> pt_284:  3.0432837828993797
========> pt_285:  2.616202235221863
========> pt_286:  2.814570739865303
========> pt_287:  3.9719218388199806
========> pt_288:  2.1642634086310863
========> pt_289:  3.6578138172626495
========> pt_290:  3.7510251253843307
========> pt_291:  2.3083876445889473
========> pt_292:  2.7807994186878204
========> pt_293:  2.792283222079277
========> pt_294:  2.4389295279979706
========> pt_295:  3.1758956611156464
========> pt_296:  1.9629864022135735
========> pt_297:  4.355441443622112
========> pt_298:  3.102552443742752
========> pt_299:  2.379588633775711
========> pt_300:  4.329351112246513
========> pt_301:  3.2517733052372932
========> pt_302:  5.098576098680496
========> pt_303:  2.9690351709723473
========> pt_304:  2.55972433835268
========> pt_305:  3.2129962369799614
========> pt_306:  2.2212528064846992
========> pt_307:  2.6059409230947495
========> pt_308:  3.3140363916754723
========> pt_309:  2.235073111951351
========> pt_310:  3.774656392633915
========> pt_311:  2.3093000799417496
========> pt_312:  2.252153307199478
========> pt_313:  2.2211265936493874
========> pt_314:  3.0049873143434525
========> pt_315:  2.8033119812607765
========> pt_316:  3.875138759613037
========> pt_317:  4.1795604676008224
========> pt_318:  5.046574845910072
========> pt_319:  2.1918749436736107
========> pt_320:  2.2114814817905426
========> pt_321:  3.5888713598251343
========> pt_322:  2.958347760140896
========> pt_323:  7.470644116401672
========> pt_324:  2.6577189564704895
========> pt_325:  3.471161462366581
========> pt_326:  2.702821008861065
========> pt_327:  2.716108039021492
========> pt_328:  2.249540649354458
========> pt_329:  7.028317153453827
========> pt_330:  7.212395966053009
========> pt_331:  3.484742119908333
========> pt_332:  2.208005413413048
========> pt_333:  2.181767225265503
========> pt_334:  1.8860645033419132
========> pt_335:  4.664866030216217
========> pt_336:  3.552965894341469
========> pt_337:  2.6403550431132317
========> pt_338:  2.253122851252556
========> pt_339:  2.559206187725067
========> pt_340:  3.245854601264
===============================================> mean Dose score: 3.216310785524547
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.037817231748,     best is           0.037817231748
            Average val evaluation index is   -3.216310785525,     best is           -3.135910626501
    Train use time   1548.25238
    Train loader use time     92.95743
    Val use time     46.02864
    Total use time   1597.98957
    End lr is 0.000041958003, 0.000041958003
    time: 21:16:17
Epoch: 121, iter: 60499
    Begin lr is 0.000041958003, 0.000041958003
========> pt_241:  2.7283261716365814
========> pt_242:  2.383161447942257
========> pt_243:  4.261609241366386
========> pt_244:  2.443152703344822
========> pt_245:  3.251538872718811
========> pt_246:  3.5560307279229164
========> pt_247:  2.4118388816714287
========> pt_248:  2.1589693799614906
========> pt_249:  3.521977514028549
========> pt_250:  2.237529568374157
========> pt_251:  3.467562310397625
========> pt_252:  3.287360370159149
========> pt_253:  3.5567379370331764
========> pt_254:  3.3301418274641037
========> pt_255:  3.422151766717434
========> pt_256:  2.07462752237916
========> pt_257:  2.6538459956645966
========> pt_258:  2.1544636599719524
========> pt_259:  2.852211631834507
========> pt_260:  3.780447319149971
========> pt_261:  3.5848938301205635
========> pt_262:  3.1599093973636627
========> pt_263:  2.664734460413456
========> pt_264:  3.602871336042881
========> pt_265:  2.436884045600891
========> pt_266:  3.5367120802402496
========> pt_267:  3.175508938729763
========> pt_268:  3.7567756325006485
========> pt_269:  2.4500998854637146
========> pt_270:  4.870327487587929
========> pt_271:  3.4007487818598747
========> pt_272:  3.2976005598902702
========> pt_273:  2.4166401848196983
========> pt_274:  4.214764200150967
========> pt_275:  3.2888024300336838
========> pt_276:  2.081320323050022
========> pt_277:  2.2590819746255875
========> pt_278:  4.059131257236004
========> pt_279:  2.4359721317887306
========> pt_280:  2.7465688809752464
========> pt_281:  2.8455614671111107
========> pt_282:  2.5734765827655792
========> pt_283:  6.482698023319244
========> pt_284:  2.773395888507366
========> pt_285:  2.519693747162819
========> pt_286:  2.873658947646618
========> pt_287:  3.9692705869674683
========> pt_288:  2.084325049072504
========> pt_289:  3.4766895323991776
========> pt_290:  3.690664879977703
========> pt_291:  2.159434463828802
========> pt_292:  2.7238988131284714
========> pt_293:  2.887246645987034
========> pt_294:  2.540498785674572
========> pt_295:  2.966338023543358
========> pt_296:  1.9125130027532578
========> pt_297:  4.211680851876736
========> pt_298:  3.1826603040099144
========> pt_299:  2.3582977801561356
========> pt_300:  4.239411950111389
========> pt_301:  3.1821875274181366
========> pt_302:  4.990541040897369
========> pt_303:  3.021763451397419
========> pt_304:  2.554728239774704
========> pt_305:  3.2554233074188232
========> pt_306:  2.214355953037739
========> pt_307:  2.645716480910778
========> pt_308:  3.2581379264593124
========> pt_309:  2.208857089281082
========> pt_310:  3.7773165106773376
========> pt_311:  2.2622928395867348
========> pt_312:  2.2413592413067818
========> pt_313:  2.1992769092321396
========> pt_314:  3.0096042528748512
========> pt_315:  2.7766993269324303
========> pt_316:  3.7564535811543465
========> pt_317:  4.184830896556377
========> pt_318:  5.2251675724983215
========> pt_319:  2.1585642732679844
========> pt_320:  2.164623662829399
========> pt_321:  3.7611098960042
========> pt_322:  3.0520975589752197
========> pt_323:  7.59928472340107
========> pt_324:  2.569633089005947
========> pt_325:  3.114217221736908
========> pt_326:  2.6608969643712044
========> pt_327:  2.592792622745037
========> pt_328:  2.347639836370945
========> pt_329:  7.482509687542915
========> pt_330:  7.01395183801651
========> pt_331:  3.3849411457777023
========> pt_332:  2.2033707424998283
========> pt_333:  2.122461535036564
========> pt_334:  1.8090536817908287
========> pt_335:  4.749655500054359
========> pt_336:  3.5690760239958763
========> pt_337:  2.6250162720680237
========> pt_338:  2.220580279827118
========> pt_339:  2.4782656878232956
========> pt_340:  3.0679745599627495
===============================================> mean Dose score: 3.1695687694475057
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.037809184212,     best is           0.037809184212
            Average val evaluation index is   -3.169568769448,     best is           -3.135910626501
    Train use time   1553.52798
    Train loader use time     96.49838
    Val use time     45.55510
    Total use time   1602.50156
    End lr is 0.000039938340, 0.000039938340
    time: 21:42:59
Epoch: 122, iter: 60999
    Begin lr is 0.000039938340, 0.000039938340
========> pt_241:  2.5156033039093018
========> pt_242:  2.255854420363903
========> pt_243:  4.375707730650902
========> pt_244:  2.466764934360981
========> pt_245:  3.2094641029834747
========> pt_246:  3.4684139862656593
========> pt_247:  2.2514213249087334
========> pt_248:  2.1596257388591766
========> pt_249:  4.0417201444506645
========> pt_250:  2.264149785041809
========> pt_251:  3.744383826851845
========> pt_252:  3.489154353737831
========> pt_253:  3.560604117810726
========> pt_254:  3.5030028223991394
========> pt_255:  3.304186835885048
========> pt_256:  2.0819528214633465
========> pt_257:  2.6876939833164215
========> pt_258:  2.188781164586544
========> pt_259:  2.8096168860793114
========> pt_260:  3.489915020763874
========> pt_261:  3.7884341925382614
========> pt_262:  3.3675196021795273
========> pt_263:  2.6564831659197807
========> pt_264:  3.775297626852989
========> pt_265:  2.5013738498091698
========> pt_266:  3.7282744795084
========> pt_267:  3.175465650856495
========> pt_268:  3.746143765747547
========> pt_269:  2.407652996480465
========> pt_270:  5.031967014074326
========> pt_271:  3.342941999435425
========> pt_272:  3.294246271252632
========> pt_273:  2.4572786316275597
========> pt_274:  4.220249503850937
========> pt_275:  3.35982296615839
========> pt_276:  2.0998690463602543
========> pt_277:  2.326377145946026
========> pt_278:  4.499452635645866
========> pt_279:  2.3839176818728447
========> pt_280:  2.6170440018177032
========> pt_281:  2.919655963778496
========> pt_282:  2.629103846848011
========> pt_283:  5.610847398638725
========> pt_284:  2.669590786099434
========> pt_285:  2.425038553774357
========> pt_286:  2.8916586190462112
========> pt_287:  4.397204592823982
========> pt_288:  2.109942864626646
========> pt_289:  3.7291571870446205
========> pt_290:  3.7366514652967453
========> pt_291:  2.2678428143262863
========> pt_292:  2.8985174000263214
========> pt_293:  2.8564580157399178
========> pt_294:  2.369978465139866
========> pt_295:  3.0364028364419937
========> pt_296:  1.8951095826923847
========> pt_297:  4.588884860277176
========> pt_298:  3.0194809287786484
========> pt_299:  2.357173338532448
========> pt_300:  4.011082239449024
========> pt_301:  3.2874800637364388
========> pt_302:  4.9834804236888885
========> pt_303:  3.0970361083745956
========> pt_304:  2.5579362362623215
========> pt_305:  3.306255526840687
========> pt_306:  2.2267545387148857
========> pt_307:  2.541251629590988
========> pt_308:  3.720015101134777
========> pt_309:  2.1987496316432953
========> pt_310:  3.5421058535575867
========> pt_311:  2.2736775502562523
========> pt_312:  2.2518302127718925
========> pt_313:  2.3015257343649864
========> pt_314:  3.1283392384648323
========> pt_315:  2.8758804500102997
========> pt_316:  4.007380865514278
========> pt_317:  3.919556550681591
========> pt_318:  5.177357420325279
========> pt_319:  2.18667009845376
========> pt_320:  2.270054407417774
========> pt_321:  3.9647363126277924
========> pt_322:  3.162490241229534
========> pt_323:  7.560282871127129
========> pt_324:  2.6260171085596085
========> pt_325:  3.2397306710481644
========> pt_326:  2.796146534383297
========> pt_327:  2.643675170838833
========> pt_328:  2.4603502452373505
========> pt_329:  7.625738307833672
========> pt_330:  6.515890434384346
========> pt_331:  3.4676136821508408
========> pt_332:  2.2547511011362076
========> pt_333:  2.116968408226967
========> pt_334:  1.823161095380783
========> pt_335:  5.194627717137337
========> pt_336:  3.5844661667943
========> pt_337:  2.7410342916846275
========> pt_338:  2.280070073902607
========> pt_339:  2.4553849175572395
========> pt_340:  3.145693764090538
===============================================> mean Dose score: 3.2057977804914115
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.037369876105,     best is           0.037369876105
            Average val evaluation index is   -3.205797780491,     best is           -3.135910626501
    Train use time   1555.54142
    Train loader use time     99.71429
    Val use time     48.51211
    Total use time   1607.65462
    End lr is 0.000037961126, 0.000037961126
    time: 22:09:47
Epoch: 123, iter: 61499
    Begin lr is 0.000037961126, 0.000037961126
========> pt_241:  2.579571306705475
========> pt_242:  2.314177267253399
========> pt_243:  4.385977387428284
========> pt_244:  2.536972649395466
========> pt_245:  3.3155107870697975
========> pt_246:  3.5753631964325905
========> pt_247:  2.302727624773979
========> pt_248:  2.12780537083745
========> pt_249:  4.181182458996773
========> pt_250:  2.273382358253002
========> pt_251:  3.6508387327194214
========> pt_252:  3.4682267531752586
========> pt_253:  3.621770143508911
========> pt_254:  3.7127017974853516
========> pt_255:  3.384959138929844
========> pt_256:  2.0883793756365776
========> pt_257:  2.3618948459625244
========> pt_258:  2.280600219964981
========> pt_259:  2.5804876536130905
========> pt_260:  3.6190007627010345
========> pt_261:  3.7132804468274117
========> pt_262:  3.2258623838424683
========> pt_263:  2.7151916921138763
========> pt_264:  3.8632890954613686
========> pt_265:  2.5072041526436806
========> pt_266:  3.7402313202619553
========> pt_267:  3.3167528361082077
========> pt_268:  3.882714658975601
========> pt_269:  2.5174983218312263
========> pt_270:  4.9548546224832535
========> pt_271:  3.2162941992282867
========> pt_272:  3.039845786988735
========> pt_273:  2.4326274916529655
========> pt_274:  4.144812040030956
========> pt_275:  3.265487216413021
========> pt_276:  2.1207468397915363
========> pt_277:  2.3336591571569443
========> pt_278:  4.643518850207329
========> pt_279:  2.3376765847206116
========> pt_280:  2.7273986116051674
========> pt_281:  2.843470349907875
========> pt_282:  2.519417330622673
========> pt_283:  6.089593544602394
========> pt_284:  2.4851588904857635
========> pt_285:  2.5246624648571014
========> pt_286:  2.8764914348721504
========> pt_287:  4.2628296464681625
========> pt_288:  2.0258540846407413
========> pt_289:  3.9099862799048424
========> pt_290:  3.9013034105300903
========> pt_291:  2.136866357177496
========> pt_292:  2.9543394595384598
========> pt_293:  3.005545362830162
========> pt_294:  2.471425421535969
========> pt_295:  3.0552121996879578
========> pt_296:  1.8865878693759441
========> pt_297:  4.507053047418594
========> pt_298:  3.156239837408066
========> pt_299:  2.337002754211426
========> pt_300:  4.017850011587143
========> pt_301:  3.2484977692365646
========> pt_302:  5.113053545355797
========> pt_303:  3.0262747779488564
========> pt_304:  2.5915831699967384
========> pt_305:  3.4037265181541443
========> pt_306:  2.1769454516470432
========> pt_307:  2.5920186564326286
========> pt_308:  3.529699184000492
========> pt_309:  2.3003775626420975
========> pt_310:  3.6979638412594795
========> pt_311:  2.2356105595827103
========> pt_312:  2.324039861559868
========> pt_313:  2.366655468940735
========> pt_314:  3.108460195362568
========> pt_315:  2.8278880193829536
========> pt_316:  4.018572345376015
========> pt_317:  3.7714356184005737
========> pt_318:  5.8053335547447205
========> pt_319:  2.207280993461609
========> pt_320:  2.1585827879607677
========> pt_321:  4.622157067060471
========> pt_322:  3.117339424788952
========> pt_323:  8.090049251914024
========> pt_324:  2.56813183426857
========> pt_325:  3.2400694116950035
========> pt_326:  2.799360789358616
========> pt_327:  2.5849629938602448
========> pt_328:  2.4894988909363747
========> pt_329:  7.5619987398386
========> pt_330:  6.504251733422279
========> pt_331:  3.681623190641403
========> pt_332:  2.202974632382393
========> pt_333:  2.137573305517435
========> pt_334:  1.8149936385452747
========> pt_335:  4.889298006892204
========> pt_336:  3.517167866230011
========> pt_337:  2.833127938210964
========> pt_338:  2.2966748848557472
========> pt_339:  2.509041279554367
========> pt_340:  3.3189378306269646
===============================================> mean Dose score: 3.233105984888971
        ==> Saving latest model successfully !
            Average train loss is             0.037630011018,     best is           0.037369876105
            Average val evaluation index is   -3.233105984889,     best is           -3.135910626501
    Train use time   1542.71349
    Train loader use time     86.35383
    Val use time     48.35991
    Total use time   1592.84923
    End lr is 0.000036027125, 0.000036027125
    time: 22:36:20
Epoch: 124, iter: 61999
    Begin lr is 0.000036027125, 0.000036027125
========> pt_241:  2.5363530591130257
========> pt_242:  2.305561415851116
========> pt_243:  4.254871197044849
========> pt_244:  2.4950454756617546
========> pt_245:  3.2639210298657417
========> pt_246:  3.428150527179241
========> pt_247:  2.2781648859381676
========> pt_248:  2.157571390271187
========> pt_249:  3.8488220795989037
========> pt_250:  2.344208098948002
========> pt_251:  3.592282496392727
========> pt_252:  3.4248796850442886
========> pt_253:  3.5100803896784782
========> pt_254:  3.456689231097698
========> pt_255:  3.1855491176247597
========> pt_256:  2.0693990774452686
========> pt_257:  2.4435222148895264
========> pt_258:  2.257452942430973
========> pt_259:  2.5945963710546494
========> pt_260:  3.94206702709198
========> pt_261:  3.5615504533052444
========> pt_262:  3.184645287692547
========> pt_263:  2.726930007338524
========> pt_264:  3.630620166659355
========> pt_265:  2.4862713366746902
========> pt_266:  3.708101026713848
========> pt_267:  3.3352886512875557
========> pt_268:  3.751583695411682
========> pt_269:  2.466879151761532
========> pt_270:  4.960689097642899
========> pt_271:  3.101898171007633
========> pt_272:  3.1476229429244995
========> pt_273:  2.398792803287506
========> pt_274:  4.039579480886459
========> pt_275:  3.2071854919195175
========> pt_276:  2.000374738126993
========> pt_277:  2.274341732263565
========> pt_278:  4.331624507904053
========> pt_279:  2.2196365520358086
========> pt_280:  2.5543467327952385
========> pt_281:  2.7367933839559555
========> pt_282:  2.5046204403042793
========> pt_283:  6.560626104474068
========> pt_284:  2.482898533344269
========> pt_285:  2.4512342363595963
========> pt_286:  2.930331639945507
========> pt_287:  4.1954366862773895
========> pt_288:  2.069164253771305
========> pt_289:  3.6394112557172775
========> pt_290:  3.6686287447810173
========> pt_291:  2.198396287858486
========> pt_292:  2.8850465267896652
========> pt_293:  2.9102033004164696
========> pt_294:  2.4738185107707977
========> pt_295:  3.043164350092411
========> pt_296:  1.902337484061718
========> pt_297:  4.477824866771698
========> pt_298:  3.0491865798830986
========> pt_299:  2.353169731795788
========> pt_300:  3.871457204222679
========> pt_301:  3.207421228289604
========> pt_302:  5.080452039837837
========> pt_303:  3.0396828055381775
========> pt_304:  2.595231346786022
========> pt_305:  3.2592399418354034
========> pt_306:  2.2406616806983948
========> pt_307:  2.6209453865885735
========> pt_308:  3.566027879714966
========> pt_309:  2.25540354847908
========> pt_310:  3.5311495885252953
========> pt_311:  2.2271136194467545
========> pt_312:  2.282150760293007
========> pt_313:  2.1923719719052315
========> pt_314:  3.151676617562771
========> pt_315:  2.8371411934494972
========> pt_316:  3.900928683578968
========> pt_317:  3.7668296322226524
========> pt_318:  5.377076715230942
========> pt_319:  2.192249149084091
========> pt_320:  2.0687601901590824
========> pt_321:  3.893256299197674
========> pt_322:  3.1316179037094116
========> pt_323:  7.925315424799919
========> pt_324:  2.599267028272152
========> pt_325:  3.188750073313713
========> pt_326:  2.6512515917420387
========> pt_327:  2.5733138620853424
========> pt_328:  2.488337680697441
========> pt_329:  7.379731237888336
========> pt_330:  6.679646372795105
========> pt_331:  3.4834593906998634
========> pt_332:  2.209733799099922
========> pt_333:  2.152804508805275
========> pt_334:  1.803407482802868
========> pt_335:  4.990178048610687
========> pt_336:  3.493448719382286
========> pt_337:  2.795702703297138
========> pt_338:  2.292025089263916
========> pt_339:  2.4867402017116547
========> pt_340:  3.1874214485287666
===============================================> mean Dose score: 3.1767882270738483
        ==> Saving latest model successfully !
            Average train loss is             0.037408197504,     best is           0.037369876105
            Average val evaluation index is   -3.176788227074,     best is           -3.135910626501
    Train use time   1541.97680
    Train loader use time     85.44257
    Val use time     47.85529
    Total use time   1591.62425
    End lr is 0.000034137083, 0.000034137083
    time: 23:02:51
Epoch: 125, iter: 62499
    Begin lr is 0.000034137083, 0.000034137083
========> pt_241:  2.764230854809284
========> pt_242:  2.239820174872875
========> pt_243:  4.632277563214302
========> pt_244:  2.5900910422205925
========> pt_245:  3.352643959224224
========> pt_246:  3.5397638753056526
========> pt_247:  2.2782525047659874
========> pt_248:  2.3914242163300514
========> pt_249:  4.098187610507011
========> pt_250:  2.3272736743092537
========> pt_251:  3.7169119343161583
========> pt_252:  3.4929730743169785
========> pt_253:  3.7674343585968018
========> pt_254:  3.650358133018017
========> pt_255:  3.6027633771300316
========> pt_256:  2.0927403680980206
========> pt_257:  2.6396139338612556
========> pt_258:  2.2711918875575066
========> pt_259:  2.9458511248230934
========> pt_260:  3.5951877385377884
========> pt_261:  3.947777897119522
========> pt_262:  3.269347660243511
========> pt_263:  2.7812234312295914
========> pt_264:  3.960174396634102
========> pt_265:  2.50359196215868
========> pt_266:  3.615315295755863
========> pt_267:  3.3310826867818832
========> pt_268:  3.9983029291033745
========> pt_269:  2.5090592727065086
========> pt_270:  5.23695282638073
========> pt_271:  3.3677660301327705
========> pt_272:  3.2245971262454987
========> pt_273:  2.500186823308468
========> pt_274:  4.194265827536583
========> pt_275:  3.370075151324272
========> pt_276:  2.2001173719763756
========> pt_277:  2.3384617641568184
========> pt_278:  4.631423801183701
========> pt_279:  2.387782819569111
========> pt_280:  2.6484470069408417
========> pt_281:  2.7343950793147087
========> pt_282:  2.615225650370121
========> pt_283:  6.16680346429348
========> pt_284:  2.6099708676338196
========> pt_285:  2.645241878926754
========> pt_286:  2.895013689994812
========> pt_287:  4.257502369582653
========> pt_288:  2.17770142480731
========> pt_289:  3.770158365368843
========> pt_290:  3.896797299385071
========> pt_291:  2.155059389770031
========> pt_292:  3.0047964304685593
========> pt_293:  3.028557300567627
========> pt_294:  2.5580843538045883
========> pt_295:  3.2056665048003197
========> pt_296:  1.890382468700409
========> pt_297:  4.6300724893808365
========> pt_298:  3.2554616406559944
========> pt_299:  2.460763305425644
========> pt_300:  4.254445880651474
========> pt_301:  3.4127700328826904
========> pt_302:  5.352732762694359
========> pt_303:  3.0757582932710648
========> pt_304:  2.6928992196917534
========> pt_305:  3.399656154215336
========> pt_306:  2.245715409517288
========> pt_307:  2.5794054567813873
========> pt_308:  3.60561802983284
========> pt_309:  2.310476154088974
========> pt_310:  3.7399958446621895
========> pt_311:  2.2178320214152336
========> pt_312:  2.331889569759369
========> pt_313:  2.3186051473021507
========> pt_314:  3.19803636521101
========> pt_315:  2.8339412808418274
========> pt_316:  4.126256667077541
========> pt_317:  3.900677040219307
========> pt_318:  5.879545137286186
========> pt_319:  2.1698269434273243
========> pt_320:  2.2029855847358704
========> pt_321:  4.389622434973717
========> pt_322:  3.070893883705139
========> pt_323:  7.840915024280548
========> pt_324:  2.641536593437195
========> pt_325:  3.2562872394919395
========> pt_326:  2.7035297825932503
========> pt_327:  2.6247573271393776
========> pt_328:  2.688036374747753
========> pt_329:  7.108051851391792
========> pt_330:  6.870987638831139
========> pt_331:  3.5224343836307526
========> pt_332:  2.238268330693245
========> pt_333:  2.3036126792430878
========> pt_334:  1.8318914249539375
========> pt_335:  4.993071034550667
========> pt_336:  3.5503722727298737
========> pt_337:  2.8446266055107117
========> pt_338:  2.3220840841531754
========> pt_339:  2.5698307529091835
========> pt_340:  3.264652229845524
===============================================> mean Dose score: 3.2844712840393187
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.037196186285,     best is           0.037196186285
            Average val evaluation index is   -3.284471284039,     best is           -3.135910626501
    Train use time   1537.95783
    Train loader use time     84.50748
    Val use time     44.26450
    Total use time   1585.36627
    End lr is 0.000032291726, 0.000032291726
    time: 23:29:17
Epoch: 126, iter: 62999
    Begin lr is 0.000032291726, 0.000032291726
========> pt_241:  2.7935782074928284
========> pt_242:  2.3886318877339363
========> pt_243:  4.4324784725904465
========> pt_244:  2.4908050894737244
========> pt_245:  3.2420679554343224
========> pt_246:  3.5785944014787674
========> pt_247:  2.377449795603752
========> pt_248:  2.220705971121788
========> pt_249:  3.940274491906166
========> pt_250:  2.2092007845640182
========> pt_251:  3.7602759525179863
========> pt_252:  3.3322788402438164
========> pt_253:  3.4947384893894196
========> pt_254:  3.73749453574419
========> pt_255:  3.541649505496025
========> pt_256:  2.106029484421015
========> pt_257:  2.664211615920067
========> pt_258:  2.1836203895509243
========> pt_259:  2.9022490233182907
========> pt_260:  3.7376249209046364
========> pt_261:  3.692872039973736
========> pt_262:  3.1891952082514763
========> pt_263:  2.6902996003627777
========> pt_264:  3.7964800000190735
========> pt_265:  2.44953453540802
========> pt_266:  3.542921543121338
========> pt_267:  3.2400785386562347
========> pt_268:  3.9064953476190567
========> pt_269:  2.4702230095863342
========> pt_270:  4.845706075429916
========> pt_271:  3.353732153773308
========> pt_272:  3.308381587266922
========> pt_273:  2.4323638528585434
========> pt_274:  4.176487028598785
========> pt_275:  3.353031985461712
========> pt_276:  2.1522299014031887
========> pt_277:  2.1949945390224457
========> pt_278:  4.340022876858711
========> pt_279:  2.293565720319748
========> pt_280:  2.714402861893177
========> pt_281:  2.7859115600585938
========> pt_282:  2.53074049949646
========> pt_283:  6.192449182271957
========> pt_284:  2.5934701040387154
========> pt_285:  2.6246311143040657
========> pt_286:  2.800689935684204
========> pt_287:  4.122369103133678
========> pt_288:  2.0735858753323555
========> pt_289:  3.721644915640354
========> pt_290:  3.792366348206997
========> pt_291:  2.1805671602487564
========> pt_292:  2.858675867319107
========> pt_293:  2.9578642919659615
========> pt_294:  2.4886320903897285
========> pt_295:  3.063260093331337
========> pt_296:  1.9020828418433666
========> pt_297:  4.502357617020607
========> pt_298:  3.110361471772194
========> pt_299:  2.3413200676441193
========> pt_300:  4.034266024827957
========> pt_301:  3.2201682031154633
========> pt_302:  5.103973001241684
========> pt_303:  3.0627865344285965
========> pt_304:  2.5323189422488213
========> pt_305:  3.274153396487236
========> pt_306:  2.201637402176857
========> pt_307:  2.6437659189105034
========> pt_308:  3.4129150211811066
========> pt_309:  2.2671176120638847
========> pt_310:  3.706199489533901
========> pt_311:  2.252991683781147
========> pt_312:  2.25527785718441
========> pt_313:  2.3332348838448524
========> pt_314:  3.0138183012604713
========> pt_315:  2.7293583005666733
========> pt_316:  3.809121362864971
========> pt_317:  3.8841084763407707
========> pt_318:  5.3146842867136
========> pt_319:  2.185679953545332
========> pt_320:  2.127139884978533
========> pt_321:  4.112314581871033
========> pt_322:  2.980594076216221
========> pt_323:  7.732924297451973
========> pt_324:  2.58558701723814
========> pt_325:  3.1781213358044624
========> pt_326:  2.7636368200182915
========> pt_327:  2.614765390753746
========> pt_328:  2.505527138710022
========> pt_329:  6.86017818748951
========> pt_330:  6.905168890953064
========> pt_331:  3.5469770431518555
========> pt_332:  2.214409150183201
========> pt_333:  2.1791244484484196
========> pt_334:  1.7618831992149353
========> pt_335:  4.909597933292389
========> pt_336:  3.4018583595752716
========> pt_337:  2.696555219590664
========> pt_338:  2.289886251091957
========> pt_339:  2.520044222474098
========> pt_340:  3.1290607899427414
===============================================> mean Dose score: 3.2017288925126195
        ==> Saving latest model successfully !
            Average train loss is             0.037404639803,     best is           0.037196186285
            Average val evaluation index is   -3.201728892513,     best is           -3.135910626501
    Train use time   1541.54575
    Train loader use time     87.09379
    Val use time     48.96157
    Total use time   1592.57077
    End lr is 0.000030491768, 0.000030491768
    time: 23:55:49
Epoch: 127, iter: 63499
    Begin lr is 0.000030491768, 0.000030491768
========> pt_241:  2.629835307598114
========> pt_242:  2.234039679169655
========> pt_243:  4.5120155066251755
========> pt_244:  2.508465498685837
========> pt_245:  3.228595517575741
========> pt_246:  3.504558317363262
========> pt_247:  2.2402050718665123
========> pt_248:  2.301684282720089
========> pt_249:  3.8969647139310837
========> pt_250:  2.306611016392708
========> pt_251:  3.7479138746857643
========> pt_252:  3.5376664996147156
========> pt_253:  3.658374734222889
========> pt_254:  3.492967337369919
========> pt_255:  3.3508793264627457
========> pt_256:  2.0935001224279404
========> pt_257:  2.5552860274910927
========> pt_258:  2.246875576674938
========> pt_259:  2.7355987951159477
========> pt_260:  3.670334704220295
========> pt_261:  3.8007183000445366
========> pt_262:  3.18789578974247
========> pt_263:  2.698756121098995
========> pt_264:  3.6927351355552673
========> pt_265:  2.499978207051754
========> pt_266:  3.7443882599473
========> pt_267:  3.3187730237841606
========> pt_268:  3.9249753579497337
========> pt_269:  2.444698289036751
========> pt_270:  5.163311809301376
========> pt_271:  3.226677291095257
========> pt_272:  3.1490563973784447
========> pt_273:  2.430287078022957
========> pt_274:  4.160673916339874
========> pt_275:  3.3464358001947403
========> pt_276:  2.105664014816284
========> pt_277:  2.28911854326725
========> pt_278:  4.288531690835953
========> pt_279:  2.2999392077326775
========> pt_280:  2.5850041955709457
========> pt_281:  2.783450670540333
========> pt_282:  2.6207464188337326
========> pt_283:  5.972755923867226
========> pt_284:  2.4848902970552444
========> pt_285:  2.4818121641874313
========> pt_286:  2.872322238981724
========> pt_287:  4.313610754907131
========> pt_288:  2.0857886224985123
========> pt_289:  3.4501981362700462
========> pt_290:  3.8115327060222626
========> pt_291:  2.1441499330103397
========> pt_292:  2.950006239116192
========> pt_293:  2.9524564370512962
========> pt_294:  2.5028234720230103
========> pt_295:  3.133443556725979
========> pt_296:  1.874702349305153
========> pt_297:  4.634990096092224
========> pt_298:  3.160371743142605
========> pt_299:  2.450743205845356
========> pt_300:  4.022208526730537
========> pt_301:  3.2300451397895813
========> pt_302:  4.939451441168785
========> pt_303:  3.091491609811783
========> pt_304:  2.6354781165719032
========> pt_305:  3.2812948524951935
========> pt_306:  2.217419482767582
========> pt_307:  2.5651102885603905
========> pt_308:  3.5316799953579903
========> pt_309:  2.240399084985256
========> pt_310:  3.7744391709566116
========> pt_311:  2.2360337898135185
========> pt_312:  2.3463688418269157
========> pt_313:  2.345118708908558
========> pt_314:  3.1381885334849358
========> pt_315:  2.8386536613106728
========> pt_316:  3.905271552503109
========> pt_317:  3.8856858760118484
========> pt_318:  5.68988636136055
========> pt_319:  2.1524559892714024
========> pt_320:  2.2852489724755287
========> pt_321:  3.826065957546234
========> pt_322:  3.1654085218906403
========> pt_323:  7.896893545985222
========> pt_324:  2.604593262076378
========> pt_325:  3.226732835173607
========> pt_326:  2.7926352620124817
========> pt_327:  2.6174523681402206
========> pt_328:  2.5907056778669357
========> pt_329:  7.414672896265984
========> pt_330:  6.546551808714867
========> pt_331:  3.4893668815493584
========> pt_332:  2.2103912010788918
========> pt_333:  2.2292540222406387
========> pt_334:  1.8463650904595852
========> pt_335:  4.9864740669727325
========> pt_336:  3.5601957514882088
========> pt_337:  2.744622491300106
========> pt_338:  2.2506486624479294
========> pt_339:  2.473962716758251
========> pt_340:  3.1264619529247284
===============================================> mean Dose score: 3.214421702735126
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.037135743655,     best is           0.037135743655
            Average val evaluation index is   -3.214421702735,     best is           -3.135910626501
    Train use time   1554.93368
    Train loader use time     98.60761
    Val use time     49.59644
    Total use time   1608.25763
    End lr is 0.000028737902, 0.000028737902
    time: 00:22:38
Epoch: 128, iter: 63999
    Begin lr is 0.000028737902, 0.000028737902
========> pt_241:  2.6038427650928497
========> pt_242:  2.2875943407416344
========> pt_243:  4.330396801233292
========> pt_244:  2.4651773646473885
========> pt_245:  3.1391604244709015
========> pt_246:  3.4261444211006165
========> pt_247:  2.3285000771284103
========> pt_248:  2.1869036182761192
========> pt_249:  3.7702540680766106
========> pt_250:  2.275524325668812
========> pt_251:  3.67160152643919
========> pt_252:  3.481637127697468
========> pt_253:  3.4954219684004784
========> pt_254:  3.5422182455658913
========> pt_255:  3.4862663224339485
========> pt_256:  2.0894821733236313
========> pt_257:  2.4944211915135384
========> pt_258:  2.2325775399804115
========> pt_259:  2.8513510897755623
========> pt_260:  3.715439885854721
========> pt_261:  3.57457984238863
========> pt_262:  3.2082851603627205
========> pt_263:  2.6669470965862274
========> pt_264:  3.65386001765728
========> pt_265:  2.4814632534980774
========> pt_266:  3.6027516424655914
========> pt_267:  3.2359064742922783
========> pt_268:  3.820558227598667
========> pt_269:  2.4531323835253716
========> pt_270:  4.879285469651222
========> pt_271:  3.314993418753147
========> pt_272:  3.2126744464039803
========> pt_273:  2.4474775791168213
========> pt_274:  4.156706556677818
========> pt_275:  3.2129696384072304
========> pt_276:  2.071899864822626
========> pt_277:  2.256653942167759
========> pt_278:  4.171113595366478
========> pt_279:  2.270563170313835
========> pt_280:  2.6555417850613594
========> pt_281:  2.864663675427437
========> pt_282:  2.5434522703289986
========> pt_283:  6.172981634736061
========> pt_284:  2.478480562567711
========> pt_285:  2.458045296370983
========> pt_286:  2.770415022969246
========> pt_287:  4.1357361897826195
========> pt_288:  2.0218963734805584
========> pt_289:  3.3791176229715347
========> pt_290:  3.7603846937417984
========> pt_291:  2.1661472134292126
========> pt_292:  2.8176413103938103
========> pt_293:  2.958434596657753
========> pt_294:  2.435324639081955
========> pt_295:  3.1258105486631393
========> pt_296:  1.9132516346871853
========> pt_297:  4.477251172065735
========> pt_298:  3.164319545030594
========> pt_299:  2.3785072192549706
========> pt_300:  4.010105133056641
========> pt_301:  3.244917131960392
========> pt_302:  5.16175240278244
========> pt_303:  2.9827360436320305
========> pt_304:  2.546994835138321
========> pt_305:  3.3431347087025642
========> pt_306:  2.227700613439083
========> pt_307:  2.636537365615368
========> pt_308:  3.432711400091648
========> pt_309:  2.2479689866304398
========> pt_310:  3.801799453794956
========> pt_311:  2.27781780064106
========> pt_312:  2.334427647292614
========> pt_313:  2.25858885794878
========> pt_314:  3.094133734703064
========> pt_315:  2.8179970011115074
========> pt_316:  3.841637335717678
========> pt_317:  3.7801145762205124
========> pt_318:  5.334581583738327
========> pt_319:  2.1533354371786118
========> pt_320:  2.162386253476143
========> pt_321:  4.021266885101795
========> pt_322:  3.038976900279522
========> pt_323:  7.859903275966644
========> pt_324:  2.5470764562487602
========> pt_325:  3.248332180082798
========> pt_326:  2.710721045732498
========> pt_327:  2.575783357024193
========> pt_328:  2.3595119267702103
========> pt_329:  7.690344154834747
========> pt_330:  6.8185243010520935
========> pt_331:  3.4461311623454094
========> pt_332:  2.1694044955074787
========> pt_333:  2.120790909975767
========> pt_334:  1.8285842053592205
========> pt_335:  4.697273522615433
========> pt_336:  3.5142342001199722
========> pt_337:  2.7562708407640457
========> pt_338:  2.2458017244935036
========> pt_339:  2.5042209401726723
========> pt_340:  3.2625308632850647
===============================================> mean Dose score: 3.183502017147839
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.036730645340,     best is           0.036730645340
            Average val evaluation index is   -3.183502017148,     best is           -3.135910626501
    Train use time   1543.22562
    Train loader use time     90.76040
    Val use time     44.45235
    Total use time   1590.76695
    End lr is 0.000027030804, 0.000027030804
    time: 00:49:08
Epoch: 129, iter: 64499
    Begin lr is 0.000027030804, 0.000027030804
========> pt_241:  2.800830490887165
========> pt_242:  2.3075424879789352
========> pt_243:  4.368852600455284
========> pt_244:  2.500946708023548
========> pt_245:  3.1904586404561996
========> pt_246:  3.412410691380501
========> pt_247:  2.3809681087732315
========> pt_248:  2.0455515012145042
========> pt_249:  3.5708576068282127
========> pt_250:  2.2937292233109474
========> pt_251:  3.5014548897743225
========> pt_252:  3.3467500284314156
========> pt_253:  3.483140729367733
========> pt_254:  3.42267669737339
========> pt_255:  3.3670463040471077
========> pt_256:  2.1257966570556164
========> pt_257:  2.769824117422104
========> pt_258:  2.2274667024612427
========> pt_259:  2.94167123734951
========> pt_260:  3.8549207150936127
========> pt_261:  3.5427676886320114
========> pt_262:  3.178427740931511
========> pt_263:  2.7127649635076523
========> pt_264:  3.5106514766812325
========> pt_265:  2.4823013693094254
========> pt_266:  3.5578060522675514
========> pt_267:  3.249250091612339
========> pt_268:  3.7430794537067413
========> pt_269:  2.4552910402417183
========> pt_270:  4.845629930496216
========> pt_271:  3.6858265474438667
========> pt_272:  3.4046272188425064
========> pt_273:  2.4087657034397125
========> pt_274:  4.266038164496422
========> pt_275:  3.160688318312168
========> pt_276:  2.13800135999918
========> pt_277:  2.1950339153409004
========> pt_278:  4.012255184352398
========> pt_279:  2.362881079316139
========> pt_280:  2.671699896454811
========> pt_281:  2.7064504101872444
========> pt_282:  2.507791668176651
========> pt_283:  6.424093022942543
========> pt_284:  2.7742136642336845
========> pt_285:  2.4518778175115585
========> pt_286:  2.80215285718441
========> pt_287:  3.9822441712021828
========> pt_288:  2.08269614726305
========> pt_289:  3.489094115793705
========> pt_290:  3.607642389833927
========> pt_291:  2.2666464000940323
========> pt_292:  2.7763837948441505
========> pt_293:  2.8749236837029457
========> pt_294:  2.489667609333992
========> pt_295:  3.049440309405327
========> pt_296:  1.8624170683324337
========> pt_297:  4.424100443720818
========> pt_298:  3.1078562512993813
========> pt_299:  2.3356490954756737
========> pt_300:  4.127146154642105
========> pt_301:  3.1864164397120476
========> pt_302:  5.055930241942406
========> pt_303:  3.017345480620861
========> pt_304:  2.5616733357310295
========> pt_305:  3.245357573032379
========> pt_306:  2.280559279024601
========> pt_307:  2.5941384583711624
========> pt_308:  3.3329255506396294
========> pt_309:  2.156039886176586
========> pt_310:  3.743438273668289
========> pt_311:  2.312050685286522
========> pt_312:  2.2578884288668633
========> pt_313:  2.17076662927866
========> pt_314:  2.9992761835455894
========> pt_315:  2.7315842360258102
========> pt_316:  3.7519393861293793
========> pt_317:  3.914954997599125
========> pt_318:  5.413454174995422
========> pt_319:  2.095894254744053
========> pt_320:  2.1113743633031845
========> pt_321:  3.761374056339264
========> pt_322:  3.1426985561847687
========> pt_323:  7.438890635967255
========> pt_324:  2.549276053905487
========> pt_325:  3.225920796394348
========> pt_326:  2.655116990208626
========> pt_327:  2.5781521946191788
========> pt_328:  2.3271044343709946
========> pt_329:  7.28140726685524
========> pt_330:  6.93497858941555
========> pt_331:  3.477756343781948
========> pt_332:  2.1614425256848335
========> pt_333:  2.0724117569625378
========> pt_334:  1.814476139843464
========> pt_335:  4.75699357688427
========> pt_336:  3.464493565261364
========> pt_337:  2.612162120640278
========> pt_338:  2.2492266818881035
========> pt_339:  2.5402314960956573
========> pt_340:  3.221342973411083
===============================================> mean Dose score: 3.1683763502165676
        ==> Saving latest model successfully !
            Average train loss is             0.036824202362,     best is           0.036730645340
            Average val evaluation index is   -3.168376350217,     best is           -3.135910626501
    Train use time   1539.21851
    Train loader use time     89.25677
    Val use time     44.80542
    Total use time   1585.41997
    End lr is 0.000025371132, 0.000025371132
    time: 01:15:34
Epoch: 130, iter: 64999
    Begin lr is 0.000025371132, 0.000025371132
========> pt_241:  2.576064206659794
========> pt_242:  2.3261674866080284
========> pt_243:  4.296540729701519
========> pt_244:  2.3962896689772606
========> pt_245:  3.101732060313225
========> pt_246:  3.393659219145775
========> pt_247:  2.260407991707325
========> pt_248:  2.2179973497986794
========> pt_249:  3.6337846145033836
========> pt_250:  2.2989102080464363
========> pt_251:  3.5426057502627373
========> pt_252:  3.437470719218254
========> pt_253:  3.4269212558865547
========> pt_254:  3.3947432413697243
========> pt_255:  3.2833969220519066
========> pt_256:  2.1033250354230404
========> pt_257:  2.610982395708561
========> pt_258:  2.1973049640655518
========> pt_259:  2.7188915014266968
========> pt_260:  3.7378251925110817
========> pt_261:  3.3860668912529945
========> pt_262:  3.208213970065117
========> pt_263:  2.623256854712963
========> pt_264:  3.4528572112321854
========> pt_265:  2.4756785854697227
========> pt_266:  3.5155025869607925
========> pt_267:  3.2297593355178833
========> pt_268:  3.7171095982193947
========> pt_269:  2.4604887142777443
========> pt_270:  4.563068598508835
========> pt_271:  3.3221260085701942
========> pt_272:  3.2837265357375145
========> pt_273:  2.329893372952938
========> pt_274:  4.135945066809654
========> pt_275:  3.226989172399044
========> pt_276:  1.9276941381394863
========> pt_277:  2.2612573206424713
========> pt_278:  3.945060670375824
========> pt_279:  2.292618080973625
========> pt_280:  2.627638317644596
========> pt_281:  2.83370241522789
========> pt_282:  2.4520280212163925
========> pt_283:  5.868300199508667
========> pt_284:  2.5524212047457695
========> pt_285:  2.4784982949495316
========> pt_286:  2.8301212564110756
========> pt_287:  4.091904871165752
========> pt_288:  2.04875610768795
========> pt_289:  3.3812298625707626
========> pt_290:  3.576151244342327
========> pt_291:  2.188550643622875
========> pt_292:  2.873588800430298
========> pt_293:  2.794439271092415
========> pt_294:  2.2656213119626045
========> pt_295:  2.995273880660534
========> pt_296:  1.8842080794274807
========> pt_297:  4.372147172689438
========> pt_298:  3.0263016372919083
========> pt_299:  2.3130356147885323
========> pt_300:  3.951159305870533
========> pt_301:  3.1248918548226357
========> pt_302:  5.07802739739418
========> pt_303:  3.0521781370043755
========> pt_304:  2.571222484111786
========> pt_305:  3.179846592247486
========> pt_306:  2.2509683668613434
========> pt_307:  2.52395186573267
========> pt_308:  3.3826161175966263
========> pt_309:  2.2229530289769173
========> pt_310:  3.7358344718813896
========> pt_311:  2.227424718439579
========> pt_312:  2.255498990416527
========> pt_313:  2.1399863436818123
========> pt_314:  3.0765319988131523
========> pt_315:  2.8545603901147842
========> pt_316:  3.5279421135783195
========> pt_317:  3.885395899415016
========> pt_318:  5.25100365281105
========> pt_319:  2.1497657522559166
========> pt_320:  2.1742926351726055
========> pt_321:  3.8418446481227875
========> pt_322:  3.1321892514824867
========> pt_323:  7.718447372317314
========> pt_324:  2.5880299136042595
========> pt_325:  3.307275138795376
========> pt_326:  2.7414700388908386
========> pt_327:  2.661699615418911
========> pt_328:  2.3687444999814034
========> pt_329:  7.610014379024506
========> pt_330:  6.709403395652771
========> pt_331:  3.483087792992592
========> pt_332:  2.2435684874653816
========> pt_333:  2.0889369025826454
========> pt_334:  1.8489645794034004
========> pt_335:  4.7137124836444855
========> pt_336:  3.3710141852498055
========> pt_337:  2.578267976641655
========> pt_338:  2.2205229103565216
========> pt_339:  2.3990976437926292
========> pt_340:  3.067498132586479
===============================================> mean Dose score: 3.130760928988457
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.037039664336,     best is           0.036730645340
            Average val evaluation index is   -3.130760928988,     best is           -3.130760928988
    Train use time   1534.52181
    Train loader use time     84.62887
    Val use time     44.23309
    Total use time   1581.83744
    End lr is 0.000023759526, 0.000023759526
    time: 01:41:56
Epoch: 131, iter: 65499
    Begin lr is 0.000023759526, 0.000023759526
========> pt_241:  2.627613805234432
========> pt_242:  2.2678545489907265
========> pt_243:  4.459734708070755
========> pt_244:  2.496390789747238
========> pt_245:  3.300369158387184
========> pt_246:  3.415009528398514
========> pt_247:  2.322734445333481
========> pt_248:  2.22672950476408
========> pt_249:  3.970995843410492
========> pt_250:  2.2855720669031143
========> pt_251:  3.5909856855869293
========> pt_252:  3.5917633026838303
========> pt_253:  3.666636198759079
========> pt_254:  3.5731135308742523
========> pt_255:  3.2190703600645065
========> pt_256:  2.103294003754854
========> pt_257:  2.5491753965616226
========> pt_258:  2.2414421662688255
========> pt_259:  2.552073858678341
========> pt_260:  3.751436099410057
========> pt_261:  3.616524487733841
========> pt_262:  3.3527425304055214
========> pt_263:  2.617037482559681
========> pt_264:  3.8173987343907356
========> pt_265:  2.461060583591461
========> pt_266:  3.73101569712162
========> pt_267:  3.209998942911625
========> pt_268:  3.873214013874531
========> pt_269:  2.495347447693348
========> pt_270:  4.920514300465584
========> pt_271:  3.2687995210289955
========> pt_272:  3.269307240843773
========> pt_273:  2.506689131259918
========> pt_274:  4.09437358379364
========> pt_275:  3.2895036414265633
========> pt_276:  2.025206331163645
========> pt_277:  2.3363492637872696
========> pt_278:  4.249039590358734
========> pt_279:  2.421874888241291
========> pt_280:  2.5936534255743027
========> pt_281:  2.8612254187464714
========> pt_282:  2.5211258977651596
========> pt_283:  6.0781895369291306
========> pt_284:  2.359831891953945
========> pt_285:  2.4919485673308372
========> pt_286:  2.9237594455480576
========> pt_287:  4.3944597244262695
========> pt_288:  2.072874493896961
========> pt_289:  3.4650928154587746
========> pt_290:  3.9301899820566177
========> pt_291:  2.18919787555933
========> pt_292:  2.9251068457961082
========> pt_293:  3.010122925043106
========> pt_294:  2.3168454691767693
========> pt_295:  3.1028220802545547
========> pt_296:  1.924840658903122
========> pt_297:  4.566851332783699
========> pt_298:  3.12058262526989
========> pt_299:  2.4080464988946915
========> pt_300:  4.086246937513351
========> pt_301:  3.2499009743332863
========> pt_302:  5.082348883152008
========> pt_303:  3.035876341164112
========> pt_304:  2.5974231213331223
========> pt_305:  3.3310774713754654
========> pt_306:  2.2300225123763084
========> pt_307:  2.615352123975754
========> pt_308:  3.492363654077053
========> pt_309:  2.2651002928614616
========> pt_310:  3.7625975906848907
========> pt_311:  2.2607892379164696
========> pt_312:  2.3127365112304688
========> pt_313:  2.2779249772429466
========> pt_314:  3.073480986058712
========> pt_315:  2.8324994817376137
========> pt_316:  3.964495100080967
========> pt_317:  3.8085348904132843
========> pt_318:  5.568380951881409
========> pt_319:  2.1728812158107758
========> pt_320:  2.1995556727051735
========> pt_321:  3.875325471162796
========> pt_322:  3.176845647394657
========> pt_323:  8.055080994963646
========> pt_324:  2.5634489208459854
========> pt_325:  3.3194799721240997
========> pt_326:  2.717931345105171
========> pt_327:  2.6790690049529076
========> pt_328:  2.424730062484741
========> pt_329:  7.791548073291779
========> pt_330:  6.576869487762451
========> pt_331:  3.4481698647141457
========> pt_332:  2.2268854454159737
========> pt_333:  2.1067117899656296
========> pt_334:  1.809636764228344
========> pt_335:  4.819445982575417
========> pt_336:  3.5035204514861107
========> pt_337:  2.8555377572774887
========> pt_338:  2.3175659775733948
========> pt_339:  2.496860697865486
========> pt_340:  3.3542875945568085
===============================================> mean Dose score: 3.2136330015957357
        ==> Saving latest model successfully !
            Average train loss is             0.036758033611,     best is           0.036730645340
            Average val evaluation index is   -3.213633001596,     best is           -3.130760928988
    Train use time   1535.19241
    Train loader use time     85.09653
    Val use time     44.22439
    Total use time   1580.98569
    End lr is 0.000022196607, 0.000022196607
    time: 02:08:17
Epoch: 132, iter: 65999
    Begin lr is 0.000022196607, 0.000022196607
========> pt_241:  2.7010493353009224
========> pt_242:  2.2909530624747276
========> pt_243:  4.352349229156971
========> pt_244:  2.45574738830328
========> pt_245:  3.151836469769478
========> pt_246:  3.3923381567001343
========> pt_247:  2.2897490859031677
========> pt_248:  2.17958627268672
========> pt_249:  3.829740211367607
========> pt_250:  2.2237710654735565
========> pt_251:  3.6597424745559692
========> pt_252:  3.4708109870553017
========> pt_253:  3.4729834645986557
========> pt_254:  3.519405536353588
========> pt_255:  3.4066536650061607
========> pt_256:  2.050628438591957
========> pt_257:  2.607189752161503
========> pt_258:  2.202492207288742
========> pt_259:  2.7474943548440933
========> pt_260:  3.613028861582279
========> pt_261:  3.45412977039814
========> pt_262:  3.225494436919689
========> pt_263:  2.7081508934497833
========> pt_264:  3.6212556436657906
========> pt_265:  2.478281855583191
========> pt_266:  3.6781296506524086
========> pt_267:  3.2793742790818214
========> pt_268:  3.7396959587931633
========> pt_269:  2.443823404610157
========> pt_270:  4.74053792655468
========> pt_271:  3.28852366656065
========> pt_272:  3.2484839484095573
========> pt_273:  2.3708512634038925
========> pt_274:  4.20831847935915
========> pt_275:  3.2318681851029396
========> pt_276:  2.1176316775381565
========> pt_277:  2.2346339747309685
========> pt_278:  4.19089563190937
========> pt_279:  2.2292443737387657
========> pt_280:  2.6875346526503563
========> pt_281:  2.82794538885355
========> pt_282:  2.511647678911686
========> pt_283:  5.97700335085392
========> pt_284:  2.551768496632576
========> pt_285:  2.5745076686143875
========> pt_286:  2.788691110908985
========> pt_287:  4.114798419177532
========> pt_288:  2.016666494309902
========> pt_289:  3.608386106789112
========> pt_290:  3.707144521176815
========> pt_291:  2.1215627901256084
========> pt_292:  2.8474022448062897
========> pt_293:  2.8429824486374855
========> pt_294:  2.4271268025040627
========> pt_295:  2.9828518256545067
========> pt_296:  1.840990874916315
========> pt_297:  4.436805695295334
========> pt_298:  3.099321238696575
========> pt_299:  2.300388775765896
========> pt_300:  3.9168570563197136
========> pt_301:  3.152044303715229
========> pt_302:  4.981483966112137
========> pt_303:  3.096427470445633
========> pt_304:  2.524816580116749
========> pt_305:  3.2053833082318306
========> pt_306:  2.2129178047180176
========> pt_307:  2.560840956866741
========> pt_308:  3.4172190353274345
========> pt_309:  2.2561464831233025
========> pt_310:  3.6570755764842033
========> pt_311:  2.2305745631456375
========> pt_312:  2.267296500504017
========> pt_313:  2.2876957803964615
========> pt_314:  3.0514245107769966
========> pt_315:  2.762359566986561
========> pt_316:  3.7178952991962433
========> pt_317:  3.7825167924165726
========> pt_318:  5.5032942444086075
========> pt_319:  2.0971209183335304
========> pt_320:  2.1643073484301567
========> pt_321:  4.269402101635933
========> pt_322:  3.1150540336966515
========> pt_323:  7.693015486001968
========> pt_324:  2.5458239763975143
========> pt_325:  3.13195638358593
========> pt_326:  2.7636923640966415
========> pt_327:  2.6333922147750854
========> pt_328:  2.512431815266609
========> pt_329:  7.313391789793968
========> pt_330:  6.610721126198769
========> pt_331:  3.5421308875083923
========> pt_332:  2.181896697729826
========> pt_333:  2.110327761620283
========> pt_334:  1.8506812304258347
========> pt_335:  4.898438528180122
========> pt_336:  3.464983031153679
========> pt_337:  2.5992125272750854
========> pt_338:  2.2088437899947166
========> pt_339:  2.444591373205185
========> pt_340:  3.261720649898052
===============================================> mean Dose score: 3.16369807459414
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.036562585354,     best is           0.036562585354
            Average val evaluation index is   -3.163698074594,     best is           -3.130760928988
    Train use time   1534.35408
    Train loader use time     84.61533
    Val use time     44.27482
    Total use time   1581.72520
    End lr is 0.000020682979, 0.000020682979
    time: 02:34:38
Epoch: 133, iter: 66499
    Begin lr is 0.000020682979, 0.000020682979
========> pt_241:  2.7430756017565727
========> pt_242:  2.2775718942284584
========> pt_243:  4.423374980688095
========> pt_244:  2.4779000878334045
========> pt_245:  3.170919120311737
========> pt_246:  3.4749840945005417
========> pt_247:  2.2873390465974808
========> pt_248:  2.1934014931321144
========> pt_249:  4.028826616704464
========> pt_250:  2.2256074100732803
========> pt_251:  3.7634456157684326
========> pt_252:  3.5061557963490486
========> pt_253:  3.54033000767231
========> pt_254:  3.762459382414818
========> pt_255:  3.3617761358618736
========> pt_256:  2.113131433725357
========> pt_257:  2.5708558410406113
========> pt_258:  2.2554684802889824
========> pt_259:  2.7171026170253754
========> pt_260:  3.661585859954357
========> pt_261:  3.5029981285333633
========> pt_262:  3.253737688064575
========> pt_263:  2.753208875656128
========> pt_264:  3.8582807406783104
========> pt_265:  2.505357377231121
========> pt_266:  3.8445159792900085
========> pt_267:  3.330279514193535
========> pt_268:  3.7979350984096527
========> pt_269:  2.421615421772003
========> pt_270:  4.900896027684212
========> pt_271:  3.4465113654732704
========> pt_272:  3.160219192504883
========> pt_273:  2.3938269540667534
========> pt_274:  4.216899126768112
========> pt_275:  3.324957974255085
========> pt_276:  2.13851286098361
========> pt_277:  2.260555848479271
========> pt_278:  4.423758834600449
========> pt_279:  2.222193665802479
========> pt_280:  2.736455425620079
========> pt_281:  2.907159850001335
========> pt_282:  2.5368761643767357
========> pt_283:  6.094284802675247
========> pt_284:  2.5478439033031464
========> pt_285:  2.552969343960285
========> pt_286:  2.7971982210874557
========> pt_287:  4.1587890684604645
========> pt_288:  2.0671318098902702
========> pt_289:  3.654002398252487
========> pt_290:  3.894883245229721
========> pt_291:  2.1432179398834705
========> pt_292:  2.8551411256194115
========> pt_293:  2.922329902648926
========> pt_294:  2.4625083804130554
========> pt_295:  3.0260828509926796
========> pt_296:  1.9147813133895397
========> pt_297:  4.437099322676659
========> pt_298:  3.1591836735606194
========> pt_299:  2.358541339635849
========> pt_300:  4.048573970794678
========> pt_301:  3.1773997843265533
========> pt_302:  5.014380663633347
========> pt_303:  3.1271780282258987
========> pt_304:  2.5394590944051743
========> pt_305:  3.362993933260441
========> pt_306:  2.1920963376760483
========> pt_307:  2.6259907707571983
========> pt_308:  3.6368948221206665
========> pt_309:  2.2600210085511208
========> pt_310:  3.5707420855760574
========> pt_311:  2.190190367400646
========> pt_312:  2.301349453628063
========> pt_313:  2.247976027429104
========> pt_314:  3.0983438715338707
========> pt_315:  2.8403927385807037
========> pt_316:  3.7832453846931458
========> pt_317:  3.631688542664051
========> pt_318:  5.531592518091202
========> pt_319:  2.1335022896528244
========> pt_320:  2.2029662877321243
========> pt_321:  4.123993441462517
========> pt_322:  3.1012165173888206
========> pt_323:  7.799405604600906
========> pt_324:  2.587919868528843
========> pt_325:  3.167909048497677
========> pt_326:  2.6935185492038727
========> pt_327:  2.6416294276714325
========> pt_328:  2.413709908723831
========> pt_329:  7.501195967197418
========> pt_330:  6.61319375038147
========> pt_331:  3.550189472734928
========> pt_332:  2.202047072350979
========> pt_333:  2.1381207928061485
========> pt_334:  1.8150371871888638
========> pt_335:  5.1320720463991165
========> pt_336:  3.448694534599781
========> pt_337:  2.6856039091944695
========> pt_338:  2.2094185277819633
========> pt_339:  2.4145180359482765
========> pt_340:  3.3669352158904076
===============================================> mean Dose score: 3.206613591313362
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.036560872745,     best is           0.036560872745
            Average val evaluation index is   -3.206613591313,     best is           -3.130760928988
    Train use time   1536.43279
    Train loader use time     86.72321
    Val use time     44.31825
    Total use time   1583.70231
    End lr is 0.000019219224, 0.000019219224
    time: 03:01:02
Epoch: 134, iter: 66999
    Begin lr is 0.000019219224, 0.000019219224
========> pt_241:  2.6024147868156433
========> pt_242:  2.245165966451168
========> pt_243:  4.353820756077766
========> pt_244:  2.4906426295638084
========> pt_245:  3.0761457979679108
========> pt_246:  3.3356399089097977
========> pt_247:  2.315949723124504
========> pt_248:  2.191344015300274
========> pt_249:  3.599225766956806
========> pt_250:  2.2960028797388077
========> pt_251:  3.51296529173851
========> pt_252:  3.434534966945648
========> pt_253:  3.5283001512289047
========> pt_254:  3.3752501383423805
========> pt_255:  3.241642899811268
========> pt_256:  2.083240896463394
========> pt_257:  2.4833358451724052
========> pt_258:  2.259392812848091
========> pt_259:  2.6356543973088264
========> pt_260:  3.543337471783161
========> pt_261:  3.391319327056408
========> pt_262:  3.2828347012400627
========> pt_263:  2.619001604616642
========> pt_264:  3.5951535776257515
========> pt_265:  2.4430152773857117
========> pt_266:  3.7489819899201393
========> pt_267:  3.3332791551947594
========> pt_268:  3.677039109170437
========> pt_269:  2.403486929833889
========> pt_270:  4.751970618963242
========> pt_271:  3.2577569410204887
========> pt_272:  3.3017655834555626
========> pt_273:  2.3601922765374184
========> pt_274:  4.076881371438503
========> pt_275:  3.1999991834163666
========> pt_276:  2.0635116659104824
========> pt_277:  2.267077974975109
========> pt_278:  3.9799410477280617
========> pt_279:  2.315726764500141
========> pt_280:  2.5818540900945663
========> pt_281:  2.7947042137384415
========> pt_282:  2.4443887546658516
========> pt_283:  5.999290868639946
========> pt_284:  2.6642468199133873
========> pt_285:  2.4603575468063354
========> pt_286:  2.8611164167523384
========> pt_287:  3.9857760444283485
========> pt_288:  2.0637300610542297
========> pt_289:  3.341788873076439
========> pt_290:  3.5570938885211945
========> pt_291:  2.148283012211323
========> pt_292:  2.879052720963955
========> pt_293:  2.791043519973755
========> pt_294:  2.3715561255812645
========> pt_295:  2.9466506466269493
========> pt_296:  1.8426208198070526
========> pt_297:  4.3492113798856735
========> pt_298:  3.1385601311922073
========> pt_299:  2.3815613612532616
========> pt_300:  4.018741846084595
========> pt_301:  3.1752583384513855
========> pt_302:  4.734449461102486
========> pt_303:  3.0367084592580795
========> pt_304:  2.5542718917131424
========> pt_305:  3.1961001455783844
========> pt_306:  2.2857433930039406
========> pt_307:  2.6245304569602013
========> pt_308:  3.372911289334297
========> pt_309:  2.166944909840822
========> pt_310:  3.757324554026127
========> pt_311:  2.2656865045428276
========> pt_312:  2.256595529615879
========> pt_313:  2.2119539976119995
========> pt_314:  3.0847353115677834
========> pt_315:  2.8591173514723778
========> pt_316:  3.606925532221794
========> pt_317:  3.7937765941023827
========> pt_318:  5.5037083476781845
========> pt_319:  2.1049642376601696
========> pt_320:  2.165640667080879
========> pt_321:  3.8374660536646843
========> pt_322:  2.9888800531625748
========> pt_323:  7.468951195478439
========> pt_324:  2.5180767104029655
========> pt_325:  3.0757731571793556
========> pt_326:  2.6915359124541283
========> pt_327:  2.6210150122642517
========> pt_328:  2.4409710988402367
========> pt_329:  7.148125991225243
========> pt_330:  6.794071346521378
========> pt_331:  3.379247225821018
========> pt_332:  2.2058381512761116
========> pt_333:  2.1035858057439327
========> pt_334:  1.8297352455556393
========> pt_335:  4.824260845780373
========> pt_336:  3.386181890964508
========> pt_337:  2.5681060180068016
========> pt_338:  2.133271247148514
========> pt_339:  2.3658032715320587
========> pt_340:  3.1836019456386566
===============================================> mean Dose score: 3.1261641649529337
        ==> Saving best_val_evaluation_index model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.036615083862,     best is           0.036560872745
            Average val evaluation index is   -3.126164164953,     best is           -3.126164164953
    Train use time   1534.92910
    Train loader use time     84.94228
    Val use time     44.23019
    Total use time   1582.23167
    End lr is 0.000017805906, 0.000017805906
    time: 03:27:24
Epoch: 135, iter: 67499
    Begin lr is 0.000017805906, 0.000017805906
========> pt_241:  2.648872062563896
========> pt_242:  2.273091860115528
========> pt_243:  4.279215931892395
========> pt_244:  2.442534416913986
========> pt_245:  3.1741873547434807
========> pt_246:  3.4405039995908737
========> pt_247:  2.3018915951251984
========> pt_248:  2.1911948546767235
========> pt_249:  3.783431053161621
========> pt_250:  2.221895344555378
========> pt_251:  3.7327127903699875
========> pt_252:  3.441222161054611
========> pt_253:  3.5468151047825813
========> pt_254:  3.52138452231884
========> pt_255:  3.4005581587553024
========> pt_256:  2.068636976182461
========> pt_257:  2.5597822293639183
========> pt_258:  2.1786411106586456
========> pt_259:  2.8246257826685905
========> pt_260:  3.585989326238632
========> pt_261:  3.5390235483646393
========> pt_262:  3.1832603365182877
========> pt_263:  2.6450124010443687
========> pt_264:  3.654470220208168
========> pt_265:  2.485094740986824
========> pt_266:  3.681124858558178
========> pt_267:  3.2448793202638626
========> pt_268:  3.837561495602131
========> pt_269:  2.392013296484947
========> pt_270:  5.024137124419212
========> pt_271:  3.2820581272244453
========> pt_272:  3.197692409157753
========> pt_273:  2.4351436644792557
========> pt_274:  4.1986290365457535
========> pt_275:  3.220229223370552
========> pt_276:  2.113967202603817
========> pt_277:  2.286003641784191
========> pt_278:  4.219698496162891
========> pt_279:  2.317187860608101
========> pt_280:  2.7219920605421066
========> pt_281:  2.8745586052536964
========> pt_282:  2.5378530099987984
========> pt_283:  5.853710621595383
========> pt_284:  2.597627565264702
========> pt_285:  2.529943324625492
========> pt_286:  2.8034288063645363
========> pt_287:  4.111594334244728
========> pt_288:  2.0409369096159935
========> pt_289:  3.4317700192332268
========> pt_290:  3.8029609248042107
========> pt_291:  2.1421013213694096
========> pt_292:  2.838958241045475
========> pt_293:  2.9170456528663635
========> pt_294:  2.430272474884987
========> pt_295:  3.091910146176815
========> pt_296:  1.8870951980352402
========> pt_297:  4.385146051645279
========> pt_298:  3.1084351614117622
========> pt_299:  2.323760576546192
========> pt_300:  4.141126573085785
========> pt_301:  3.116662986576557
========> pt_302:  4.968007877469063
========> pt_303:  3.057412840425968
========> pt_304:  2.559826038777828
========> pt_305:  3.243553563952446
========> pt_306:  2.185256462544203
========> pt_307:  2.5821876153349876
========> pt_308:  3.4515004232525826
========> pt_309:  2.2536756843328476
========> pt_310:  3.694066107273102
========> pt_311:  2.208961136639118
========> pt_312:  2.2916748747229576
========> pt_313:  2.2776250913739204
========> pt_314:  3.0642544105648994
========> pt_315:  2.8027062118053436
========> pt_316:  3.7474197149276733
========> pt_317:  3.8791804388165474
========> pt_318:  5.288966074585915
========> pt_319:  2.114486787468195
========> pt_320:  2.2196169942617416
========> pt_321:  3.9490003883838654
========> pt_322:  3.1092651933431625
========> pt_323:  7.673401907086372
========> pt_324:  2.5619638338685036
========> pt_325:  3.2262637093663216
========> pt_326:  2.6672793179750443
========> pt_327:  2.5960856303572655
========> pt_328:  2.4141420051455498
========> pt_329:  7.38553024828434
========> pt_330:  6.826881989836693
========> pt_331:  3.5214710980653763
========> pt_332:  2.2063782066106796
========> pt_333:  2.1567328833043575
========> pt_334:  1.8527347967028618
========> pt_335:  5.0334372371435165
========> pt_336:  3.399035520851612
========> pt_337:  2.679150104522705
========> pt_338:  2.232833355665207
========> pt_339:  2.4332624673843384
========> pt_340:  3.1626086309552193
===============================================> mean Dose score: 3.172411010786891
        ==> Saving latest model successfully !
            Average train loss is             0.036648022033,     best is           0.036560872745
            Average val evaluation index is   -3.172411010787,     best is           -3.126164164953
    Train use time   1534.53630
    Train loader use time     84.62880
    Val use time     43.98943
    Total use time   1580.09355
    End lr is 0.000016443572, 0.000016443572
    time: 03:53:44
Epoch: 136, iter: 67999
    Begin lr is 0.000016443572, 0.000016443572
========> pt_241:  2.539208233356476
========> pt_242:  2.2827210649847984
========> pt_243:  4.3993064016103745
========> pt_244:  2.5196773186326027
========> pt_245:  3.122033029794693
========> pt_246:  3.439544625580311
========> pt_247:  2.2373806685209274
========> pt_248:  2.198580391705036
========> pt_249:  3.7085135653615
========> pt_250:  2.2788846120238304
========> pt_251:  3.7130947783589363
========> pt_252:  3.4045064821839333
========> pt_253:  3.5226135328412056
========> pt_254:  3.42051699757576
========> pt_255:  3.2092586159706116
========> pt_256:  2.0952015183866024
========> pt_257:  2.506934776902199
========> pt_258:  2.2167938947677612
========> pt_259:  2.727389745414257
========> pt_260:  3.587792031466961
========> pt_261:  3.5635745525360107
========> pt_262:  3.1074755266308784
========> pt_263:  2.6437747851014137
========> pt_264:  3.668723404407501
========> pt_265:  2.5085319951176643
========> pt_266:  3.710397891700268
========> pt_267:  3.265393078327179
========> pt_268:  3.7392544746398926
========> pt_269:  2.499242052435875
========> pt_270:  4.855012968182564
========> pt_271:  3.1846551969647408
========> pt_272:  3.099590614438057
========> pt_273:  2.3763540387153625
========> pt_274:  3.9882703125476837
========> pt_275:  3.117366284132004
========> pt_276:  2.0480820164084435
========> pt_277:  2.2414572909474373
========> pt_278:  4.2152948677539825
========> pt_279:  2.259708344936371
========> pt_280:  2.6689383387565613
========> pt_281:  2.8255851566791534
========> pt_282:  2.4957623332738876
========> pt_283:  5.785852447152138
========> pt_284:  2.4936380982398987
========> pt_285:  2.487739995121956
========> pt_286:  2.817182093858719
========> pt_287:  4.109617173671722
========> pt_288:  2.0473823696374893
========> pt_289:  3.583308607339859
========> pt_290:  3.6750955879688263
========> pt_291:  2.1013519167900085
========> pt_292:  2.8715866059064865
========> pt_293:  2.8586218878626823
========> pt_294:  2.4251407757401466
========> pt_295:  3.036738969385624
========> pt_296:  1.8567192368209362
========> pt_297:  4.483337551355362
========> pt_298:  3.0904287099838257
========> pt_299:  2.3097287863492966
========> pt_300:  3.945333957672119
========> pt_301:  3.140186294913292
========> pt_302:  5.128431171178818
========> pt_303:  3.051876425743103
========> pt_304:  2.573394440114498
========> pt_305:  3.252187669277191
========> pt_306:  2.1871780790388584
========> pt_307:  2.5853825733065605
========> pt_308:  3.571784906089306
========> pt_309:  2.2527914121747017
========> pt_310:  3.709893561899662
========> pt_311:  2.2051794454455376
========> pt_312:  2.265832796692848
========> pt_313:  2.4042288213968277
========> pt_314:  3.0971119925379753
========> pt_315:  2.8285545483231544
========> pt_316:  3.8243263587355614
========> pt_317:  3.746394105255604
========> pt_318:  5.686501041054726
========> pt_319:  2.128509972244501
========> pt_320:  2.180587761104107
========> pt_321:  3.9538976550102234
========> pt_322:  3.1193932518363
========> pt_323:  7.74860180914402
========> pt_324:  2.5424490869045258
========> pt_325:  3.1315853074193
========> pt_326:  2.740746922791004
========> pt_327:  2.581634260714054
========> pt_328:  2.495034784078598
========> pt_329:  7.454136312007904
========> pt_330:  6.711547449231148
========> pt_331:  3.512248955667019
========> pt_332:  2.218910828232765
========> pt_333:  2.128890175372362
========> pt_334:  1.8047181144356728
========> pt_335:  4.960433021187782
========> pt_336:  3.4452583640813828
========> pt_337:  2.6957765594124794
========> pt_338:  2.1816524863243103
========> pt_339:  2.403937540948391
========> pt_340:  3.1848732009530067
===============================================> mean Dose score: 3.160272620432079
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.036344902229,     best is           0.036344902229
            Average val evaluation index is   -3.160272620432,     best is           -3.126164164953
    Train use time   1534.31373
    Train loader use time     84.50837
    Val use time     44.20901
    Total use time   1581.62743
    End lr is 0.000015132745, 0.000015132745
    time: 04:20:06
Epoch: 137, iter: 68499
    Begin lr is 0.000015132745, 0.000015132745
========> pt_241:  2.7160582318902016
========> pt_242:  2.351014204323292
========> pt_243:  4.479610621929169
========> pt_244:  2.436470203101635
========> pt_245:  3.173895552754402
========> pt_246:  3.4665995463728905
========> pt_247:  2.357548587024212
========> pt_248:  2.124341819435358
========> pt_249:  3.7511946260929108
========> pt_250:  2.218586951494217
========> pt_251:  3.6947232484817505
========> pt_252:  3.4330232813954353
========> pt_253:  3.668108507990837
========> pt_254:  3.6014459654688835
========> pt_255:  3.3701784163713455
========> pt_256:  2.1373521722853184
========> pt_257:  2.7914177253842354
========> pt_258:  2.201797775924206
========> pt_259:  2.9327578470110893
========> pt_260:  3.5724183171987534
========> pt_261:  3.6640816926956177
========> pt_262:  3.2283470034599304
========> pt_263:  2.7205896377563477
========> pt_264:  3.7355658784508705
========> pt_265:  2.477973625063896
========> pt_266:  3.645273894071579
========> pt_267:  3.234107680618763
========> pt_268:  3.849981464445591
========> pt_269:  2.4267901480197906
========> pt_270:  4.954374805092812
========> pt_271:  3.703049384057522
========> pt_272:  3.5517697408795357
========> pt_273:  2.400624193251133
========> pt_274:  4.225232042372227
========> pt_275:  3.2308634370565414
========> pt_276:  2.1218403801321983
========> pt_277:  2.195402905344963
========> pt_278:  4.231608137488365
========> pt_279:  2.3105131834745407
========> pt_280:  2.6400872319936752
========> pt_281:  2.801516316831112
========> pt_282:  2.516056001186371
========> pt_283:  5.713525712490082
========> pt_284:  2.7379444241523743
========> pt_285:  2.5934435054659843
========> pt_286:  2.793000601232052
========> pt_287:  4.093290865421295
========> pt_288:  2.133636064827442
========> pt_289:  3.5540908575057983
========> pt_290:  3.7388380244374275
========> pt_291:  2.2584430873394012
========> pt_292:  2.891690954566002
========> pt_293:  2.9162732511758804
========> pt_294:  2.4587420746684074
========> pt_295:  2.9982589185237885
========> pt_296:  1.9069981016218662
========> pt_297:  4.549097046256065
========> pt_298:  3.1124335527420044
========> pt_299:  2.324042208492756
========> pt_300:  4.155940413475037
========> pt_301:  3.1916088983416557
========> pt_302:  4.976275339722633
========> pt_303:  3.1013834103941917
========> pt_304:  2.5497952476143837
========> pt_305:  3.228798136115074
========> pt_306:  2.2056258842349052
========> pt_307:  2.5944719836115837
========> pt_308:  3.5320555046200752
========> pt_309:  2.183043695986271
========> pt_310:  3.773050829768181
========> pt_311:  2.1683576330542564
========> pt_312:  2.2024259716272354
========> pt_313:  2.190150208771229
========> pt_314:  3.029043637216091
========> pt_315:  2.7836300805211067
========> pt_316:  3.9054444432258606
========> pt_317:  3.8305960595607758
========> pt_318:  5.552729517221451
========> pt_319:  2.118495088070631
========> pt_320:  2.1576367132365704
========> pt_321:  4.1620784252882
========> pt_322:  3.0398932471871376
========> pt_323:  7.388735115528107
========> pt_324:  2.613966390490532
========> pt_325:  3.1942666694521904
========> pt_326:  2.740980312228203
========> pt_327:  2.5526749342679977
========> pt_328:  2.27137990295887
========> pt_329:  7.571075111627579
========> pt_330:  6.8530429899692535
========> pt_331:  3.491441048681736
========> pt_332:  2.1644025295972824
========> pt_333:  2.123594582080841
========> pt_334:  1.8646818585693836
========> pt_335:  4.8753973841667175
========> pt_336:  3.4495897591114044
========> pt_337:  2.710079289972782
========> pt_338:  2.243288680911064
========> pt_339:  2.480568289756775
========> pt_340:  3.2010474801063538
===============================================> mean Dose score: 3.1954068433493377
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.036257054288,     best is           0.036257054288
            Average val evaluation index is   -3.195406843349,     best is           -3.126164164953
    Train use time   1536.38656
    Train loader use time     84.74100
    Val use time     44.21545
    Total use time   1583.66822
    End lr is 0.000013873931, 0.000013873931
    time: 04:46:30
Epoch: 138, iter: 68999
    Begin lr is 0.000013873931, 0.000013873931
========> pt_241:  2.6393064856529236
========> pt_242:  2.255069501698017
========> pt_243:  4.537048414349556
========> pt_244:  2.52091858536005
========> pt_245:  3.211761489510536
========> pt_246:  3.4667226299643517
========> pt_247:  2.3073137924075127
========> pt_248:  2.195778153836727
========> pt_249:  4.033022932708263
========> pt_250:  2.2220533713698387
========> pt_251:  3.6949973180890083
========> pt_252:  3.4872762858867645
========> pt_253:  3.68747279047966
========> pt_254:  3.661522753536701
========> pt_255:  3.3445942401885986
========> pt_256:  2.091420218348503
========> pt_257:  2.632664144039154
========> pt_258:  2.2165998816490173
========> pt_259:  2.6787863299250603
========> pt_260:  3.6019479483366013
========> pt_261:  3.7951333820819855
========> pt_262:  3.183503895998001
========> pt_263:  2.6675572991371155
========> pt_264:  3.821031004190445
========> pt_265:  2.477992922067642
========> pt_266:  3.8193153962492943
========> pt_267:  3.3016065135598183
========> pt_268:  3.924075961112976
========> pt_269:  2.489759661257267
========> pt_270:  4.978979006409645
========> pt_271:  3.4503353014588356
========> pt_272:  3.2012487947940826
========> pt_273:  2.442799098789692
========> pt_274:  4.135848321020603
========> pt_275:  3.302721567451954
========> pt_276:  2.105704825371504
========> pt_277:  2.245095819234848
========> pt_278:  4.3679279088974
========> pt_279:  2.235702872276306
========> pt_280:  2.642830014228821
========> pt_281:  2.8057201951742172
========> pt_282:  2.543487995862961
========> pt_283:  5.698344707489014
========> pt_284:  2.5631889328360558
========> pt_285:  2.512834705412388
========> pt_286:  2.8570789098739624
========> pt_287:  4.277127422392368
========> pt_288:  2.077340967953205
========> pt_289:  3.726187013089657
========> pt_290:  3.828248344361782
========> pt_291:  2.1749779395759106
========> pt_292:  2.849229723215103
========> pt_293:  3.0390509590506554
========> pt_294:  2.4717946723103523
========> pt_295:  3.095805011689663
========> pt_296:  1.883093025535345
========> pt_297:  4.567570015788078
========> pt_298:  3.103305548429489
========> pt_299:  2.3609592020511627
========> pt_300:  4.0385111048817635
========> pt_301:  3.215230256319046
========> pt_302:  5.194551050662994
========> pt_303:  3.080630786716938
========> pt_304:  2.591656967997551
========> pt_305:  3.276307098567486
========> pt_306:  2.1996907517313957
========> pt_307:  2.620447054505348
========> pt_308:  3.573131784796715
========> pt_309:  2.25775595754385
========> pt_310:  3.6617840453982353
========> pt_311:  2.17633668333292
========> pt_312:  2.2944466024637222
========> pt_313:  2.3422588407993317
========> pt_314:  3.0978721380233765
========> pt_315:  2.827269472181797
========> pt_316:  4.044969342648983
========> pt_317:  3.7071484327316284
========> pt_318:  5.67842498421669
========> pt_319:  2.1540323458611965
========> pt_320:  2.202483080327511
========> pt_321:  4.429790452122688
========> pt_322:  3.134562522172928
========> pt_323:  7.866595163941383
========> pt_324:  2.594628445804119
========> pt_325:  3.235482983291149
========> pt_326:  2.843494862318039
========> pt_327:  2.5918327271938324
========> pt_328:  2.4948785826563835
========> pt_329:  7.860865518450737
========> pt_330:  6.6323744505643845
========> pt_331:  3.5872016474604607
========> pt_332:  2.2029803693294525
========> pt_333:  2.132783606648445
========> pt_334:  1.7947989329695702
========> pt_335:  5.075312778353691
========> pt_336:  3.416035920381546
========> pt_337:  2.829686291515827
========> pt_338:  2.2678443789482117
========> pt_339:  2.477160021662712
========> pt_340:  3.2540279254317284
===============================================> mean Dose score: 3.2246806848794223
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.036081127442,     best is           0.036081127442
            Average val evaluation index is   -3.224680684879,     best is           -3.126164164953
    Train use time   1535.42937
    Train loader use time     84.66332
    Val use time     44.01411
    Total use time   1583.10480
    End lr is 0.000012667616, 0.000012667616
    time: 05:12:53
Epoch: 139, iter: 69499
    Begin lr is 0.000012667616, 0.000012667616
========> pt_241:  2.53062941133976
========> pt_242:  2.2691187635064125
========> pt_243:  4.469349831342697
========> pt_244:  2.46429231017828
========> pt_245:  3.238319903612137
========> pt_246:  3.387712612748146
========> pt_247:  2.2497346624732018
========> pt_248:  2.2676383703947067
========> pt_249:  3.9639511331915855
========> pt_250:  2.2929761186242104
========> pt_251:  3.684244193136692
========> pt_252:  3.471960462629795
========> pt_253:  3.594769984483719
========> pt_254:  3.660225160419941
========> pt_255:  3.3031129837036133
========> pt_256:  2.0958161540329456
========> pt_257:  2.565898336470127
========> pt_258:  2.230847589671612
========> pt_259:  2.7565548196434975
========> pt_260:  3.6594582349061966
========> pt_261:  3.5553451627492905
========> pt_262:  3.23576956987381
========> pt_263:  2.676919214427471
========> pt_264:  3.712422512471676
========> pt_265:  2.506801523268223
========> pt_266:  3.655761554837227
========> pt_267:  3.2449332997202873
========> pt_268:  3.8796276599168777
========> pt_269:  2.4604490771889687
========> pt_270:  4.8729341477155685
========> pt_271:  3.2073529064655304
========> pt_272:  3.2910221070051193
========> pt_273:  2.4823670834302902
========> pt_274:  4.0365394204854965
========> pt_275:  3.256697691977024
========> pt_276:  2.011061627417803
========> pt_277:  2.339278757572174
========> pt_278:  4.201640151441097
========> pt_279:  2.3807623609900475
========> pt_280:  2.6839999109506607
========> pt_281:  2.8687551617622375
========> pt_282:  2.5129828229546547
========> pt_283:  6.0883840918540955
========> pt_284:  2.496684417128563
========> pt_285:  2.5118153542280197
========> pt_286:  2.8381790593266487
========> pt_287:  4.202597439289093
========> pt_288:  2.0511212944984436
========> pt_289:  3.4572582319378853
========> pt_290:  3.801211677491665
========> pt_291:  2.1667774952948093
========> pt_292:  2.9572345316410065
========> pt_293:  2.9560162127017975
========> pt_294:  2.3534229397773743
========> pt_295:  3.0591991171240807
========> pt_296:  1.9055131450295448
========> pt_297:  4.5534732937812805
========> pt_298:  3.117665909230709
========> pt_299:  2.3713814094662666
========> pt_300:  4.053228199481964
========> pt_301:  3.172730952501297
========> pt_302:  5.064859017729759
========> pt_303:  3.0148981511592865
========> pt_304:  2.581021711230278
========> pt_305:  3.257817178964615
========> pt_306:  2.245967574417591
========> pt_307:  2.564373090863228
========> pt_308:  3.673429787158966
========> pt_309:  2.2758781909942627
========> pt_310:  3.664764128625393
========> pt_311:  2.2426894307136536
========> pt_312:  2.3293134197592735
========> pt_313:  2.3449617251753807
========> pt_314:  3.1195922195911407
========> pt_315:  2.82663032412529
========> pt_316:  3.8356581330299377
========> pt_317:  3.750489242374897
========> pt_318:  5.387343764305115
========> pt_319:  2.1845100075006485
========> pt_320:  2.1947212517261505
========> pt_321:  3.783121518790722
========> pt_322:  3.191242255270481
========> pt_323:  7.8575146198272705
========> pt_324:  2.56994366645813
========> pt_325:  3.256271071732044
========> pt_326:  2.7228286117315292
========> pt_327:  2.6501931250095367
========> pt_328:  2.4897630512714386
========> pt_329:  8.036468252539635
========> pt_330:  6.78381472826004
========> pt_331:  3.411581963300705
========> pt_332:  2.241007462143898
========> pt_333:  2.1561199426651
========> pt_334:  1.86470415443182
========> pt_335:  5.003724545240402
========> pt_336:  3.4837527573108673
========> pt_337:  2.746078372001648
========> pt_338:  2.246486246585846
========> pt_339:  2.446308806538582
========> pt_340:  3.2199371606111526
===============================================> mean Dose score: 3.200637101940811
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.036058625337,     best is           0.036058625337
            Average val evaluation index is   -3.200637101941,     best is           -3.126164164953
    Train use time   1536.27814
    Train loader use time     84.56939
    Val use time     44.19374
    Total use time   1583.56062
    End lr is 0.000011514264, 0.000011514264
    time: 05:39:16
Epoch: 140, iter: 69999
    Begin lr is 0.000011514264, 0.000011514264
========> pt_241:  2.733592949807644
========> pt_242:  2.3333120718598366
========> pt_243:  4.313732534646988
========> pt_244:  2.418275475502014
========> pt_245:  3.1171169877052307
========> pt_246:  3.4173087403178215
========> pt_247:  2.3795435205101967
========> pt_248:  2.160378582775593
========> pt_249:  3.5649175196886063
========> pt_250:  2.217640094459057
========> pt_251:  3.529259003698826
========> pt_252:  3.333301842212677
========> pt_253:  3.473150096833706
========> pt_254:  3.413553647696972
========> pt_255:  3.497188165783882
========> pt_256:  2.059026677161455
========> pt_257:  2.6064027473330498
========> pt_258:  2.1572529897093773
========> pt_259:  2.990313246846199
========> pt_260:  3.6110613495111465
========> pt_261:  3.4514375776052475
========> pt_262:  3.1491343677043915
========> pt_263:  2.614285834133625
========> pt_264:  3.55950940400362
========> pt_265:  2.4582786858081818
========> pt_266:  3.5159344226121902
========> pt_267:  3.264281675219536
========> pt_268:  3.7162764370441437
========> pt_269:  2.393885627388954
========> pt_270:  4.821567609906197
========> pt_271:  3.4280790761113167
========> pt_272:  3.4621265530586243
========> pt_273:  2.3699184879660606
========> pt_274:  4.269462861120701
========> pt_275:  3.179490379989147
========> pt_276:  2.0578993670642376
========> pt_277:  2.2029120475053787
========> pt_278:  3.944920375943184
========> pt_279:  2.3182499781250954
========> pt_280:  2.628185674548149
========> pt_281:  2.8023382648825645
========> pt_282:  2.487354055047035
========> pt_283:  6.156724169850349
========> pt_284:  2.8928258270025253
========> pt_285:  2.5135912001132965
========> pt_286:  2.7549531683325768
========> pt_287:  4.0430474653840065
========> pt_288:  2.0221544057130814
========> pt_289:  3.345961458981037
========> pt_290:  3.626925051212311
========> pt_291:  2.1590310521423817
========> pt_292:  2.79767282307148
========> pt_293:  2.815958298742771
========> pt_294:  2.4026519432663918
========> pt_295:  3.048911467194557
========> pt_296:  1.860718671232462
========> pt_297:  4.332905672490597
========> pt_298:  3.143777623772621
========> pt_299:  2.3144839331507683
========> pt_300:  4.0493737533688545
========> pt_301:  3.0690794438123703
========> pt_302:  4.786525294184685
========> pt_303:  3.035646341741085
========> pt_304:  2.4991288781166077
========> pt_305:  3.2184867560863495
========> pt_306:  2.2349508106708527
========> pt_307:  2.5975996628403664
========> pt_308:  3.323468193411827
========> pt_309:  2.2217250615358353
========> pt_310:  3.8164012879133224
========> pt_311:  2.2784337401390076
========> pt_312:  2.2113625705242157
========> pt_313:  2.2028815373778343
========> pt_314:  2.9905320331454277
========> pt_315:  2.7554434165358543
========> pt_316:  3.677245117723942
========> pt_317:  3.8525615260004997
========> pt_318:  5.471254959702492
========> pt_319:  2.1004737727344036
========> pt_320:  2.141218613833189
========> pt_321:  3.9859356358647346
========> pt_322:  2.9560501128435135
========> pt_323:  7.431461811065674
========> pt_324:  2.532702535390854
========> pt_325:  3.1513383984565735
========> pt_326:  2.6719867438077927
========> pt_327:  2.587502636015415
========> pt_328:  2.369656153023243
========> pt_329:  8.258004114031792
========> pt_330:  7.178759202361107
========> pt_331:  3.497054912149906
========> pt_332:  2.204729877412319
========> pt_333:  2.0709960348904133
========> pt_334:  1.8175450153648853
========> pt_335:  4.787206947803497
========> pt_336:  3.43450341373682
========> pt_337:  2.6078487187623978
========> pt_338:  2.160973008722067
========> pt_339:  2.3994942754507065
========> pt_340:  3.139932043850422
===============================================> mean Dose score: 3.154316256009042
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.035988044564,     best is           0.035988044564
            Average val evaluation index is   -3.154316256009,     best is           -3.126164164953
    Train use time   1536.73288
    Train loader use time     84.92361
    Val use time     43.92108
    Total use time   1584.30443
    End lr is 0.000010414321, 0.000010414321
    time: 06:05:41
Epoch: 141, iter: 70499
    Begin lr is 0.000010414321, 0.000010414321
========> pt_241:  2.6711807027459145
========> pt_242:  2.299642451107502
========> pt_243:  4.426544904708862
========> pt_244:  2.4242859706282616
========> pt_245:  3.160356618463993
========> pt_246:  3.384655863046646
========> pt_247:  2.346179783344269
========> pt_248:  2.1304122917354107
========> pt_249:  3.8288619369268417
========> pt_250:  2.223615385591984
========> pt_251:  3.653302490711212
========> pt_252:  3.439525328576565
========> pt_253:  3.473050482571125
========> pt_254:  3.5632162541151047
========> pt_255:  3.4692463651299477
========> pt_256:  2.0813235826790333
========> pt_257:  2.6106207072734833
========> pt_258:  2.1963706240057945
========> pt_259:  2.821362242102623
========> pt_260:  3.5445526614785194
========> pt_261:  3.542412780225277
========> pt_262:  3.212350830435753
========> pt_263:  2.6741858199238777
========> pt_264:  3.7146085500717163
========> pt_265:  2.4778766185045242
========> pt_266:  3.7000998109579086
========> pt_267:  3.2655636221170425
========> pt_268:  3.833817094564438
========> pt_269:  2.395525611937046
========> pt_270:  4.96310330927372
========> pt_271:  3.3856819942593575
========> pt_272:  3.328155539929867
========> pt_273:  2.3991675302386284
========> pt_274:  4.178282171487808
========> pt_275:  3.2621989026665688
========> pt_276:  2.0733987726271152
========> pt_277:  2.2355427592992783
========> pt_278:  4.204344339668751
========> pt_279:  2.26430781185627
========> pt_280:  2.590360939502716
========> pt_281:  2.818247079849243
========> pt_282:  2.4922998249530792
========> pt_283:  5.884808525443077
========> pt_284:  2.6413483172655106
========> pt_285:  2.52591285854578
========> pt_286:  2.797507755458355
========> pt_287:  4.070427566766739
========> pt_288:  2.0017257891595364
========> pt_289:  3.4592168778181076
========> pt_290:  3.7277070432901382
========> pt_291:  2.158313151448965
========> pt_292:  2.8540346771478653
========> pt_293:  2.907199487090111
========> pt_294:  2.4118558317422867
========> pt_295:  3.030836693942547
========> pt_296:  1.8777514062821865
========> pt_297:  4.461955949664116
========> pt_298:  3.078523240983486
========> pt_299:  2.3475172743201256
========> pt_300:  3.982096314430237
========> pt_301:  3.1556841358542442
========> pt_302:  4.957975521683693
========> pt_303:  3.0122390761971474
========> pt_304:  2.523714564740658
========> pt_305:  3.262436203658581
========> pt_306:  2.19069704413414
========> pt_307:  2.6228049397468567
========> pt_308:  3.480660282075405
========> pt_309:  2.23601084202528
========> pt_310:  3.6905861273407936
========> pt_311:  2.2138341516256332
========> pt_312:  2.2698233649134636
========> pt_313:  2.2232281416654587
========> pt_314:  3.0302290990948677
========> pt_315:  2.7822761610150337
========> pt_316:  3.8435161858797073
========> pt_317:  3.7072253599762917
========> pt_318:  5.523807480931282
========> pt_319:  2.150196023285389
========> pt_320:  2.1369117312133312
========> pt_321:  3.846398741006851
========> pt_322:  3.091175816953182
========> pt_323:  7.544628828763962
========> pt_324:  2.553018629550934
========> pt_325:  3.1525548920035362
========> pt_326:  2.739333026111126
========> pt_327:  2.6132307574152946
========> pt_328:  2.371361069381237
========> pt_329:  7.881840318441391
========> pt_330:  6.9458021223545074
========> pt_331:  3.516725078225136
========> pt_332:  2.200288698077202
========> pt_333:  2.1070230193436146
========> pt_334:  1.7968105152249336
========> pt_335:  4.896931275725365
========> pt_336:  3.4241701290011406
========> pt_337:  2.68299151211977
========> pt_338:  2.2193390130996704
========> pt_339:  2.4352234601974487
========> pt_340:  3.1766302511096
===============================================> mean Dose score: 3.171879107132554
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.035735387430,     best is           0.035735387430
            Average val evaluation index is   -3.171879107133,     best is           -3.126164164953
    Train use time   1535.52738
    Train loader use time     84.81657
    Val use time     44.52110
    Total use time   1583.13737
    End lr is 0.000009368209, 0.000009368209
    time: 06:32:04
Epoch: 142, iter: 70999
    Begin lr is 0.000009368209, 0.000009368209
========> pt_241:  2.6173918694257736
========> pt_242:  2.299865670502186
========> pt_243:  4.458841308951378
========> pt_244:  2.4207640066742897
========> pt_245:  3.180971033871174
========> pt_246:  3.3879415690898895
========> pt_247:  2.38094724714756
========> pt_248:  2.220388874411583
========> pt_249:  3.784150257706642
========> pt_250:  2.2160564363002777
========> pt_251:  3.61659437417984
========> pt_252:  3.412518911063671
========> pt_253:  3.4973910450935364
========> pt_254:  3.578661158680916
========> pt_255:  3.209804929792881
========> pt_256:  2.0804350078105927
========> pt_257:  2.601790502667427
========> pt_258:  2.1652740240097046
========> pt_259:  2.644829861819744
========> pt_260:  3.644503578543663
========> pt_261:  3.523048236966133
========> pt_262:  3.2146677747368813
========> pt_263:  2.638830840587616
========> pt_264:  3.698395937681198
========> pt_265:  2.474787011742592
========> pt_266:  3.752788193523884
========> pt_267:  3.2681629806756973
========> pt_268:  3.832332268357277
========> pt_269:  2.4366968125104904
========> pt_270:  4.864502400159836
========> pt_271:  3.3655481785535812
========> pt_272:  3.2274655997753143
========> pt_273:  2.4073755368590355
========> pt_274:  4.149122051894665
========> pt_275:  3.2588407024741173
========> pt_276:  2.04942524433136
========> pt_277:  2.214464694261551
========> pt_278:  4.099541008472443
========> pt_279:  2.3054203391075134
========> pt_280:  2.6500356197357178
========> pt_281:  2.8323951736092567
========> pt_282:  2.512372098863125
========> pt_283:  5.996076613664627
========> pt_284:  2.6418469101190567
========> pt_285:  2.489871270954609
========> pt_286:  2.778915613889694
========> pt_287:  4.061836488544941
========> pt_288:  2.058312688022852
========> pt_289:  3.41150164604187
========> pt_290:  3.72978825122118
========> pt_291:  2.155820317566395
========> pt_292:  2.8335099667310715
========> pt_293:  2.9207204282283783
========> pt_294:  2.471899501979351
========> pt_295:  3.028569035232067
========> pt_296:  1.8625238537788391
========> pt_297:  4.464671090245247
========> pt_298:  3.1290534883737564
========> pt_299:  2.340315841138363
========> pt_300:  4.043259471654892
========> pt_301:  3.1321876868605614
========> pt_302:  5.0118981301784515
========> pt_303:  3.0920470505952835
========> pt_304:  2.545434385538101
========> pt_305:  3.2124288007616997
========> pt_306:  2.1921031177043915
========> pt_307:  2.6323965936899185
========> pt_308:  3.590627908706665
========> pt_309:  2.2395766153931618
========> pt_310:  3.7662311643362045
========> pt_311:  2.2252903133630753
========> pt_312:  2.2422291710972786
========> pt_313:  2.2633202746510506
========> pt_314:  2.988406755030155
========> pt_315:  2.759137488901615
========> pt_316:  3.764156736433506
========> pt_317:  3.734079487621784
========> pt_318:  5.523689612746239
========> pt_319:  2.1042186953127384
========> pt_320:  2.1773114427924156
========> pt_321:  3.9747047796845436
========> pt_322:  3.0881521850824356
========> pt_323:  7.697948217391968
========> pt_324:  2.5942936167120934
========> pt_325:  3.1684603169560432
========> pt_326:  2.7113348990678787
========> pt_327:  2.553914114832878
========> pt_328:  2.4487503990530968
========> pt_329:  7.960160076618195
========> pt_330:  6.73087365925312
========> pt_331:  3.479647971689701
========> pt_332:  2.198042944073677
========> pt_333:  2.08740396425128
========> pt_334:  1.8242276459932327
========> pt_335:  5.14934130012989
========> pt_336:  3.4372996538877487
========> pt_337:  2.686782591044903
========> pt_338:  2.2263672947883606
========> pt_339:  2.443324290215969
========> pt_340:  3.1796883046627045
===============================================> mean Dose score: 3.174513225071132
        ==> Saving latest model successfully !
            Average train loss is             0.036163640670,     best is           0.035735387430
            Average val evaluation index is   -3.174513225071,     best is           -3.126164164953
    Train use time   1536.22276
    Train loader use time     84.37662
    Val use time     44.48997
    Total use time   1582.28583
    End lr is 0.000008376333, 0.000008376333
    time: 06:58:26
Epoch: 143, iter: 71499
    Begin lr is 0.000008376333, 0.000008376333
========> pt_241:  2.579953335225582
========> pt_242:  2.2814487665891647
========> pt_243:  4.489229395985603
========> pt_244:  2.430107146501541
========> pt_245:  3.1708841770887375
========> pt_246:  3.41273196041584
========> pt_247:  2.3490602523088455
========> pt_248:  2.210826426744461
========> pt_249:  3.870478793978691
========> pt_250:  2.2447969764471054
========> pt_251:  3.6422445252537727
========> pt_252:  3.4233959019184113
========> pt_253:  3.57694111764431
========> pt_254:  3.5630052909255028
========> pt_255:  3.2296185195446014
========> pt_256:  2.110472358763218
========> pt_257:  2.6582809165120125
========> pt_258:  2.2107523679733276
========> pt_259:  2.7040481939911842
========> pt_260:  3.672204166650772
========> pt_261:  3.6274194717407227
========> pt_262:  3.215881660580635
========> pt_263:  2.6425178721547127
========> pt_264:  3.6877111345529556
========> pt_265:  2.498740330338478
========> pt_266:  3.665893003344536
========> pt_267:  3.2612429186701775
========> pt_268:  3.856167197227478
========> pt_269:  2.416476421058178
========> pt_270:  4.875118359923363
========> pt_271:  3.4435904771089554
========> pt_272:  3.2382192462682724
========> pt_273:  2.435496747493744
========> pt_274:  4.0912114828825
========> pt_275:  3.258395828306675
========> pt_276:  2.0420833863317966
========> pt_277:  2.2591914981603622
========> pt_278:  4.256792552769184
========> pt_279:  2.3099465295672417
========> pt_280:  2.65278909355402
========> pt_281:  2.8398294746875763
========> pt_282:  2.490357868373394
========> pt_283:  5.916359126567841
========> pt_284:  2.5870074331760406
========> pt_285:  2.510286457836628
========> pt_286:  2.7921004220843315
========> pt_287:  4.109889157116413
========> pt_288:  2.0630763098597527
========> pt_289:  3.528686612844467
========> pt_290:  3.7462032213807106
========> pt_291:  2.214367166161537
========> pt_292:  2.8397655859589577
========> pt_293:  2.934352457523346
========> pt_294:  2.3962946236133575
========> pt_295:  3.064943626523018
========> pt_296:  1.8816881254315376
========> pt_297:  4.533657878637314
========> pt_298:  3.118651360273361
========> pt_299:  2.347719371318817
========> pt_300:  4.050201438367367
========> pt_301:  3.1665994599461555
========> pt_302:  5.093262121081352
========> pt_303:  3.014429807662964
========> pt_304:  2.5627996027469635
========> pt_305:  3.2499995455145836
========> pt_306:  2.222091183066368
========> pt_307:  2.6025595143437386
========> pt_308:  3.5225600749254227
========> pt_309:  2.2397414222359657
========> pt_310:  3.7550488114356995
========> pt_311:  2.2580214217305183
========> pt_312:  2.268160432577133
========> pt_313:  2.268494740128517
========> pt_314:  3.024318218231201
========> pt_315:  2.794734202325344
========> pt_316:  3.8535568863153458
========> pt_317:  3.765427991747856
========> pt_318:  5.526040196418762
========> pt_319:  2.1280602738261223
========> pt_320:  2.115048225969076
========> pt_321:  4.042787477374077
========> pt_322:  3.16443532705307
========> pt_323:  7.743544429540634
========> pt_324:  2.5944552943110466
========> pt_325:  3.2049963250756264
========> pt_326:  2.7182114124298096
========> pt_327:  2.6001812890172005
========> pt_328:  2.418953999876976
========> pt_329:  7.446371614933014
========> pt_330:  6.721619442105293
========> pt_331:  3.5407472401857376
========> pt_332:  2.173750624060631
========> pt_333:  2.073555625975132
========> pt_334:  1.806621477007866
========> pt_335:  4.905244633555412
========> pt_336:  3.494930677115917
========> pt_337:  2.712133899331093
========> pt_338:  2.2471851110458374
========> pt_339:  2.4886247888207436
========> pt_340:  3.222780078649521
===============================================> mean Dose score: 3.1828288841992616
        ==> Saving latest model successfully !
            Average train loss is             0.035750872247,     best is           0.035735387430
            Average val evaluation index is   -3.182828884199,     best is           -3.126164164953
    Train use time   1534.52123
    Train loader use time     84.76698
    Val use time     44.27586
    Total use time   1580.38724
    End lr is 0.000007439075, 0.000007439075
    time: 07:24:46
Epoch: 144, iter: 71999
    Begin lr is 0.000007439075, 0.000007439075
========> pt_241:  2.547689527273178
========> pt_242:  2.2943271696567535
========> pt_243:  4.424837902188301
========> pt_244:  2.390248402953148
========> pt_245:  3.1587176769971848
========> pt_246:  3.3839624747633934
========> pt_247:  2.344474606215954
========> pt_248:  2.219039388000965
========> pt_249:  3.826816715300083
========> pt_250:  2.255586087703705
========> pt_251:  3.5968590155243874
========> pt_252:  3.4137792140245438
========> pt_253:  3.575691245496273
========> pt_254:  3.5230036452412605
========> pt_255:  3.171081840991974
========> pt_256:  2.0973228849470615
========> pt_257:  2.592269778251648
========> pt_258:  2.2063766419887543
========> pt_259:  2.620403505861759
========> pt_260:  3.7177902087569237
========> pt_261:  3.523034155368805
========> pt_262:  3.184994198381901
========> pt_263:  2.6257717236876488
========> pt_264:  3.6909058317542076
========> pt_265:  2.465561479330063
========> pt_266:  3.631149008870125
========> pt_267:  3.2721013948321342
========> pt_268:  3.8136259093880653
========> pt_269:  2.427937276661396
========> pt_270:  4.784594550728798
========> pt_271:  3.3748532459139824
========> pt_272:  3.0951622128486633
========> pt_273:  2.41463303565979
========> pt_274:  4.084876589477062
========> pt_275:  3.215108998119831
========> pt_276:  2.0237394981086254
========> pt_277:  2.221527397632599
========> pt_278:  4.128620028495789
========> pt_279:  2.3602264374494553
========> pt_280:  2.5731605291366577
========> pt_281:  2.8235626220703125
========> pt_282:  2.4732542037963867
========> pt_283:  6.058987975120544
========> pt_284:  2.5554437935352325
========> pt_285:  2.474665232002735
========> pt_286:  2.7968618273735046
========> pt_287:  4.054899215698242
========> pt_288:  2.05599470064044
========> pt_289:  3.4521223604679108
========> pt_290:  3.728678412735462
========> pt_291:  2.18830943107605
========> pt_292:  2.8393715620040894
========> pt_293:  2.9347746446728706
========> pt_294:  2.3823973909020424
========> pt_295:  3.0196256563067436
========> pt_296:  1.8695508316159248
========> pt_297:  4.500432088971138
========> pt_298:  3.069993183016777
========> pt_299:  2.3402496054768562
========> pt_300:  3.9620138704776764
========> pt_301:  3.123946040868759
========> pt_302:  4.999473467469215
========> pt_303:  2.993910573422909
========> pt_304:  2.5442880392074585
========> pt_305:  3.2246440649032593
========> pt_306:  2.2352976351976395
========> pt_307:  2.6035699993371964
========> pt_308:  3.5420025885105133
========> pt_309:  2.2290610522031784
========> pt_310:  3.7073296681046486
========> pt_311:  2.255389206111431
========> pt_312:  2.266073487699032
========> pt_313:  2.239052727818489
========> pt_314:  3.0172237008810043
========> pt_315:  2.7998054027557373
========> pt_316:  3.7842663004994392
========> pt_317:  3.667423203587532
========> pt_318:  5.637262389063835
========> pt_319:  2.077355571091175
========> pt_320:  2.131661381572485
========> pt_321:  3.8593514636158943
========> pt_322:  3.1548162922263145
========> pt_323:  7.824829667806625
========> pt_324:  2.581162005662918
========> pt_325:  3.174266368150711
========> pt_326:  2.751711532473564
========> pt_327:  2.578423134982586
========> pt_328:  2.4184736609458923
========> pt_329:  7.41755336523056
========> pt_330:  6.699910834431648
========> pt_331:  3.519137464463711
========> pt_332:  2.18881506472826
========> pt_333:  2.0724478736519814
========> pt_334:  1.7998825199902058
========> pt_335:  4.871685057878494
========> pt_336:  3.4219011664390564
========> pt_337:  2.736463248729706
========> pt_338:  2.23231703042984
========> pt_339:  2.434602826833725
========> pt_340:  3.1806935742497444
===============================================> mean Dose score: 3.1585053069517017
        ==> Saving latest model successfully !
            Average train loss is             0.035966320626,     best is           0.035735387430
            Average val evaluation index is   -3.158505306952,     best is           -3.126164164953
    Train use time   1534.40526
    Train loader use time     84.44539
    Val use time     44.18089
    Total use time   1580.13802
    End lr is 0.000006556797, 0.000006556797
    time: 07:51:07
Epoch: 145, iter: 72499
    Begin lr is 0.000006556797, 0.000006556797
========> pt_241:  2.643926553428173
========> pt_242:  2.3151950538158417
========> pt_243:  4.430258274078369
========> pt_244:  2.3849547654390335
========> pt_245:  3.151309974491596
========> pt_246:  3.421812504529953
========> pt_247:  2.3748136684298515
========> pt_248:  2.217469811439514
========> pt_249:  3.7939831241965294
========> pt_250:  2.192341201007366
========> pt_251:  3.596954196691513
========> pt_252:  3.4000207111239433
========> pt_253:  3.5006848350167274
========> pt_254:  3.5695988684892654
========> pt_255:  3.334580659866333
========> pt_256:  2.1050805412232876
========> pt_257:  2.668725550174713
========> pt_258:  2.153459172695875
========> pt_259:  2.7642493695020676
========> pt_260:  3.664095774292946
========> pt_261:  3.5432201251387596
========> pt_262:  3.169955834746361
========> pt_263:  2.6657671108841896
========> pt_264:  3.638024218380451
========> pt_265:  2.461385503411293
========> pt_266:  3.6592934280633926
========> pt_267:  3.2842248678207397
========> pt_268:  3.78789022564888
========> pt_269:  2.4156354367733
========> pt_270:  4.789251387119293
========> pt_271:  3.5170940682291985
========> pt_272:  3.2780039310455322
========> pt_273:  2.4073100835084915
========> pt_274:  4.224125854671001
========> pt_275:  3.2001937180757523
========> pt_276:  2.0862960815429688
========> pt_277:  2.2163445875048637
========> pt_278:  4.133254699409008
========> pt_279:  2.3666254803538322
========> pt_280:  2.6249635964632034
========> pt_281:  2.841445468366146
========> pt_282:  2.498967722058296
========> pt_283:  5.985433533787727
========> pt_284:  2.706400863826275
========> pt_285:  2.5456101447343826
========> pt_286:  2.7672216296195984
========> pt_287:  4.033294655382633
========> pt_288:  2.0683912001550198
========> pt_289:  3.4566420316696167
========> pt_290:  3.7192771211266518
========> pt_291:  2.193172536790371
========> pt_292:  2.798006609082222
========> pt_293:  2.9011524841189384
========> pt_294:  2.455584667623043
========> pt_295:  2.999374493956566
========> pt_296:  1.8718976341187954
========> pt_297:  4.445854425430298
========> pt_298:  3.116205856204033
========> pt_299:  2.3295434191823006
========> pt_300:  4.056315720081329
========> pt_301:  3.133971095085144
========> pt_302:  4.997923448681831
========> pt_303:  3.055241145193577
========> pt_304:  2.5383221358060837
========> pt_305:  3.265148475766182
========> pt_306:  2.1933331713080406
========> pt_307:  2.6057370007038116
========> pt_308:  3.4576227888464928
========> pt_309:  2.233625054359436
========> pt_310:  3.7559518590569496
========> pt_311:  2.2181905806064606
========> pt_312:  2.2534357756376266
========> pt_313:  2.233429215848446
========> pt_314:  3.009357564151287
========> pt_315:  2.7909626811742783
========> pt_316:  3.697025068104267
========> pt_317:  3.797415643930435
========> pt_318:  5.509872958064079
========> pt_319:  2.0828648656606674
========> pt_320:  2.1470066718757153
========> pt_321:  4.020815491676331
========> pt_322:  3.06928388774395
========> pt_323:  7.6245710998773575
========> pt_324:  2.5528619065880775
========> pt_325:  3.1975429877638817
========> pt_326:  2.715003155171871
========> pt_327:  2.5719787180423737
========> pt_328:  2.3927145078778267
========> pt_329:  7.54823736846447
========> pt_330:  6.72804482281208
========> pt_331:  3.50063294172287
========> pt_332:  2.17816311866045
========> pt_333:  2.069911751896143
========> pt_334:  1.8143552727997303
========> pt_335:  4.881823807954788
========> pt_336:  3.4132694080471992
========> pt_337:  2.673097364604473
========> pt_338:  2.2058316320180893
========> pt_339:  2.4539146944880486
========> pt_340:  3.1643346697092056
===============================================> mean Dose score: 3.166949928738177
        ==> Saving latest model successfully !
            Average train loss is             0.036003540292,     best is           0.035735387430
            Average val evaluation index is   -3.166949928738,     best is           -3.126164164953
    Train use time   1534.47826
    Train loader use time     84.56138
    Val use time     44.03524
    Total use time   1580.06177
    End lr is 0.000005729837, 0.000005729837
    time: 08:17:27
Epoch: 146, iter: 72999
    Begin lr is 0.000005729837, 0.000005729837
========> pt_241:  2.6156027242541313
========> pt_242:  2.2780827432870865
========> pt_243:  4.400930479168892
========> pt_244:  2.4091047048568726
========> pt_245:  3.097372241318226
========> pt_246:  3.374655321240425
========> pt_247:  2.3284872993826866
========> pt_248:  2.1320311538875103
========> pt_249:  3.7136510014533997
========> pt_250:  2.2217344492673874
========> pt_251:  3.6528028547763824
========> pt_252:  3.3522960916161537
========> pt_253:  3.577229268848896
========> pt_254:  3.515537269413471
========> pt_255:  3.3390703424811363
========> pt_256:  2.062938492745161
========> pt_257:  2.5818415731191635
========> pt_258:  2.1822885051369667
========> pt_259:  2.761049196124077
========> pt_260:  3.653349429368973
========> pt_261:  3.5520505905151367
========> pt_262:  3.1843435764312744
========> pt_263:  2.615293711423874
========> pt_264:  3.605276942253113
========> pt_265:  2.452072612941265
========> pt_266:  3.6358431354165077
========> pt_267:  3.2048604637384415
========> pt_268:  3.7861456722021103
========> pt_269:  2.373790666460991
========> pt_270:  4.82291005551815
========> pt_271:  3.292488418519497
========> pt_272:  3.235391713678837
========> pt_273:  2.421604208648205
========> pt_274:  4.123232252895832
========> pt_275:  3.1971507892012596
========> pt_276:  2.030897121876478
========> pt_277:  2.2426359727978706
========> pt_278:  4.066611975431442
========> pt_279:  2.3058712109923363
========> pt_280:  2.6081248745322227
========> pt_281:  2.8217891231179237
========> pt_282:  2.4810494109988213
========> pt_283:  6.0386572778224945
========> pt_284:  2.6149260252714157
========> pt_285:  2.47739315032959
========> pt_286:  2.7870923280715942
========> pt_287:  4.079510979354382
========> pt_288:  2.058879993855953
========> pt_289:  3.3338463306427
========> pt_290:  3.7290745228528976
========> pt_291:  2.1964439004659653
========> pt_292:  2.798333875834942
========> pt_293:  2.862350381910801
========> pt_294:  2.3725131526589394
========> pt_295:  3.022729866206646
========> pt_296:  1.8641058169305325
========> pt_297:  4.465475305914879
========> pt_298:  3.1248535215854645
========> pt_299:  2.342347241938114
========> pt_300:  4.196736104786396
========> pt_301:  3.136846087872982
========> pt_302:  4.908996075391769
========> pt_303:  2.984037809073925
========> pt_304:  2.5220784917473793
========> pt_305:  3.1983133032917976
========> pt_306:  2.21124991774559
========> pt_307:  2.5760816782712936
========> pt_308:  3.4584403038024902
========> pt_309:  2.221454381942749
========> pt_310:  3.8149049878120422
========> pt_311:  2.287624329328537
========> pt_312:  2.2325462475419044
========> pt_313:  2.26275023072958
========> pt_314:  2.994629517197609
========> pt_315:  2.797079309821129
========> pt_316:  3.709866963326931
========> pt_317:  3.8659127056598663
========> pt_318:  5.525486320257187
========> pt_319:  2.1030106768012047
========> pt_320:  2.1394198201596737
========> pt_321:  3.764290511608124
========> pt_322:  3.10999508947134
========> pt_323:  7.645405605435371
========> pt_324:  2.5753481313586235
========> pt_325:  3.1277358159422874
========> pt_326:  2.68155075609684
========> pt_327:  2.589181736111641
========> pt_328:  2.3725254088640213
========> pt_329:  7.442709356546402
========> pt_330:  6.821960732340813
========> pt_331:  3.4370626136660576
========> pt_332:  2.206486687064171
========> pt_333:  2.0712334662675858
========> pt_334:  1.7870348878204823
========> pt_335:  4.834229052066803
========> pt_336:  3.4657470881938934
========> pt_337:  2.6968832686543465
========> pt_338:  2.225499451160431
========> pt_339:  2.4238431826233864
========> pt_340:  3.145262710750103
===============================================> mean Dose score: 3.1504947412759066
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.035731985934,     best is           0.035731985934
            Average val evaluation index is   -3.150494741276,     best is           -3.126164164953
    Train use time   1538.95974
    Train loader use time     88.69897
    Val use time     44.06815
    Total use time   1586.06118
    End lr is 0.000004958516, 0.000004958516
    time: 08:43:53
Epoch: 147, iter: 73499
    Begin lr is 0.000004958516, 0.000004958516
========> pt_241:  2.5739863887429237
========> pt_242:  2.284945957362652
========> pt_243:  4.39071349799633
========> pt_244:  2.4185824021697044
========> pt_245:  3.100760690867901
========> pt_246:  3.3716807141900063
========> pt_247:  2.337985336780548
========> pt_248:  2.1858476288616657
========> pt_249:  3.7911884486675262
========> pt_250:  2.2567084431648254
========> pt_251:  3.573622554540634
========> pt_252:  3.360235244035721
========> pt_253:  3.4940247610211372
========> pt_254:  3.464016355574131
========> pt_255:  3.3250831440091133
========> pt_256:  2.073463574051857
========> pt_257:  2.5026151165366173
========> pt_258:  2.2190701588988304
========> pt_259:  2.8096693009138107
========> pt_260:  3.6919525638222694
========> pt_261:  3.4805885702371597
========> pt_262:  3.1392965465784073
========> pt_263:  2.623021639883518
========> pt_264:  3.6295676976442337
========> pt_265:  2.4615907296538353
========> pt_266:  3.6396973207592964
========> pt_267:  3.2289136573672295
========> pt_268:  3.7592680752277374
========> pt_269:  2.4329688400030136
========> pt_270:  4.7355446964502335
========> pt_271:  3.26002798974514
========> pt_272:  3.242933973670006
========> pt_273:  2.412274368107319
========> pt_274:  4.110874347388744
========> pt_275:  3.179437182843685
========> pt_276:  2.045408207923174
========> pt_277:  2.2059933096170425
========> pt_278:  4.121352098882198
========> pt_279:  2.3056573793292046
========> pt_280:  2.6041097939014435
========> pt_281:  2.8172188624739647
========> pt_282:  2.464475631713867
========> pt_283:  6.0748521983623505
========> pt_284:  2.6263193413615227
========> pt_285:  2.446853555738926
========> pt_286:  2.746838256716728
========> pt_287:  4.0810127556324005
========> pt_288:  2.029825486242771
========> pt_289:  3.4155628830194473
========> pt_290:  3.68185605853796
========> pt_291:  2.1663757786154747
========> pt_292:  2.8069760650396347
========> pt_293:  2.8992003574967384
========> pt_294:  2.38704901188612
========> pt_295:  3.0020445212721825
========> pt_296:  1.8490999191999435
========> pt_297:  4.457193240523338
========> pt_298:  3.0616365373134613
========> pt_299:  2.3199155181646347
========> pt_300:  3.9577241986989975
========> pt_301:  3.1258608773350716
========> pt_302:  5.000052899122238
========> pt_303:  3.002742864191532
========> pt_304:  2.524363361299038
========> pt_305:  3.2252558320760727
========> pt_306:  2.2434334084391594
========> pt_307:  2.5814618915319443
========> pt_308:  3.4502244740724564
========> pt_309:  2.210579998791218
========> pt_310:  3.703373782336712
========> pt_311:  2.3057152703404427
========> pt_312:  2.2702862322330475
========> pt_313:  2.25495059043169
========> pt_314:  2.985645979642868
========> pt_315:  2.7504676580429077
========> pt_316:  3.7456130981445312
========> pt_317:  3.6850327625870705
========> pt_318:  5.515761151909828
========> pt_319:  2.0941729098558426
========> pt_320:  2.1376395411789417
========> pt_321:  3.848327659070492
========> pt_322:  3.101014941930771
========> pt_323:  7.675936073064804
========> pt_324:  2.511937655508518
========> pt_325:  3.1687961891293526
========> pt_326:  2.7200480177998543
========> pt_327:  2.567686438560486
========> pt_328:  2.413165159523487
========> pt_329:  7.595325708389282
========> pt_330:  6.849648803472519
========> pt_331:  3.476141393184662
========> pt_332:  2.160214688628912
========> pt_333:  2.046626005321741
========> pt_334:  1.8011879362165928
========> pt_335:  4.884928539395332
========> pt_336:  3.4232089295983315
========> pt_337:  2.690597139298916
========> pt_338:  2.216506525874138
========> pt_339:  2.4615542218089104
========> pt_340:  3.2154133170843124
===============================================> mean Dose score: 3.1477760881185533
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.035615563083,     best is           0.035615563083
            Average val evaluation index is   -3.147776088119,     best is           -3.126164164953
    Train use time   1537.98290
    Train loader use time     84.68669
    Val use time     44.32481
    Total use time   1585.26445
    End lr is 0.000004243130, 0.000004243130
    time: 09:10:18
Epoch: 148, iter: 73999
    Begin lr is 0.000004243130, 0.000004243130
========> pt_241:  2.537948451936245
========> pt_242:  2.29145348072052
========> pt_243:  4.386386796832085
========> pt_244:  2.4427662417292595
========> pt_245:  3.130379766225815
========> pt_246:  3.3791452646255493
========> pt_247:  2.2831617668271065
========> pt_248:  2.2700178995728493
========> pt_249:  3.7768106162548065
========> pt_250:  2.271183282136917
========> pt_251:  3.6734983697533607
========> pt_252:  3.4017660468816757
========> pt_253:  3.4790268167853355
========> pt_254:  3.4490932524204254
========> pt_255:  3.3217046037316322
========> pt_256:  2.048744633793831
========> pt_257:  2.4472950398921967
========> pt_258:  2.226114347577095
========> pt_259:  2.6825450733304024
========> pt_260:  3.6605821549892426
========> pt_261:  3.466341905295849
========> pt_262:  3.125910684466362
========> pt_263:  2.6213399320840836
========> pt_264:  3.648960664868355
========> pt_265:  2.4619657173752785
========> pt_266:  3.6806001886725426
========> pt_267:  3.21993924677372
========> pt_268:  3.748384825885296
========> pt_269:  2.44759414345026
========> pt_270:  4.805376902222633
========> pt_271:  3.1384965032339096
========> pt_272:  3.1259628385305405
========> pt_273:  2.414546199142933
========> pt_274:  4.081014059484005
========> pt_275:  3.1851178035140038
========> pt_276:  2.024943083524704
========> pt_277:  2.245367541909218
========> pt_278:  4.076327756047249
========> pt_279:  2.282710373401642
========> pt_280:  2.5780416280031204
========> pt_281:  2.8261103481054306
========> pt_282:  2.482760064303875
========> pt_283:  6.076072081923485
========> pt_284:  2.5171898305416107
========> pt_285:  2.48016644269228
========> pt_286:  2.7559028938412666
========> pt_287:  4.165090061724186
========> pt_288:  2.000813093036413
========> pt_289:  3.340017721056938
========> pt_290:  3.7055595591664314
========> pt_291:  2.0925425738096237
========> pt_292:  2.783992551267147
========> pt_293:  2.9235536977648735
========> pt_294:  2.383154146373272
========> pt_295:  2.9974226281046867
========> pt_296:  1.8513574078679085
========> pt_297:  4.490734562277794
========> pt_298:  3.0833302810788155
========> pt_299:  2.321079596877098
========> pt_300:  3.9247823879122734
========> pt_301:  3.1309805810451508
========> pt_302:  5.046739652752876
========> pt_303:  3.001321665942669
========> pt_304:  2.5284459814429283
========> pt_305:  3.2550230249762535
========> pt_306:  2.2023477405309677
========> pt_307:  2.590559646487236
========> pt_308:  3.492671102285385
========> pt_309:  2.244572453200817
========> pt_310:  3.7313972041010857
========> pt_311:  2.262384369969368
========> pt_312:  2.2921327874064445
========> pt_313:  2.2728206589818
========> pt_314:  3.003004416823387
========> pt_315:  2.7647756040096283
========> pt_316:  3.7188176438212395
========> pt_317:  3.644995130598545
========> pt_318:  5.491814091801643
========> pt_319:  2.1211406029760838
========> pt_320:  2.1572381258010864
========> pt_321:  3.7887369468808174
========> pt_322:  3.0995994806289673
========> pt_323:  7.877956405282021
========> pt_324:  2.5273992493748665
========> pt_325:  3.2053136825561523
========> pt_326:  2.7280544489622116
========> pt_327:  2.5814929232001305
========> pt_328:  2.4640294536948204
========> pt_329:  7.524238675832748
========> pt_330:  6.803567558526993
========> pt_331:  3.466757833957672
========> pt_332:  2.1910542994737625
========> pt_333:  2.096705250442028
========> pt_334:  1.7963765934109688
========> pt_335:  4.933176264166832
========> pt_336:  3.4190621599555016
========> pt_337:  2.760634310543537
========> pt_338:  2.1988625451922417
========> pt_339:  2.4331388622522354
========> pt_340:  3.2353507727384567
===============================================> mean Dose score: 3.14824888035655
        ==> Saving latest model successfully !
            Average train loss is             0.035723573405,     best is           0.035615563083
            Average val evaluation index is   -3.148248880357,     best is           -3.126164164953
    Train use time   1539.35860
    Train loader use time     86.70948
    Val use time     44.65363
    Total use time   1585.52316
    End lr is 0.000003583956, 0.000003583956
    time: 09:36:43
Epoch: 149, iter: 74499
    Begin lr is 0.000003583956, 0.000003583956
========> pt_241:  2.5724637508392334
========> pt_242:  2.283592037856579
========> pt_243:  4.4180260598659515
========> pt_244:  2.4265116453170776
========> pt_245:  3.15831009298563
========> pt_246:  3.397126942873001
========> pt_247:  2.32796523720026
========> pt_248:  2.1953557059168816
========> pt_249:  3.7784550338983536
========> pt_250:  2.1945183724164963
========> pt_251:  3.68604376912117
========> pt_252:  3.4656618162989616
========> pt_253:  3.526949882507324
========> pt_254:  3.52392990142107
========> pt_255:  3.353193663060665
========> pt_256:  2.105806265026331
========> pt_257:  2.516302950680256
========> pt_258:  2.207542285323143
========> pt_259:  2.767479792237282
========> pt_260:  3.6016345024108887
========> pt_261:  3.5303106904029846
========> pt_262:  3.197862170636654
========> pt_263:  2.6729286462068558
========> pt_264:  3.7531668320298195
========> pt_265:  2.5145357102155685
========> pt_266:  3.7291470170021057
========> pt_267:  3.345497287809849
========> pt_268:  3.7775884941220284
========> pt_269:  2.419201210141182
========> pt_270:  4.776882007718086
========> pt_271:  3.2955730706453323
========> pt_272:  3.128397651016712
========> pt_273:  2.3970161750912666
========> pt_274:  4.177872501313686
========> pt_275:  3.2304884493350983
========> pt_276:  2.0369835011661053
========> pt_277:  2.253906987607479
========> pt_278:  4.207354411482811
========> pt_279:  2.270059883594513
========> pt_280:  2.5988224148750305
========> pt_281:  2.8878414630889893
========> pt_282:  2.493080049753189
========> pt_283:  5.963240414857864
========> pt_284:  2.572067119181156
========> pt_285:  2.5694891437888145
========> pt_286:  2.806268334388733
========> pt_287:  4.109957218170166
========> pt_288:  2.0646908693015575
========> pt_289:  3.5065873712301254
========> pt_290:  3.7335626408457756
========> pt_291:  2.140669822692871
========> pt_292:  2.8332259878516197
========> pt_293:  2.9441018775105476
========> pt_294:  2.431289479136467
========> pt_295:  2.9705804958939552
========> pt_296:  1.8872451409697533
========> pt_297:  4.541927948594093
========> pt_298:  3.0608052015304565
========> pt_299:  2.3576338589191437
========> pt_300:  3.9404116570949554
========> pt_301:  3.174695074558258
========> pt_302:  5.014841705560684
========> pt_303:  3.0938951298594475
========> pt_304:  2.5322508811950684
========> pt_305:  3.2228898629546165
========> pt_306:  2.1891582384705544
========> pt_307:  2.630744092166424
========> pt_308:  3.55144951492548
========> pt_309:  2.2689570859074593
========> pt_310:  3.661016598343849
========> pt_311:  2.179032526910305
========> pt_312:  2.2801636904478073
========> pt_313:  2.271752282977104
========> pt_314:  3.069523014128208
========> pt_315:  2.8402026370167732
========> pt_316:  3.775126039981842
========> pt_317:  3.6562593653798103
========> pt_318:  5.576109141111374
========> pt_319:  2.1134477481245995
========> pt_320:  2.157393414527178
========> pt_321:  4.077297821640968
========> pt_322:  3.1443729624152184
========> pt_323:  7.7049655467271805
========> pt_324:  2.5822963565587997
========> pt_325:  3.1409287080168724
========> pt_326:  2.722114883363247
========> pt_327:  2.609892636537552
========> pt_328:  2.384556569159031
========> pt_329:  8.095550462603569
========> pt_330:  6.634282767772675
========> pt_331:  3.542139232158661
========> pt_332:  2.2162504494190216
========> pt_333:  2.0878487080335617
========> pt_334:  1.8175802193582058
========> pt_335:  4.980776757001877
========> pt_336:  3.373073749244213
========> pt_337:  2.698483355343342
========> pt_338:  2.206508591771126
========> pt_339:  2.420908994972706
========> pt_340:  3.1986702978610992
===============================================> mean Dose score: 3.1753254802897573
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.035386417422,     best is           0.035386417422
            Average val evaluation index is   -3.175325480290,     best is           -3.126164164953
    Train use time   1533.81027
    Train loader use time     76.29989
    Val use time     44.19182
    Total use time   1581.16835
    End lr is 0.000002981247, 0.000002981247
    time: 10:03:05
Epoch: 150, iter: 74999
    Begin lr is 0.000002981247, 0.000002981247
========> pt_241:  2.619839198887348
========> pt_242:  2.2712448239326477
========> pt_243:  4.426900073885918
========> pt_244:  2.416190356016159
========> pt_245:  3.0867912247776985
========> pt_246:  3.3688732609152794
========> pt_247:  2.296598218381405
========> pt_248:  2.1697597950696945
========> pt_249:  3.765065521001816
========> pt_250:  2.204907462000847
========> pt_251:  3.667786195874214
========> pt_252:  3.3732588961720467
========> pt_253:  3.485898897051811
========> pt_254:  3.490115813910961
========> pt_255:  3.34325909614563
========> pt_256:  2.0579940266907215
========> pt_257:  2.592158690094948
========> pt_258:  2.211970165371895
========> pt_259:  2.762226313352585
========> pt_260:  3.5588084533810616
========> pt_261:  3.485458977520466
========> pt_262:  3.1557029113173485
========> pt_263:  2.668767012655735
========> pt_264:  3.6367354914546013
========> pt_265:  2.480413392186165
========> pt_266:  3.6909718066453934
========> pt_267:  3.2155611738562584
========> pt_268:  3.7343892827630043
========> pt_269:  2.4178650230169296
========> pt_270:  4.796193614602089
========> pt_271:  3.3089112117886543
========> pt_272:  3.2433462515473366
========> pt_273:  2.3874399065971375
========> pt_274:  4.090312868356705
========> pt_275:  3.17923691123724
========> pt_276:  2.0517557486891747
========> pt_277:  2.227672189474106
========> pt_278:  4.033918417990208
========> pt_279:  2.272830829024315
========> pt_280:  2.587032727897167
========> pt_281:  2.8489621728658676
========> pt_282:  2.487461492419243
========> pt_283:  5.987960919737816
========> pt_284:  2.5827086344361305
========> pt_285:  2.524532862007618
========> pt_286:  2.75336142629385
========> pt_287:  4.1036606580019
========> pt_288:  2.022639438509941
========> pt_289:  3.3904308825731277
========> pt_290:  3.667769245803356
========> pt_291:  2.141423709690571
========> pt_292:  2.8120947256684303
========> pt_293:  2.8768492117524147
========> pt_294:  2.3872990906238556
========> pt_295:  2.9709095880389214
========> pt_296:  1.847673375159502
========> pt_297:  4.442783072590828
========> pt_298:  3.0981936678290367
========> pt_299:  2.310926243662834
========> pt_300:  3.997032195329666
========> pt_301:  3.120681717991829
========> pt_302:  4.9195776134729385
========> pt_303:  3.02497535943985
========> pt_304:  2.4973300844430923
========> pt_305:  3.2186351343989372
========> pt_306:  2.196739614009857
========> pt_307:  2.5720037519931793
========> pt_308:  3.460734821856022
========> pt_309:  2.2376132756471634
========> pt_310:  3.70673980563879
========> pt_311:  2.276017963886261
========> pt_312:  2.2359688580036163
========> pt_313:  2.249768301844597
========> pt_314:  2.988642491400242
========> pt_315:  2.771639861166477
========> pt_316:  3.692546598613262
========> pt_317:  3.678727075457573
========> pt_318:  5.537095293402672
========> pt_319:  2.118899282068014
========> pt_320:  2.1750512160360813
========> pt_321:  3.922681361436844
========> pt_322:  3.0490893125534058
========> pt_323:  7.678052484989166
========> pt_324:  2.53017645329237
========> pt_325:  3.1271975859999657
========> pt_326:  2.7113156020641327
========> pt_327:  2.6057223975658417
========> pt_328:  2.3482834175229073
========> pt_329:  7.532669380307198
========> pt_330:  6.815664172172546
========> pt_331:  3.43741986900568
========> pt_332:  2.1992315351963043
========> pt_333:  2.0879261568188667
========> pt_334:  1.7853487469255924
========> pt_335:  4.890437051653862
========> pt_336:  3.364029973745346
========> pt_337:  2.686186730861664
========> pt_338:  2.1641562320291996
========> pt_339:  2.405962161719799
========> pt_340:  3.173651210963726
===============================================> mean Dose score: 3.142834248021245
        ==> Saving latest model successfully !
            Average train loss is             0.035850229036,     best is           0.035386417422
            Average val evaluation index is   -3.142834248021,     best is           -3.126164164953
    Train use time   1530.52835
    Train loader use time     72.25361
    Val use time     44.13786
    Total use time   1576.13141
    End lr is 0.000002435236, 0.000002435236
    time: 10:29:21
Epoch: 151, iter: 75499
    Begin lr is 0.000002435236, 0.000002435236
========> pt_241:  2.5711458176374435
========> pt_242:  2.2692715749144554
========> pt_243:  4.383269026875496
========> pt_244:  2.398018054664135
========> pt_245:  3.1575238704681396
========> pt_246:  3.3533569052815437
========> pt_247:  2.295668311417103
========> pt_248:  2.2130481898784637
========> pt_249:  3.7790847942233086
========> pt_250:  2.213202826678753
========> pt_251:  3.6270298808813095
========> pt_252:  3.4266701340675354
========> pt_253:  3.443913571536541
========> pt_254:  3.4641704708337784
========> pt_255:  3.314308375120163
========> pt_256:  2.062872778624296
========> pt_257:  2.576330453157425
========> pt_258:  2.196931801736355
========> pt_259:  2.7462872490286827
========> pt_260:  3.6054547876119614
========> pt_261:  3.4464260935783386
========> pt_262:  3.1857478246092796
========> pt_263:  2.623005211353302
========> pt_264:  3.6591750383377075
========> pt_265:  2.4726134911179543
========> pt_266:  3.660360500216484
========> pt_267:  3.2587087526917458
========> pt_268:  3.7218962982296944
========> pt_269:  2.4154187366366386
========> pt_270:  4.786339104175568
========> pt_271:  3.1982582807540894
========> pt_272:  3.1556137278676033
========> pt_273:  2.4002818018198013
========> pt_274:  4.139097258448601
========> pt_275:  3.184436149895191
========> pt_276:  2.0226895064115524
========> pt_277:  2.2483395412564278
========> pt_278:  4.092426672577858
========> pt_279:  2.277642823755741
========> pt_280:  2.593001239001751
========> pt_281:  2.8458496183156967
========> pt_282:  2.4786988273262978
========> pt_283:  5.972451865673065
========> pt_284:  2.5604411959648132
========> pt_285:  2.527633160352707
========> pt_286:  2.772003896534443
========> pt_287:  4.108710736036301
========> pt_288:  2.012228835374117
========> pt_289:  3.4007151424884796
========> pt_290:  3.6581893265247345
========> pt_291:  2.116092089563608
========> pt_292:  2.796080559492111
========> pt_293:  2.8889810293912888
========> pt_294:  2.3898668959736824
========> pt_295:  2.99881175160408
========> pt_296:  1.8474583700299263
========> pt_297:  4.453299418091774
========> pt_298:  3.0450262501835823
========> pt_299:  2.3131566122174263
========> pt_300:  3.949209786951542
========> pt_301:  3.1210632249712944
========> pt_302:  4.982962533831596
========> pt_303:  3.0416936054825783
========> pt_304:  2.5149299949407578
========> pt_305:  3.200209364295006
========> pt_306:  2.1812778897583485
========> pt_307:  2.568315416574478
========> pt_308:  3.5017740726470947
========> pt_309:  2.267327792942524
========> pt_310:  3.6734243109822273
========> pt_311:  2.243800573050976
========> pt_312:  2.263856679201126
========> pt_313:  2.2709112986922264
========> pt_314:  3.003486581146717
========> pt_315:  2.7746833115816116
========> pt_316:  3.7075429782271385
========> pt_317:  3.6737987771630287
========> pt_318:  5.396014377474785
========> pt_319:  2.111623529344797
========> pt_320:  2.161616589874029
========> pt_321:  3.875182569026947
========> pt_322:  3.0896857753396034
========> pt_323:  7.759611532092094
========> pt_324:  2.5338089838624
========> pt_325:  3.1776170060038567
========> pt_326:  2.7173438295722008
========> pt_327:  2.579519674181938
========> pt_328:  2.405056245625019
========> pt_329:  7.442528381943703
========> pt_330:  6.781297251582146
========> pt_331:  3.484087586402893
========> pt_332:  2.197931595146656
========> pt_333:  2.090487964451313
========> pt_334:  1.8032399378716946
========> pt_335:  4.973947703838348
========> pt_336:  3.3835624530911446
========> pt_337:  2.6855801790952682
========> pt_338:  2.1856746077537537
========> pt_339:  2.4055420607328415
========> pt_340:  3.1843482702970505
===============================================> mean Dose score: 3.141903088055551
        ==> Saving latest model successfully !
            Average train loss is             0.035605045032,     best is           0.035386417422
            Average val evaluation index is   -3.141903088056,     best is           -3.126164164953
    Train use time   1528.01447
    Train loader use time     72.46187
    Val use time     44.23432
    Total use time   1573.86114
    End lr is 0.000001946133, 0.000001946133
    time: 10:55:35
Epoch: 152, iter: 75999
    Begin lr is 0.000001946133, 0.000001946133
========> pt_241:  2.633236013352871
========> pt_242:  2.269374839961529
========> pt_243:  4.40156415104866
========> pt_244:  2.419535517692566
========> pt_245:  3.1216684728860855
========> pt_246:  3.351195640861988
========> pt_247:  2.3223065212368965
========> pt_248:  2.148511055856943
========> pt_249:  3.814154230058193
========> pt_250:  2.2392764687538147
========> pt_251:  3.616759441792965
========> pt_252:  3.3638818562030792
========> pt_253:  3.4732116386294365
========> pt_254:  3.482598587870598
========> pt_255:  3.3256158977746964
========> pt_256:  2.0305874571204185
========> pt_257:  2.5767315179109573
========> pt_258:  2.2086823731660843
========> pt_259:  2.7846744656562805
========> pt_260:  3.623695932328701
========> pt_261:  3.4551483392715454
========> pt_262:  3.142583817243576
========> pt_263:  2.611524276435375
========> pt_264:  3.6358822509646416
========> pt_265:  2.460402138531208
========> pt_266:  3.645731545984745
========> pt_267:  3.2151590660214424
========> pt_268:  3.7604378908872604
========> pt_269:  2.3976268991827965
========> pt_270:  4.835110977292061
========> pt_271:  3.3156896755099297
========> pt_272:  3.2045313715934753
========> pt_273:  2.403639480471611
========> pt_274:  4.1540879011154175
========> pt_275:  3.1978535652160645
========> pt_276:  2.0357430167496204
========> pt_277:  2.2129493579268456
========> pt_278:  4.077329114079475
========> pt_279:  2.2811660915613174
========> pt_280:  2.597683370113373
========> pt_281:  2.816975824534893
========> pt_282:  2.4991189688444138
========> pt_283:  5.964176580309868
========> pt_284:  2.6405709609389305
========> pt_285:  2.4790912866592407
========> pt_286:  2.765546441078186
========> pt_287:  4.110461547970772
========> pt_288:  2.0060541853308678
========> pt_289:  3.3811917901039124
========> pt_290:  3.688249886035919
========> pt_291:  2.1414265781641006
========> pt_292:  2.7766067534685135
========> pt_293:  2.890170142054558
========> pt_294:  2.412528097629547
========> pt_295:  3.029564917087555
========> pt_296:  1.8431038968265057
========> pt_297:  4.442416951060295
========> pt_298:  3.0855215340852737
========> pt_299:  2.2956234589219093
========> pt_300:  3.9825743064284325
========> pt_301:  3.1059494987130165
========> pt_302:  4.999311789870262
========> pt_303:  3.025304451584816
========> pt_304:  2.51185342669487
========> pt_305:  3.2217247411608696
========> pt_306:  2.2048834711313248
========> pt_307:  2.560506910085678
========> pt_308:  3.48513875156641
========> pt_309:  2.238888442516327
========> pt_310:  3.7276408076286316
========> pt_311:  2.2969215735793114
========> pt_312:  2.247854247689247
========> pt_313:  2.2241168469190598
========> pt_314:  2.9610376060009003
========> pt_315:  2.738984636962414
========> pt_316:  3.70087169110775
========> pt_317:  3.7094932794570923
========> pt_318:  5.41934497654438
========> pt_319:  2.109399028122425
========> pt_320:  2.1521348506212234
========> pt_321:  3.8326644897460938
========> pt_322:  3.0903074517846107
========> pt_323:  7.683891132473946
========> pt_324:  2.542599029839039
========> pt_325:  3.187151812016964
========> pt_326:  2.703639306128025
========> pt_327:  2.5522545725107193
========> pt_328:  2.3904572799801826
========> pt_329:  7.620823308825493
========> pt_330:  6.883620917797089
========> pt_331:  3.4625398740172386
========> pt_332:  2.1895308792591095
========> pt_333:  2.066923975944519
========> pt_334:  1.7927002534270287
========> pt_335:  4.9779364466667175
========> pt_336:  3.439800702035427
========> pt_337:  2.696429267525673
========> pt_338:  2.205043062567711
========> pt_339:  2.436964623630047
========> pt_340:  3.178473114967346
===============================================> mean Dose score: 3.146736288629472
        ==> Saving latest model successfully !
            Average train loss is             0.035589440223,     best is           0.035386417422
            Average val evaluation index is   -3.146736288629,     best is           -3.126164164953
    Train use time   1528.03830
    Train loader use time     72.23922
    Val use time     44.17781
    Total use time   1573.83665
    End lr is 0.000001514127, 0.000001514127
    time: 11:21:48
Epoch: 153, iter: 76499
    Begin lr is 0.000001514127, 0.000001514127
========> pt_241:  2.57528867572546
========> pt_242:  2.270299009978771
========> pt_243:  4.404614642262459
========> pt_244:  2.437087707221508
========> pt_245:  3.149854615330696
========> pt_246:  3.3710866793990135
========> pt_247:  2.312946170568466
========> pt_248:  2.186920177191496
========> pt_249:  3.8477706536650658
========> pt_250:  2.260422855615616
========> pt_251:  3.635660335421562
========> pt_252:  3.4020961821079254
========> pt_253:  3.498612232506275
========> pt_254:  3.52585569024086
========> pt_255:  3.34567591547966
========> pt_256:  2.0576664991676807
========> pt_257:  2.512243799865246
========> pt_258:  2.234990708529949
========> pt_259:  2.7555975317955017
========> pt_260:  3.680018149316311
========> pt_261:  3.5004183277487755
========> pt_262:  3.1617123633623123
========> pt_263:  2.631438262760639
========> pt_264:  3.6990121379494667
========> pt_265:  2.4874557554721832
========> pt_266:  3.6729447543621063
========> pt_267:  3.2617081329226494
========> pt_268:  3.7919313833117485
========> pt_269:  2.4249298125505447
========> pt_270:  4.795190691947937
========> pt_271:  3.2191498950123787
========> pt_272:  3.1392809003591537
========> pt_273:  2.4229534342885017
========> pt_274:  4.1219450905919075
========> pt_275:  3.2044802606105804
========> pt_276:  2.0419536530971527
========> pt_277:  2.2582874074578285
========> pt_278:  4.151355288922787
========> pt_279:  2.2937294840812683
========> pt_280:  2.6280565932393074
========> pt_281:  2.835172116756439
========> pt_282:  2.4905474483966827
========> pt_283:  6.11724928021431
========> pt_284:  2.587125562131405
========> pt_285:  2.4966103583574295
========> pt_286:  2.794060632586479
========> pt_287:  4.150487445294857
========> pt_288:  2.017087507992983
========> pt_289:  3.3951596915721893
========> pt_290:  3.729897513985634
========> pt_291:  2.1349404379725456
========> pt_292:  2.797490283846855
========> pt_293:  2.9550015553832054
========> pt_294:  2.407682202756405
========> pt_295:  3.0299605056643486
========> pt_296:  1.8527762591838837
========> pt_297:  4.507629349827766
========> pt_298:  3.0977970361709595
========> pt_299:  2.318030670285225
========> pt_300:  3.9487847313284874
========> pt_301:  3.1390829756855965
========> pt_302:  5.067302957177162
========> pt_303:  3.0203067883849144
========> pt_304:  2.536133751273155
========> pt_305:  3.237031176686287
========> pt_306:  2.218165285885334
========> pt_307:  2.5920358672738075
========> pt_308:  3.548666574060917
========> pt_309:  2.263331487774849
========> pt_310:  3.7098534032702446
========> pt_311:  2.2845472395420074
========> pt_312:  2.273634783923626
========> pt_313:  2.27303110063076
========> pt_314:  2.997063025832176
========> pt_315:  2.763376049697399
========> pt_316:  3.736887462437153
========> pt_317:  3.662048727273941
========> pt_318:  5.474818646907806
========> pt_319:  2.1165028028190136
========> pt_320:  2.1634355932474136
========> pt_321:  3.9403336867690086
========> pt_322:  3.183409497141838
========> pt_323:  7.772692292928696
========> pt_324:  2.55218468606472
========> pt_325:  3.19739643484354
========> pt_326:  2.7311453595757484
========> pt_327:  2.580847777426243
========> pt_328:  2.4207473173737526
========> pt_329:  7.755910158157349
========> pt_330:  6.776390075683594
========> pt_331:  3.5236769542098045
========> pt_332:  2.2058384120464325
========> pt_333:  2.0666804164648056
========> pt_334:  1.7986861057579517
========> pt_335:  4.970761090517044
========> pt_336:  3.4123585373163223
========> pt_337:  2.7152271568775177
========> pt_338:  2.227928526699543
========> pt_339:  2.4560772627592087
========> pt_340:  3.2452496141195297
===============================================> mean Dose score: 3.1664692951366304
        ==> Saving latest model successfully !
            Average train loss is             0.035479455832,     best is           0.035386417422
            Average val evaluation index is   -3.166469295137,     best is           -3.126164164953
    Train use time   1528.12507
    Train loader use time     72.05060
    Val use time     44.22682
    Total use time   1573.95952
    End lr is 0.000001139385, 0.000001139385
    time: 11:48:02
Epoch: 154, iter: 76999
    Begin lr is 0.000001139385, 0.000001139385
========> pt_241:  2.620214708149433
========> pt_242:  2.287013605237007
========> pt_243:  4.465036690235138
========> pt_244:  2.433101572096348
========> pt_245:  3.1522539630532265
========> pt_246:  3.3868395537137985
========> pt_247:  2.3254824429750443
========> pt_248:  2.1534347906708717
========> pt_249:  3.87560423463583
========> pt_250:  2.2073065489530563
========> pt_251:  3.6805563792586327
========> pt_252:  3.4139252454042435
========> pt_253:  3.539879396557808
========> pt_254:  3.530851788818836
========> pt_255:  3.3438265323638916
========> pt_256:  2.073261085897684
========> pt_257:  2.6425812393426895
========> pt_258:  2.1836078725755215
========> pt_259:  2.754579223692417
========> pt_260:  3.661654181778431
========> pt_261:  3.6309122294187546
========> pt_262:  3.169950358569622
========> pt_263:  2.6536454632878304
========> pt_264:  3.71202040463686
========> pt_265:  2.4729856103658676
========> pt_266:  3.6485491693019867
========> pt_267:  3.2699748128652573
========> pt_268:  3.8303371146321297
========> pt_269:  2.41869505494833
========> pt_270:  4.897301569581032
========> pt_271:  3.420187383890152
========> pt_272:  3.161681331694126
========> pt_273:  2.419876866042614
========> pt_274:  4.156198054552078
========> pt_275:  3.241858556866646
========> pt_276:  2.0746370404958725
========> pt_277:  2.22112737596035
========> pt_278:  4.221871756017208
========> pt_279:  2.292421981692314
========> pt_280:  2.6300254091620445
========> pt_281:  2.8199459984898567
========> pt_282:  2.5022729858756065
========> pt_283:  5.955803245306015
========> pt_284:  2.5958334654569626
========> pt_285:  2.5239599496126175
========> pt_286:  2.770121917128563
========> pt_287:  4.137641377747059
========> pt_288:  2.0312937535345554
========> pt_289:  3.528049550950527
========> pt_290:  3.748457320034504
========> pt_291:  2.1695606969296932
========> pt_292:  2.8234849125146866
========> pt_293:  2.9385938867926598
========> pt_294:  2.4251027032732964
========> pt_295:  3.0524490773677826
========> pt_296:  1.8753278069198132
========> pt_297:  4.505584388971329
========> pt_298:  3.0903708189725876
========> pt_299:  2.312614470720291
========> pt_300:  3.994101919233799
========> pt_301:  3.179197795689106
========> pt_302:  5.0760069489479065
========> pt_303:  3.0395738035440445
========> pt_304:  2.523820959031582
========> pt_305:  3.2345110923051834
========> pt_306:  2.1929576620459557
========> pt_307:  2.576182596385479
========> pt_308:  3.578804060816765
========> pt_309:  2.240164130926132
========> pt_310:  3.698178455233574
========> pt_311:  2.2389481589198112
========> pt_312:  2.2465425729751587
========> pt_313:  2.2807011380791664
========> pt_314:  3.004974015057087
========> pt_315:  2.758384123444557
========> pt_316:  3.8326331973075867
========> pt_317:  3.699861988425255
========> pt_318:  5.542954802513123
========> pt_319:  2.1071816980838776
========> pt_320:  2.152453251183033
========> pt_321:  3.9966415613889694
========> pt_322:  3.0994346737861633
========> pt_323:  7.745256647467613
========> pt_324:  2.582981400191784
========> pt_325:  3.1841889396309853
========> pt_326:  2.7277879416942596
========> pt_327:  2.5676139444112778
========> pt_328:  2.381175421178341
========> pt_329:  7.54588209092617
========> pt_330:  6.733276396989822
========> pt_331:  3.5463274642825127
========> pt_332:  2.182273380458355
========> pt_333:  2.095040362328291
========> pt_334:  1.7953833192586899
========> pt_335:  5.033639594912529
========> pt_336:  3.413871005177498
========> pt_337:  2.73263618350029
========> pt_338:  2.2357571125030518
========> pt_339:  2.444342076778412
========> pt_340:  3.1777604296803474
===============================================> mean Dose score: 3.175011412426829
        ==> Saving latest model successfully !
            Average train loss is             0.035524462841,     best is           0.035386417422
            Average val evaluation index is   -3.175011412427,     best is           -3.126164164953
    Train use time   1527.46271
    Train loader use time     72.65558
    Val use time     43.99975
    Total use time   1573.04101
    End lr is 0.000000822050, 0.000000822050
    time: 12:14:15
Epoch: 155, iter: 77499
    Begin lr is 0.000000822050, 0.000000822050
========> pt_241:  2.642062045633793
========> pt_242:  2.3026608675718307
========> pt_243:  4.40609946846962
========> pt_244:  2.3935935646295547
========> pt_245:  3.1575796753168106
========> pt_246:  3.3564869314432144
========> pt_247:  2.3170866817235947
========> pt_248:  2.2304334864020348
========> pt_249:  3.8252291455864906
========> pt_250:  2.2186899557709694
========> pt_251:  3.572828769683838
========> pt_252:  3.414853848516941
========> pt_253:  3.4667234122753143
========> pt_254:  3.4934359416365623
========> pt_255:  3.2861579582095146
========> pt_256:  2.058456502854824
========> pt_257:  2.487415075302124
========> pt_258:  2.1761001646518707
========> pt_259:  2.8029362112283707
========> pt_260:  3.6124929785728455
========> pt_261:  3.39861698448658
========> pt_262:  3.1886817514896393
========> pt_263:  2.661092020571232
========> pt_264:  3.681997135281563
========> pt_265:  2.4733147025108337
========> pt_266:  3.6718586459755898
========> pt_267:  3.2749756053090096
========> pt_268:  3.756234273314476
========> pt_269:  2.4321886152029037
========> pt_270:  4.748690128326416
========> pt_271:  3.2150281593203545
========> pt_272:  3.2200751081109047
========> pt_273:  2.3949524387717247
========> pt_274:  4.16204322129488
========> pt_275:  3.1837529316544533
========> pt_276:  2.006597500294447
========> pt_277:  2.239062376320362
========> pt_278:  4.06775988638401
========> pt_279:  2.2772154211997986
========> pt_280:  2.576826699078083
========> pt_281:  2.820049002766609
========> pt_282:  2.491932138800621
========> pt_283:  6.009560003876686
========> pt_284:  2.614324167370796
========> pt_285:  2.5163008645176888
========> pt_286:  2.7760784327983856
========> pt_287:  4.091984406113625
========> pt_288:  2.02342227101326
========> pt_289:  3.3403919264674187
========> pt_290:  3.6786118149757385
========> pt_291:  2.109221573919058
========> pt_292:  2.8220736235380173
========> pt_293:  2.912546582520008
========> pt_294:  2.4223870411515236
========> pt_295:  2.9772966355085373
========> pt_296:  1.86647217720747
========> pt_297:  4.482988119125366
========> pt_298:  3.081638664007187
========> pt_299:  2.3235133662819862
========> pt_300:  3.907504267990589
========> pt_301:  3.11629269272089
========> pt_302:  4.988015741109848
========> pt_303:  3.048510402441025
========> pt_304:  2.5183606892824173
========> pt_305:  3.204945996403694
========> pt_306:  2.1974755078554153
========> pt_307:  2.5859912112355232
========> pt_308:  3.502240590751171
========> pt_309:  2.276265174150467
========> pt_310:  3.6687327921390533
========> pt_311:  2.263314798474312
========> pt_312:  2.2640639916062355
========> pt_313:  2.2559383884072304
========> pt_314:  2.9991301521658897
========> pt_315:  2.7594081684947014
========> pt_316:  3.671477399766445
========> pt_317:  3.6172867193818092
========> pt_318:  5.441874489188194
========> pt_319:  2.120650615543127
========> pt_320:  2.1363401226699352
========> pt_321:  3.8157329335808754
========> pt_322:  3.140917234122753
========> pt_323:  7.760937288403511
========> pt_324:  2.5233807787299156
========> pt_325:  3.1526563316583633
========> pt_326:  2.7042721956968307
========> pt_327:  2.6055187359452248
========> pt_328:  2.428751662373543
========> pt_329:  7.978064566850662
========> pt_330:  6.840032637119293
========> pt_331:  3.4517979621887207
========> pt_332:  2.201059013605118
========> pt_333:  2.0795498229563236
========> pt_334:  1.8233341164886951
========> pt_335:  4.96350385248661
========> pt_336:  3.3603742346167564
========> pt_337:  2.6691297441720963
========> pt_338:  2.1717180497944355
========> pt_339:  2.4118055030703545
========> pt_340:  3.2270335033535957
===============================================> mean Dose score: 3.15066469181329
        ==> Saving latest model successfully !
            Average train loss is             0.035869080983,     best is           0.035386417422
            Average val evaluation index is   -3.150664691813,     best is           -3.126164164953
    Train use time   1527.77917
    Train loader use time     72.83627
    Val use time     44.07693
    Total use time   1573.38229
    End lr is 0.000000562246, 0.000000562246
    time: 12:40:29
Epoch: 156, iter: 77999
    Begin lr is 0.000000562246, 0.000000562246
========> pt_241:  2.6474086195230484
========> pt_242:  2.292212061583996
========> pt_243:  4.41150888800621
========> pt_244:  2.39485751837492
========> pt_245:  3.1415673345327377
========> pt_246:  3.3542388305068016
========> pt_247:  2.3263659328222275
========> pt_248:  2.1900638937950134
========> pt_249:  3.8244257122278214
========> pt_250:  2.21686951816082
========> pt_251:  3.6152180284261703
========> pt_252:  3.4134485572576523
========> pt_253:  3.488161601126194
========> pt_254:  3.485000543296337
========> pt_255:  3.335312381386757
========> pt_256:  2.0545178279280663
========> pt_257:  2.5657549127936363
========> pt_258:  2.1886656433343887
========> pt_259:  2.7924159541726112
========> pt_260:  3.6302756890654564
========> pt_261:  3.4595566615462303
========> pt_262:  3.1712768971920013
========> pt_263:  2.6469778269529343
========> pt_264:  3.659662939608097
========> pt_265:  2.4612929299473763
========> pt_266:  3.6441966518759727
========> pt_267:  3.2424481585621834
========> pt_268:  3.765883557498455
========> pt_269:  2.423744350671768
========> pt_270:  4.7623466700315475
========> pt_271:  3.2493403181433678
========> pt_272:  3.197305165231228
========> pt_273:  2.4161778390407562
========> pt_274:  4.163666255772114
========> pt_275:  3.180037997663021
========> pt_276:  2.0260423608124256
========> pt_277:  2.2365621104836464
========> pt_278:  4.088946953415871
========> pt_279:  2.2936684638261795
========> pt_280:  2.60326124727726
========> pt_281:  2.8316324204206467
========> pt_282:  2.480814717710018
========> pt_283:  6.00629098713398
========> pt_284:  2.6011143252253532
========> pt_285:  2.5005485117435455
========> pt_286:  2.775380089879036
========> pt_287:  4.067902006208897
========> pt_288:  2.0165604911744595
========> pt_289:  3.358345180749893
========> pt_290:  3.6928730830550194
========> pt_291:  2.1430493518710136
========> pt_292:  2.8298552706837654
========> pt_293:  2.922574244439602
========> pt_294:  2.4058059602975845
========> pt_295:  2.9975420609116554
========> pt_296:  1.8648334965109825
========> pt_297:  4.484030157327652
========> pt_298:  3.0748330801725388
========> pt_299:  2.3075198009610176
========> pt_300:  3.950427323579788
========> pt_301:  3.1356314197182655
========> pt_302:  4.957665205001831
========> pt_303:  3.036753311753273
========> pt_304:  2.5160596519708633
========> pt_305:  3.2042356580495834
========> pt_306:  2.219380997121334
========> pt_307:  2.5695905834436417
========> pt_308:  3.4789303317666054
========> pt_309:  2.2506293654441833
========> pt_310:  3.7043673172593117
========> pt_311:  2.2921309620141983
========> pt_312:  2.254936769604683
========> pt_313:  2.260139137506485
========> pt_314:  2.978883422911167
========> pt_315:  2.752973660826683
========> pt_316:  3.714580126106739
========> pt_317:  3.6743398755788803
========> pt_318:  5.489892214536667
========> pt_319:  2.1193740144371986
========> pt_320:  2.1503444015979767
========> pt_321:  3.815203309059143
========> pt_322:  3.1378119811415672
========> pt_323:  7.76640772819519
========> pt_324:  2.5241729989647865
========> pt_325:  3.1515300646424294
========> pt_326:  2.696305923163891
========> pt_327:  2.587779574096203
========> pt_328:  2.412797473371029
========> pt_329:  7.641021013259888
========> pt_330:  6.839307174086571
========> pt_331:  3.4683487936854362
========> pt_332:  2.1859752759337425
========> pt_333:  2.0729949697852135
========> pt_334:  1.804016511887312
========> pt_335:  4.9813394993543625
========> pt_336:  3.4006160497665405
========> pt_337:  2.6990265399217606
========> pt_338:  2.193526402115822
========> pt_339:  2.4276160076260567
========> pt_340:  3.216121308505535
===============================================> mean Dose score: 3.151553743891418
        ==> Saving latest model successfully !
            Average train loss is             0.035554805335,     best is           0.035386417422
            Average val evaluation index is   -3.151553743891,     best is           -3.126164164953
    Train use time   1528.18737
    Train loader use time     73.45928
    Val use time     44.20497
    Total use time   1573.98334
    End lr is 0.000000360072, 0.000000360072
    time: 13:06:43
Epoch: 157, iter: 78499
    Begin lr is 0.000000360072, 0.000000360072
========> pt_241:  2.5762511789798737
========> pt_242:  2.278929464519024
========> pt_243:  4.421326890587807
========> pt_244:  2.429802566766739
========> pt_245:  3.12932051718235
========> pt_246:  3.37134275585413
========> pt_247:  2.2976918891072273
========> pt_248:  2.185708899050951
========> pt_249:  3.860586993396282
========> pt_250:  2.2490618750452995
========> pt_251:  3.6609315872192383
========> pt_252:  3.397972099483013
========> pt_253:  3.523402623832226
========> pt_254:  3.5020366683602333
========> pt_255:  3.306901454925537
========> pt_256:  2.056586779654026
========> pt_257:  2.5330913439393044
========> pt_258:  2.205459251999855
========> pt_259:  2.7547017857432365
========> pt_260:  3.654431104660034
========> pt_261:  3.5099301859736443
========> pt_262:  3.1627896055579185
========> pt_263:  2.6413725689053535
========> pt_264:  3.6798908933997154
========> pt_265:  2.4701666831970215
========> pt_266:  3.64320520311594
========> pt_267:  3.254457414150238
========> pt_268:  3.789462149143219
========> pt_269:  2.4209102988243103
========> pt_270:  4.8068867623806
========> pt_271:  3.2540130615234375
========> pt_272:  3.1793753802776337
========> pt_273:  2.416835241019726
========> pt_274:  4.109240360558033
========> pt_275:  3.196902275085449
========> pt_276:  2.0314182713627815
========> pt_277:  2.242412492632866
========> pt_278:  4.132348261773586
========> pt_279:  2.2820091620087624
========> pt_280:  2.6042086258530617
========> pt_281:  2.8243569284677505
========> pt_282:  2.479931227862835
========> pt_283:  5.977777317166328
========> pt_284:  2.5976137444376945
========> pt_285:  2.5022203102707863
========> pt_286:  2.79868982732296
========> pt_287:  4.118584282696247
========> pt_288:  2.02760124579072
========> pt_289:  3.3924416825175285
========> pt_290:  3.711177334189415
========> pt_291:  2.1492736786603928
========> pt_292:  2.8293024376034737
========> pt_293:  2.9460247978568077
========> pt_294:  2.397613599896431
========> pt_295:  3.0191127210855484
========> pt_296:  1.86526820063591
========> pt_297:  4.519486576318741
========> pt_298:  3.0817515775561333
========> pt_299:  2.3189686611294746
========> pt_300:  3.951607048511505
========> pt_301:  3.1402308866381645
========> pt_302:  5.020903050899506
========> pt_303:  3.0098045244812965
========> pt_304:  2.536490224301815
========> pt_305:  3.2335981354117393
========> pt_306:  2.21845343708992
========> pt_307:  2.5821445882320404
========> pt_308:  3.5621168464422226
========> pt_309:  2.2336383536458015
========> pt_310:  3.6779896169900894
========> pt_311:  2.278004251420498
========> pt_312:  2.2636201605200768
========> pt_313:  2.2710802778601646
========> pt_314:  3.002167083323002
========> pt_315:  2.764871306717396
========> pt_316:  3.7414058297872543
========> pt_317:  3.643387220799923
========> pt_318:  5.480041354894638
========> pt_319:  2.121898401528597
========> pt_320:  2.1483120881021023
========> pt_321:  3.8714582473039627
========> pt_322:  3.12078133225441
========> pt_323:  7.7483973652124405
========> pt_324:  2.5469445064663887
========> pt_325:  3.174053058028221
========> pt_326:  2.711676247417927
========> pt_327:  2.581830620765686
========> pt_328:  2.427917718887329
========> pt_329:  7.596972733736038
========> pt_330:  6.790705844759941
========> pt_331:  3.5037697479128838
========> pt_332:  2.1962790936231613
========> pt_333:  2.084681522101164
========> pt_334:  1.7938004434108734
========> pt_335:  4.96998555958271
========> pt_336:  3.389843888580799
========> pt_337:  2.716658003628254
========> pt_338:  2.215220667421818
========> pt_339:  2.4393871799111366
========> pt_340:  3.2091929018497467
===============================================> mean Dose score: 3.157518901489675
        ==> Saving latest model successfully !
            Average train loss is             0.035398637403,     best is           0.035386417422
            Average val evaluation index is   -3.157518901490,     best is           -3.126164164953
    Train use time   1526.50037
    Train loader use time     72.17748
    Val use time     45.17108
    Total use time   1573.32532
    End lr is 0.000000215606, 0.000000215606
    time: 13:32:56
Epoch: 158, iter: 78999
    Begin lr is 0.000000215606, 0.000000215606
========> pt_241:  2.567957639694214
========> pt_242:  2.267153598368168
========> pt_243:  4.420825690031052
========> pt_244:  2.4222279712557793
========> pt_245:  3.1253669783473015
========> pt_246:  3.3811868354678154
========> pt_247:  2.3110443726181984
========> pt_248:  2.187614217400551
========> pt_249:  3.869602084159851
========> pt_250:  2.235547713935375
========> pt_251:  3.6438774690032005
========> pt_252:  3.392987996339798
========> pt_253:  3.538123108446598
========> pt_254:  3.510640002787113
========> pt_255:  3.292790912091732
========> pt_256:  2.05281525850296
========> pt_257:  2.5102781131863594
========> pt_258:  2.1997442096471786
========> pt_259:  2.7632662653923035
========> pt_260:  3.621525801718235
========> pt_261:  3.5425079613924026
========> pt_262:  3.177998773753643
========> pt_263:  2.6447584107518196
========> pt_264:  3.6699891835451126
========> pt_265:  2.480854354798794
========> pt_266:  3.672969527542591
========> pt_267:  3.251909166574478
========> pt_268:  3.809121884405613
========> pt_269:  2.421869672834873
========> pt_270:  4.835148006677628
========> pt_271:  3.276321180164814
========> pt_272:  3.1795839965343475
========> pt_273:  2.416010946035385
========> pt_274:  4.123544916510582
========> pt_275:  3.2029808312654495
========> pt_276:  2.036272771656513
========> pt_277:  2.236711271107197
========> pt_278:  4.194341711699963
========> pt_279:  2.2816354781389236
========> pt_280:  2.614147886633873
========> pt_281:  2.835202105343342
========> pt_282:  2.4873845651745796
========> pt_283:  5.9618160873651505
========> pt_284:  2.5867443159222603
========> pt_285:  2.5032782554626465
========> pt_286:  2.801741622388363
========> pt_287:  4.135953933000565
========> pt_288:  2.031112778931856
========> pt_289:  3.444164954125881
========> pt_290:  3.724423162639141
========> pt_291:  2.143051829189062
========> pt_292:  2.832856737077236
========> pt_293:  2.948346696794033
========> pt_294:  2.400285452604294
========> pt_295:  3.032783344388008
========> pt_296:  1.8656064197421074
========> pt_297:  4.5094505697488785
========> pt_298:  3.092629872262478
========> pt_299:  2.323072925209999
========> pt_300:  3.9643996581435204
========> pt_301:  3.1425321847200394
========> pt_302:  5.045749247074127
========> pt_303:  3.0269068852066994
========> pt_304:  2.536730132997036
========> pt_305:  3.244330920279026
========> pt_306:  2.2126760706305504
========> pt_307:  2.592815048992634
========> pt_308:  3.5646893456578255
========> pt_309:  2.228206768631935
========> pt_310:  3.6928363144397736
========> pt_311:  2.263472303748131
========> pt_312:  2.2584493458271027
========> pt_313:  2.2665251418948174
========> pt_314:  3.0017122998833656
========> pt_315:  2.7734850719571114
========> pt_316:  3.7749403715133667
========> pt_317:  3.6954062059521675
========> pt_318:  5.49418032169342
========> pt_319:  2.107243500649929
========> pt_320:  2.140429001301527
========> pt_321:  3.945532664656639
========> pt_322:  3.140987381339073
========> pt_323:  7.744424790143967
========> pt_324:  2.545596845448017
========> pt_325:  3.166031241416931
========> pt_326:  2.7189480885863304
========> pt_327:  2.5709325075149536
========> pt_328:  2.4253856390714645
========> pt_329:  7.855702787637711
========> pt_330:  6.765011623501778
========> pt_331:  3.499409928917885
========> pt_332:  2.1894901990890503
========> pt_333:  2.0840577594935894
========> pt_334:  1.805036123842001
========> pt_335:  4.982517659664154
========> pt_336:  3.406028598546982
========> pt_337:  2.732325866818428
========> pt_338:  2.218121476471424
========> pt_339:  2.438705526292324
========> pt_340:  3.2076825201511383
===============================================> mean Dose score: 3.165128051675856
        ==> Saving best_train_loss model successfully !
        ==> Saving latest model successfully !
            Average train loss is             0.035316757511,     best is           0.035316757511
            Average val evaluation index is   -3.165128051676,     best is           -3.126164164953
    Train use time   1531.68316
    Train loader use time     77.03210
    Val use time     43.65839
    Total use time   1578.38918
    End lr is 0.000000128904, 0.000000128904
    time: 13:59:15
Epoch: 159, iter: 79499
    Begin lr is 0.000000128904, 0.000000128904
========> pt_241:  2.5626906007528305
========> pt_242:  2.2652364149689674
========> pt_243:  4.429389387369156
========> pt_244:  2.431836575269699
========> pt_245:  3.135056421160698
========> pt_246:  3.384568765759468
========> pt_247:  2.307138554751873
========> pt_248:  2.178161684423685
========> pt_249:  3.859534002840519
========> pt_250:  2.237885259091854
========> pt_251:  3.655564934015274
========> pt_252:  3.4110987558960915
========> pt_253:  3.5248585045337677
========> pt_254:  3.535010553896427
========> pt_255:  3.298454061150551
========> pt_256:  2.0572922937572002
========> pt_257:  2.510285936295986
========> pt_258:  2.232585623860359
========> pt_259:  2.7493395656347275
========> pt_260:  3.6394302919507027
========> pt_261:  3.513883464038372
========> pt_262:  3.1752799823880196
========> pt_263:  2.663414441049099
========> pt_264:  3.6812302097678185
========> pt_265:  2.4948853626847267
========> pt_266:  3.6902377381920815
========> pt_267:  3.2811350002884865
========> pt_268:  3.8006585836410522
========> pt_269:  2.42180448025465
========> pt_270:  4.8255109786987305
========> pt_271:  3.2672959193587303
========> pt_272:  3.163917176425457
========> pt_273:  2.4151039868593216
========> pt_274:  4.110526219010353
========> pt_275:  3.2193386927247047
========> pt_276:  2.0495221205055714
========> pt_277:  2.253810502588749
========> pt_278:  4.203661642968655
========> pt_279:  2.2521913796663284
========> pt_280:  2.6042547821998596
========> pt_281:  2.8475558385252953
========> pt_282:  2.491627559065819
========> pt_283:  6.05874128639698
========> pt_284:  2.5776176154613495
========> pt_285:  2.4888266250491142
========> pt_286:  2.813066355884075
========> pt_287:  4.145950824022293
========> pt_288:  2.0313391275703907
========> pt_289:  3.446041978895664
========> pt_290:  3.7436990439891815
========> pt_291:  2.1304695308208466
========> pt_292:  2.8314266726374626
========> pt_293:  2.952440269291401
========> pt_294:  2.399059571325779
========> pt_295:  3.0127739161252975
========> pt_296:  1.8602320738136768
========> pt_297:  4.534260258078575
========> pt_298:  3.0779099091887474
========> pt_299:  2.3332184553146362
========> pt_300:  3.955504521727562
========> pt_301:  3.1490733474493027
========> pt_302:  5.080874487757683
========> pt_303:  3.032790645956993
========> pt_304:  2.5443414971232414
========> pt_305:  3.261736035346985
========> pt_306:  2.209523618221283
========> pt_307:  2.6145776361227036
========> pt_308:  3.5592246428132057
========> pt_309:  2.23356194794178
========> pt_310:  3.667564280331135
========> pt_311:  2.2512372210621834
========> pt_312:  2.270672172307968
========> pt_313:  2.2409334033727646
========> pt_314:  3.0170946195721626
========> pt_315:  2.7887649089097977
========> pt_316:  3.746275193989277
========> pt_317:  3.664107248187065
========> pt_318:  5.44637069106102
========> pt_319:  2.123019192367792
========> pt_320:  2.1529242023825645
========> pt_321:  3.9329515397548676
========> pt_322:  3.1490733474493027
========> pt_323:  7.755133584141731
========> pt_324:  2.554030679166317
========> pt_325:  3.1749657541513443
========> pt_326:  2.7162298187613487
========> pt_327:  2.575162462890148
========> pt_328:  2.412024810910225
========> pt_329:  7.841029241681099
========> pt_330:  6.7291369289159775
========> pt_331:  3.5039470717310905
========> pt_332:  2.197279669344425
========> pt_333:  2.086838874965906
========> pt_334:  1.8042624182999134
========> pt_335:  4.971500635147095
========> pt_336:  3.399473875761032
========> pt_337:  2.7187300845980644
========> pt_338:  2.222481295466423
========> pt_339:  2.439933232963085
========> pt_340:  3.248439095914364
===============================================> mean Dose score: 3.167121336981654
        ==> Saving latest model successfully !
            Average train loss is             0.035718768861,     best is           0.035316757511
            Average val evaluation index is   -3.167121336982,     best is           -3.126164164953
    Train use time   1529.60931
    Train loader use time     75.91233
    Val use time     43.80302
    Total use time   1575.05962
    End lr is 0.000000100000, 0.000000100000
    time: 14:25:30
===============================> End successfully
Traceback (most recent call last):
  File "YOUR_ROOT/Code/lenas/RTDosePrediction/Src/Vnet/train.py", line 89, in <module>
    trainer.run()
  File "YOUR_ROOT/Code/lenas/RTDosePrediction/Src/NetworkTrainer/network_trainer.py", line 332, in run
    self.save_wandb()
  File "YOUR_ROOT/Code/lenas/RTDosePrediction/Src/NetworkTrainer/network_trainer.py", line 387, in save_wandb
    config.project_name = self.project_name
AttributeError: 'NetworkTrainer' object has no attribute 'project_name'